{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobylub/final_coursework/blob/main/coursework_data_import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqwi4b5geG6"
      },
      "source": [
        "**Experiment**\n",
        "<br>\n",
        "This research seeks to compare One-Hot-Encoding against Multi-Layer Perceptron, which has be de-facto for many ML problems. \n",
        "<br>\n",
        "The following experiement will be performed and evaluated: \n",
        "\n",
        "*   One hot encoding (OHE), sigmoid\n",
        "*   Multi Layer Perceptron (MLP), sigmoid\n",
        "*   OHE, softmax  \n",
        "*   MLP, softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the Data and Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhNWdx7l84yP",
        "outputId": "8563f28d-5f96-41bb-c5b6-4a9adba9bb8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: wget\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 21.2M  100 21.2M    0     0  4455k      0  0:00:04  0:00:04 --:--:-- 4526k\n"
          ]
        }
      ],
      "source": [
        "# Run this when editing in code editor \n",
        "!curl -O https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YHzNVVHQPd9",
        "outputId": "bf37b091-809e-4b00-f8c3-83c688764170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REVIEW\tRATING\tPRODUCT_TYPE\tHELPFUL\n",
            "\"This is a wonderful album, that evokes memories of the 60's folk boom, yet contains original songs. I was amazed at the fantastic harmonies and musical arrangements.Anyone who loves the movie \"\"A Mighty Wind\"\" and who loves folk music will fall in love with this album. I know I did\"\tpositive\tmusic\tneutral\n",
            "\"On one hand, this CD is a straight ahead instrumental rocker, but Johnny A really shows how great he is with ballads, such as his covers of \"\"Wichita Lineman,\"\" and \"\"Yes it Is.\"\"  In fact, those two ballads alone are worth the price of the CD by themselves.But Johnny A can flat kick your ass, too.  He's a biker and his tunes like Oh Yeah, In the Wind and Two Wheel Horse are named for his other hobby.  And they rock, but there's nothing cliched or tired in his style.  He always seems to be looking for new ways to say something.I saw him in person at the Triple Door in Seattle sometime in February 2005 in a power trio format and he played most of the tunes on this album.  The guy is one amazing guitar player.  He played his signature Gibson hollow body, fitted with a vibrato tailpiece (Bigsby? It was like the old Chet Atkins \"\"country gentleman\"\" model) and he utilized a battery of foot pedal effects, coming eventually through a pair of Marshall combo amps.  The guy had some of the best clean tones I've ever heard from anyone, anywhere.Basically, Johnny A is a guitarist who has complete command of the instrument.  And he's got a rocking soul that cuts loose on originals and covers alike in a style that's all his own, and that's saying something these days.  If you love great rocking/rockabilly guitar, combined with really cool ballad playing in the power trio format, this CD is just what you're looking for.  In fact, I guarantee you'll be knocked out.Five stars.\"\tpositive\tmusic\thelpful\n",
            "\"this band reminds me of the thrill i first got when i listened to an Atreyu Album. It dies today rip off the former bands style, but they are still a very good band.  In the over-crowded metalcore market of today, that is a rarity.  My only complaint, is that the vocalist has a beautiful singing voice (as heard on \"\"The Radiance), but more often then not goes for the screams and growls that are associated with this type of music. still 5/5 material though.Favorite songs: \"\"The Radiance,\"\" \"\"Freak Gasoline Fight,\"\" \"\"Our Disintigration,\"\" and :THe Caitliff Choir:Defeatism\"\tpositive\tmusic\tunhelpful\n",
            "\"Like I said I would, I finally got around to purchasing this CD. I especially like tracks 9-12. Cheap Trick is solid as always. I have been a long time fan and enjoy all their releases. If I do have any criticism at all, it is the fact that some songs sound like recycled Trick riffs. I guess it's hard to create new stuff after 30+ years. Anyway, there are some great tunes on this. Buy it like I did!\"\tpositive\tmusic\tunhelpful\n"
          ]
        }
      ],
      "source": [
        "# Example of what the data looks like\n",
        "!head -n5 Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_eYghCK5OX"
      },
      "source": [
        "**Data loading and pre-processing**\n",
        "<br>\n",
        "Below we preprocess the data from the raw file \"Compiled_reviews.txt\"\n",
        "<br>\n",
        "We remove any unwanted characters, to improve the integrity of the text corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Li-IcrXi9O8G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reviews=[]\n",
        "sentiment_ratings=[]\n",
        "product_types=[]\n",
        "helpfulness_ratings=[]\n",
        "\n",
        "with open(\"Compiled_Reviews.txt\") as f:\n",
        "   for line in f.readlines()[1:]:\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # remove punctuation/numbers and replace it with a space\n",
        "        fields[0] = re.sub(r'[.,!?;:()\\[\\]{}\\-â€”\\'\\/\\\"\\\"\\d+]', \" \",fields[0])\n",
        "        reviews.append(fields[0])\n",
        "        sentiment_ratings.append(fields[1])\n",
        "        product_types.append(fields[2])\n",
        "        helpfulness_ratings.append(fields[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qV1XhmNXI4"
      },
      "source": [
        "**Data Analysis**\n",
        "<br/>\n",
        "Below we see what the data looks like after pre-processing\n",
        "<br/>\n",
        "Data analysis can also be shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMAE3UTkK-jO",
        "outputId": "6c0af6b2-c7fe-43d7-e8a7-87965c784bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is   This is a wonderful album  that evokes memories of the    s folk boom  yet contains original songs  I was amazed at the fantastic harmonies and musical arrangements Anyone who loves the movie   A Mighty Wind   and who loves folk music will fall in love with this album  I know I did \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   On one hand  this CD is a straight ahead instrumental rocker  but Johnny A really shows how great he is with ballads  such as his covers of   Wichita Lineman    and   Yes it Is     In fact  those two ballads alone are worth the price of the CD by themselves But Johnny A can flat kick your ass  too   He s a biker and his tunes like Oh Yeah  In the Wind and Two Wheel Horse are named for his other hobby   And they rock  but there s nothing cliched or tired in his style   He always seems to be looking for new ways to say something I saw him in person at the Triple Door in Seattle sometime in February      in a power trio format and he played most of the tunes on this album   The guy is one amazing guitar player   He played his signature Gibson hollow body  fitted with a vibrato tailpiece  Bigsby  It was like the old Chet Atkins   country gentleman   model  and he utilized a battery of foot pedal effects  coming eventually through a pair of Marshall combo amps   The guy had some of the best clean tones I ve ever heard from anyone  anywhere Basically  Johnny A is a guitarist who has complete command of the instrument   And he s got a rocking soul that cuts loose on originals and covers alike in a style that s all his own  and that s saying something these days   If you love great rocking rockabilly guitar  combined with really cool ballad playing in the power trio format  this CD is just what you re looking for   In fact  I guarantee you ll be knocked out Five stars  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is   this band reminds me of the thrill i first got when i listened to an Atreyu Album  It dies today rip off the former bands style  but they are still a very good band   In the over crowded metalcore market of today  that is a rarity   My only complaint  is that the vocalist has a beautiful singing voice  as heard on   The Radiance   but more often then not goes for the screams and growls that are associated with this type of music  still     material though Favorite songs    The Radiance      Freak Gasoline Fight      Our Disintigration    and  THe Caitliff Choir Defeatism \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Like I said I would  I finally got around to purchasing this CD  I especially like tracks       Cheap Trick is solid as always  I have been a long time fan and enjoy all their releases  If I do have any criticism at all  it is the fact that some songs sound like recycled Trick riffs  I guess it s hard to create new stuff after     years  Anyway  there are some great tunes on this  Buy it like I did  \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Ok good CD  im not suprised  Ok jaheim may not have the b best voice but his music is good and it goes will with the voice that he has   Yes yall this album does use profanity but the songs actually have meanings  I like that daddy thing song  and like a dj  This album looks at issues from a mans  point of view  Ya know we ve heard the angry woman and how the man did t his and that now jaheim looks at it from a males view  Ok yall i know he is not the first one to do this but he did it well  Also this is a break from that       that is played on the radio  \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   Review by Mike Watson With the recent slew of Victory Records releases  not to mention the wide variety  one can be to say the least skeptical of any of their assortment of new releases   However  picking through all of the less notable releases from the label  the up and coming band  The Forecast  have stepped to the plate and scored a home run on Late Night Conversations   The Forecast are a perfect blend of male and female vocals over a very modern rock soundtrack  executed flawlessly over a ten song full length Late Night Conversations is an album confronting different topics ranging from relationships to the teen trend of mutilation and loathing   The harmonies arranged by both vocalists are undeniably catching and well written and the music follows closely in suit leaving your head bobbing back and forth to the beat and your feet tapping almost involuntarily  The Forecast is not only a great recorded band  but bring the same toe tapping energy to the stage  with both great stage presence and cohesion as a band   The Forecast will defiantly be one of the new big bands in poppier rock music  and checking their Victory Records debut out is a must for fans of the genre  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is  great anniversary present  fav song    I love the way you love me    my husband loves to listen to it in the car\n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "index = 0\n",
        "import re\n",
        "for index in range(len(reviews)):\n",
        "  print(\"Review is \",reviews[index])\n",
        "  print(\"Sentiment \", sentiment_ratings[index])\n",
        "  print(\"Helpfullness is \", helpfulness_ratings[index])\n",
        "  print(\"-----------\")\n",
        "  if(index >5):\n",
        "    break;\n",
        "# helpfulness_ratings[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class neutral: 10321 (28.24%)\n",
            "Class helpful: 20351 (55.68%)\n",
            "Class unhelpful: 5876 (16.08%)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Class Distribution Analysis \n",
        "class_distribution = Counter(helpfulness_ratings)\n",
        "total_data = len(helpfulness_ratings)\n",
        "\n",
        "\n",
        "for rating, count in class_distribution.items():\n",
        "    percentage = (count / total_data) * 100\n",
        "    print(f\"Class {rating}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(class_distribution.keys(), class_distribution.values())\n",
        "plt.xlabel('Helpfulness Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution in Full Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Fc8KrpQhYV"
      },
      "source": [
        "**Tokenising Corpus Dataset**\n",
        "<br/>\n",
        "Tokenising the words all the words in the reviews database. This will then be used to create the list of features, which would be the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TjHzxsgNNLH4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "# Tokenising it by spaces\n",
        "tokenised_set = []\n",
        "for review in reviews:\n",
        "  # Basically, re.split(' ') results in an array of words split by spaces\n",
        "  # Then iterate through that array of words and append it individually to tokenised_set\n",
        "  [tokenised_set.append(tokens) for tokens in re.split(' ', review)]\n",
        "\n",
        "counts = Counter(tokenised_set)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "so=list(zip(*so))[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "QxtldfjdV_7h",
        "outputId": "c82341d6-4514-435a-e15e-c40d1e7eb15f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('',\n",
              " 'the',\n",
              " 'I',\n",
              " 'and',\n",
              " 'to',\n",
              " 'a',\n",
              " 'it',\n",
              " 'of',\n",
              " 'is',\n",
              " 'this',\n",
              " 'in',\n",
              " 'for',\n",
              " 'that',\n",
              " 'you',\n",
              " 'with',\n",
              " 'on',\n",
              " 's',\n",
              " 'have',\n",
              " 'was',\n",
              " 'The',\n",
              " 'my',\n",
              " 'not',\n",
              " 't',\n",
              " 'but',\n",
              " 'are',\n",
              " 'as',\n",
              " 'be',\n",
              " 'It',\n",
              " 'one',\n",
              " 'so',\n",
              " 'or',\n",
              " 'can',\n",
              " 'they',\n",
              " 'all',\n",
              " 'at',\n",
              " 'like',\n",
              " 'This',\n",
              " 'very',\n",
              " 'from',\n",
              " 'just',\n",
              " 'out',\n",
              " 'would',\n",
              " 'has',\n",
              " 'an',\n",
              " 'up',\n",
              " 'about',\n",
              " 'had',\n",
              " 'more',\n",
              " 'your',\n",
              " 'get')"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "so[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We want to focus on content words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-EDhKyxxtMl"
      },
      "source": [
        "**Experiments with One Hot Encoders for Sentiment Analysis**\n",
        "<br>\n",
        "This google collab will experiemnt the effect of the amount of features, and the data split, and note their impact on different task for the helpfullness and sentiment analysis\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yW_OPROzSFl"
      },
      "source": [
        "Experiment for 8000 features, meaning 8000 words\n",
        "<br>\n",
        "Below is the code for one-hot encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zmY7X110Afxk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# 5000 Features\n",
        "word_list = so[0:8000]\n",
        "M = np.zeros((len(reviews), len(word_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "  for(j,word) in enumerate(word_list):\n",
        "    if word in rev:\n",
        "      M[i,j]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kYLzeduFGGW",
        "outputId": "69e38a55-6b0c-4f54-b0d4-ee60f8863776"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36548, 8000)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 1., 1., ..., 0., 0., 0.],\n",
              "       [1., 1., 1., ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W589yFtz2d-1"
      },
      "outputs": [],
      "source": [
        "# Splitting, this data will be split with 60/20/20 split\n",
        "train_ints=np.random.choice(len(reviews),int(len(reviews)*0.6),replace=False)\n",
        "test_train_ints = list(set(range(0,len(reviews))) - set(train_ints))\n",
        "test_ints = np.random.choice(len(test_train_ints),int(len(test_train_ints)*0.5), replace=False)\n",
        "final_test_ints = list(set(range(0,len(test_train_ints))) - set(test_ints))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyLoWv1eAzLy",
        "outputId": "d574bda5-5466-431f-fb84-638320c552f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36548"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gczjgS1oAU_R",
        "outputId": "85f9c874-2b1f-443d-f956-35a366e9893d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training test 21928\n",
            "Validation test 7310\n",
            "Final test 7310\n",
            "Total 36548\n"
          ]
        }
      ],
      "source": [
        "print(\"Training test\", len(train_ints))\n",
        "print(\"Validation test\", len(test_ints))\n",
        "print(\"Final test\", len(final_test_ints))\n",
        "print(\"Total\", len(train_ints)+len(test_ints)+len(final_test_ints))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "D5zci15pCO7t"
      },
      "outputs": [],
      "source": [
        "# Divide the features by the training indices\n",
        "# Select all rows that are in the indices of the respective lists and select all the rows\n",
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "M_final_test = M[final_test_ints,]\n",
        "sentiment_labels = [sentiment_ratings[i] for i in train_ints]\n",
        "sentiment_labels_test = [sentiment_ratings[i] for i in test_ints]\n",
        "sentiment_labels_final_test = [sentiment_ratings[i] for i in final_test_ints]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUfBV2eCHOeK",
        "outputId": "dad11401-36e1-4801-85f1-7b57602092fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7310, 8000)\n",
            "(21928, 8000)\n",
            "(7310, 8000)\n",
            "21928\n",
            "7310\n",
            "7310\n"
          ]
        }
      ],
      "source": [
        "print(M_test.shape)\n",
        "print(M_train.shape)\n",
        "print(M_final_test.shape)\n",
        "print(len(sentiment_labels))\n",
        "print(len(sentiment_labels_test))\n",
        "print(len(sentiment_labels_final_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "weights = np.random.rand(5000)\n",
        "print(weights.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "DU1rI6YNIKW-",
        "outputId": "b0d4e33a-7266-4c37-c59e-8c0fce971b45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'loss')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_features=8000\n",
        "y=[int(l == \"positive\") for l in sentiment_labels]\n",
        "weights = np.random.rand(num_features)\n",
        "bias=np.random.rand(1)\n",
        "n_iters = 1000\n",
        "lr=0.4\n",
        "logistic_loss=[]\n",
        "num_samples=len(y)\n",
        "for i in range(n_iters):\n",
        "  # Basically you are multiplying all the values of M_train with the weights\n",
        "  # It would be similar to this: z= bias + (x[0]*weights[0] + x[1]*weights[1])\n",
        "  # The values here would be 21928, 5000 and 5000, 1, leading to a matrix of 21928, 1\n",
        "  z= M_train.dot(weights) + bias\n",
        "  # print(z)\n",
        "  # (1 / (1+np.exp(-z))) we use sigmoid because we only need to know whether it is positive or negative, two possible values\n",
        "  q = (1 / (1+np.exp(-z)))\n",
        "  # print(q)\n",
        "  eps=0.00001\n",
        "  loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
        "  logistic_loss.append(loss)\n",
        "  # We then make the prediction, if it is below a certain number, 0.5 it is negative and vice versa\n",
        "  y_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "  # For logistic regression one shot encoder= dw1 = np.dot(x[0],q-y)/num_samples\n",
        "  # dw1 = np.do(x[0], q-y)/num_samples\n",
        "\n",
        "  dw = (q-y).dot(M_train)/num_samples\n",
        "  db = sum(q-y)/num_samples\n",
        "  weights = weights - dw*lr\n",
        "  bias = bias - db*lr\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00sFwtxFFXW8"
      },
      "outputs": [],
      "source": [
        "z= M_final_test.dot(weights) + bias\n",
        "q = (1 / (1+np.exp(-z)))\n",
        "y_test_pred=[int(ql > 0.5) for ql in q]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5Yk3gyESARC"
      },
      "source": [
        "**Result for One Hot Encoding**\n",
        "<br>\n",
        "1.  Features: 5000\n",
        "2.  Binary classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtlLYPv7FseX",
        "outputId": "53132742-4ba5-4c21-9b29-5b7c68fb4f3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8374829001367989\n"
          ]
        }
      ],
      "source": [
        "# y_test=[int(l == \"positive\") for l in sentiment_labels_test]\n",
        "# acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "# print(sum(acc_test)/len(acc_test))\n",
        "\n",
        "y_final_test = [int(l=='positive') for l in sentiment_labels_final_test]\n",
        "acc_test = [int(yp == y_final_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "print(sum(acc_test)/len(acc_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKDkT2LuLg_C",
        "outputId": "07e631ba-cb3f-4ab5-9925-bdbe17909407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8000\n",
            "(7310, 8000)\n",
            "(7310, 8000)\n",
            "21928\n",
            "7310\n",
            "7310\n"
          ]
        }
      ],
      "source": [
        "print(M_train.shape[1])\n",
        "print(M_test.shape)\n",
        "print(M_final_test.shape)\n",
        "print(len(sentiment_labels))\n",
        "print(len(sentiment_labels_test))\n",
        "print(len(sentiment_labels_final_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0EWsUqSISS4w",
        "outputId": "131b2a8b-f50d-4f61-b361-34c8048c212c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: Logistic Loss = 24558.918428858506\n",
            "Iteration 2: Logistic Loss = 101881.35362248306\n",
            "Iteration 3: Logistic Loss = 81837.543389697\n",
            "Iteration 4: Logistic Loss = 27596.288586720628\n",
            "Iteration 5: Logistic Loss = 21939.70685950046\n",
            "Iteration 6: Logistic Loss = 21681.243840503877\n",
            "Iteration 7: Logistic Loss = 21621.35396885721\n",
            "Iteration 8: Logistic Loss = 21580.28623374102\n",
            "Iteration 9: Logistic Loss = 21536.481287248305\n",
            "Iteration 10: Logistic Loss = 21488.57023756181\n",
            "Iteration 11: Logistic Loss = 21438.76741058728\n",
            "Iteration 12: Logistic Loss = 21389.11169088152\n",
            "Iteration 13: Logistic Loss = 21339.478848569117\n",
            "Iteration 14: Logistic Loss = 21288.65340035851\n",
            "Iteration 15: Logistic Loss = 21236.168263659398\n",
            "Iteration 16: Logistic Loss = 21181.46540524477\n",
            "Iteration 17: Logistic Loss = 21123.671030001053\n",
            "Iteration 18: Logistic Loss = 21062.72738111124\n",
            "Iteration 19: Logistic Loss = 20998.723133035728\n",
            "Iteration 20: Logistic Loss = 20931.495684494064\n",
            "Iteration 21: Logistic Loss = 20861.282786231634\n",
            "Iteration 22: Logistic Loss = 20788.49315652067\n",
            "Iteration 23: Logistic Loss = 20713.184501678188\n",
            "Iteration 24: Logistic Loss = 20634.987027572482\n",
            "Iteration 25: Logistic Loss = 20553.52598528517\n",
            "Iteration 26: Logistic Loss = 20468.63704648528\n",
            "Iteration 27: Logistic Loss = 20380.739386097346\n",
            "Iteration 28: Logistic Loss = 20290.271696086074\n",
            "Iteration 29: Logistic Loss = 20197.323576929404\n",
            "Iteration 30: Logistic Loss = 20101.993552915417\n",
            "Iteration 31: Logistic Loss = 20003.793222397922\n",
            "Iteration 32: Logistic Loss = 19902.139984156624\n",
            "Iteration 33: Logistic Loss = 19797.209047777505\n",
            "Iteration 34: Logistic Loss = 19689.009578923396\n",
            "Iteration 35: Logistic Loss = 19577.158084205566\n",
            "Iteration 36: Logistic Loss = 19462.22685609701\n",
            "Iteration 37: Logistic Loss = 19343.914803101216\n",
            "Iteration 38: Logistic Loss = 19222.142331782026\n",
            "Iteration 39: Logistic Loss = 19097.196061436644\n",
            "Iteration 40: Logistic Loss = 18969.78287574322\n",
            "Iteration 41: Logistic Loss = 18841.098471248075\n",
            "Iteration 42: Logistic Loss = 18714.69875473787\n",
            "Iteration 43: Logistic Loss = 18606.257797340975\n",
            "Iteration 44: Logistic Loss = 18598.950877521766\n",
            "Iteration 45: Logistic Loss = 19141.74458734372\n",
            "Iteration 46: Logistic Loss = 22445.091788025966\n",
            "Iteration 47: Logistic Loss = 31100.61286564063\n",
            "Iteration 48: Logistic Loss = 32001.209385163616\n",
            "Iteration 49: Logistic Loss = 20343.268363734955\n",
            "Iteration 50: Logistic Loss = 19403.595444010098\n",
            "Iteration 51: Logistic Loss = 19032.03758944194\n",
            "Iteration 52: Logistic Loss = 18788.267186182147\n",
            "Iteration 53: Logistic Loss = 18591.350680232397\n",
            "Iteration 54: Logistic Loss = 18416.122408832314\n",
            "Iteration 55: Logistic Loss = 18255.51078472889\n",
            "Iteration 56: Logistic Loss = 18106.713794972697\n",
            "Iteration 57: Logistic Loss = 17973.330667064416\n",
            "Iteration 58: Logistic Loss = 17877.04818453281\n",
            "Iteration 59: Logistic Loss = 17881.895705528044\n",
            "Iteration 60: Logistic Loss = 18248.382428985067\n",
            "Iteration 61: Logistic Loss = 19670.137887663233\n",
            "Iteration 62: Logistic Loss = 24082.453238583694\n",
            "Iteration 63: Logistic Loss = 26719.97546112866\n",
            "Iteration 64: Logistic Loss = 25547.34944270061\n",
            "Iteration 65: Logistic Loss = 19498.67621923291\n",
            "Iteration 66: Logistic Loss = 18626.180282051875\n",
            "Iteration 67: Logistic Loss = 18154.65428069678\n",
            "Iteration 68: Logistic Loss = 17917.77873213866\n",
            "Iteration 69: Logistic Loss = 17737.27794410471\n",
            "Iteration 70: Logistic Loss = 17653.651325908704\n",
            "Iteration 71: Logistic Loss = 17662.064987034995\n",
            "Iteration 72: Logistic Loss = 17972.172107909973\n",
            "Iteration 73: Logistic Loss = 18713.224141571543\n",
            "Iteration 74: Logistic Loss = 20760.72494045351\n",
            "Iteration 75: Logistic Loss = 22428.772524175085\n",
            "Iteration 76: Logistic Loss = 24050.981307116243\n",
            "Iteration 77: Logistic Loss = 20398.975404748584\n",
            "Iteration 78: Logistic Loss = 19409.51500641393\n",
            "Iteration 79: Logistic Loss = 18238.27505200266\n",
            "Iteration 80: Logistic Loss = 17965.636965027003\n",
            "Iteration 81: Logistic Loss = 17693.197865750844\n",
            "Iteration 82: Logistic Loss = 17723.96560141872\n",
            "Iteration 83: Logistic Loss = 17799.435522177493\n",
            "Iteration 84: Logistic Loss = 18343.461059295914\n",
            "Iteration 85: Logistic Loss = 18949.136342574442\n",
            "Iteration 86: Logistic Loss = 20397.49801226502\n",
            "Iteration 87: Logistic Loss = 20586.38651592362\n",
            "Iteration 88: Logistic Loss = 21137.523086034045\n",
            "Iteration 89: Logistic Loss = 19461.923350945974\n",
            "Iteration 90: Logistic Loss = 19025.039320089185\n",
            "Iteration 91: Logistic Loss = 18126.7442371071\n",
            "Iteration 92: Logistic Loss = 17987.11038593603\n",
            "Iteration 93: Logistic Loss = 17712.56222730552\n",
            "Iteration 94: Logistic Loss = 17880.04069620144\n",
            "Iteration 95: Logistic Loss = 17947.8449522963\n",
            "Iteration 96: Logistic Loss = 18540.417032631252\n",
            "Iteration 97: Logistic Loss = 18816.435258076137\n",
            "Iteration 98: Logistic Loss = 19672.08823013524\n",
            "Iteration 99: Logistic Loss = 19403.96397395291\n",
            "Iteration 100: Logistic Loss = 19631.249125728224\n",
            "Iteration 101: Logistic Loss = 18716.675303962427\n",
            "Iteration 102: Logistic Loss = 18562.102142909564\n",
            "Iteration 103: Logistic Loss = 17956.42394804358\n",
            "Iteration 104: Logistic Loss = 17947.982194821052\n",
            "Iteration 105: Logistic Loss = 17711.84392732755\n",
            "Iteration 106: Logistic Loss = 17947.5807079618\n",
            "Iteration 107: Logistic Loss = 17941.7224436722\n",
            "Iteration 108: Logistic Loss = 18423.16785529583\n",
            "Iteration 109: Logistic Loss = 18437.673107548562\n",
            "Iteration 110: Logistic Loss = 18915.196684925177\n",
            "Iteration 111: Logistic Loss = 18596.50421548064\n",
            "Iteration 112: Logistic Loss = 18747.551851439657\n",
            "Iteration 113: Logistic Loss = 18194.1384146181\n",
            "Iteration 114: Logistic Loss = 18183.101734616692\n",
            "Iteration 115: Logistic Loss = 17767.240464409675\n",
            "Iteration 116: Logistic Loss = 17838.365525853766\n",
            "Iteration 117: Logistic Loss = 17631.21659754429\n",
            "Iteration 118: Logistic Loss = 17852.864299965622\n",
            "Iteration 119: Logistic Loss = 17773.501735099162\n",
            "Iteration 120: Logistic Loss = 18110.692863977136\n",
            "Iteration 121: Logistic Loss = 18011.518298035917\n",
            "Iteration 122: Logistic Loss = 18299.845119445832\n",
            "Iteration 123: Logistic Loss = 18035.96014127631\n",
            "Iteration 124: Logistic Loss = 18161.443556035294\n",
            "Iteration 125: Logistic Loss = 17801.48592047118\n",
            "Iteration 126: Logistic Loss = 17852.659417240076\n",
            "Iteration 127: Logistic Loss = 17556.2079161007\n",
            "Iteration 128: Logistic Loss = 17653.50411067026\n",
            "Iteration 129: Logistic Loss = 17468.85423321991\n",
            "Iteration 130: Logistic Loss = 17650.931207547117\n",
            "Iteration 131: Logistic Loss = 17533.37443628005\n",
            "Iteration 132: Logistic Loss = 17760.789981592763\n",
            "Iteration 133: Logistic Loss = 17629.27517213636\n",
            "Iteration 134: Logistic Loss = 17821.83259510572\n",
            "Iteration 135: Logistic Loss = 17613.305714765796\n",
            "Iteration 136: Logistic Loss = 17725.64742554611\n",
            "Iteration 137: Logistic Loss = 17473.828632510358\n",
            "Iteration 138: Logistic Loss = 17545.539183623543\n",
            "Iteration 139: Logistic Loss = 17321.142063715823\n",
            "Iteration 140: Logistic Loss = 17414.439095910326\n",
            "Iteration 141: Logistic Loss = 17248.649266030116\n",
            "Iteration 142: Logistic Loss = 17384.73653830508\n",
            "Iteration 143: Logistic Loss = 17258.566826959697\n",
            "Iteration 144: Logistic Loss = 17416.884620928216\n",
            "Iteration 145: Logistic Loss = 17287.876139590997\n",
            "Iteration 146: Logistic Loss = 17422.129576542982\n",
            "Iteration 147: Logistic Loss = 17257.790497638085\n",
            "Iteration 148: Logistic Loss = 17353.518193704527\n",
            "Iteration 149: Logistic Loss = 17169.132688742502\n",
            "Iteration 150: Logistic Loss = 17242.395613902234\n",
            "Iteration 151: Logistic Loss = 17069.94283779535\n",
            "Iteration 152: Logistic Loss = 17151.063637707073\n",
            "Iteration 153: Logistic Loss = 17009.520182576227\n",
            "Iteration 154: Logistic Loss = 17109.97273316459\n",
            "Iteration 155: Logistic Loss = 16989.92019573924\n",
            "Iteration 156: Logistic Loss = 17099.00382904296\n",
            "Iteration 157: Logistic Loss = 16980.339721715103\n",
            "Iteration 158: Logistic Loss = 17077.694061373444\n",
            "Iteration 159: Logistic Loss = 16946.413239795893\n",
            "Iteration 160: Logistic Loss = 17025.809829026744\n",
            "Iteration 161: Logistic Loss = 16887.075628370578\n",
            "Iteration 162: Logistic Loss = 16950.311804081517\n",
            "Iteration 163: Logistic Loss = 16815.618201179583\n",
            "Iteration 164: Logistic Loss = 16880.79532298374\n",
            "Iteration 165: Logistic Loss = 16761.472718305078\n",
            "Iteration 166: Logistic Loss = 16833.798605782635\n",
            "Iteration 167: Logistic Loss = 16728.496740573446\n",
            "Iteration 168: Logistic Loss = 16802.003722209574\n",
            "Iteration 169: Logistic Loss = 16701.106099431738\n",
            "Iteration 170: Logistic Loss = 16766.3310791275\n",
            "Iteration 171: Logistic Loss = 16665.652439982296\n",
            "Iteration 172: Logistic Loss = 16716.615569757436\n",
            "Iteration 173: Logistic Loss = 16614.41411671287\n",
            "Iteration 174: Logistic Loss = 16661.47512015705\n",
            "Iteration 175: Logistic Loss = 16564.1021877646\n",
            "Iteration 176: Logistic Loss = 16608.845290777586\n",
            "Iteration 177: Logistic Loss = 16519.432825006206\n",
            "Iteration 178: Logistic Loss = 16561.339424673366\n",
            "Iteration 179: Logistic Loss = 16477.84094612039\n",
            "Iteration 180: Logistic Loss = 16520.58067042451\n",
            "Iteration 181: Logistic Loss = 16440.371766454096\n",
            "Iteration 182: Logistic Loss = 16481.54875226229\n",
            "Iteration 183: Logistic Loss = 16402.55545709809\n",
            "Iteration 184: Logistic Loss = 16439.820775403274\n",
            "Iteration 185: Logistic Loss = 16362.735014236636\n",
            "Iteration 186: Logistic Loss = 16395.640989722993\n",
            "Iteration 187: Logistic Loss = 16322.43024078299\n",
            "Iteration 188: Logistic Loss = 16351.003591334826\n",
            "Iteration 189: Logistic Loss = 16279.390329950933\n",
            "Iteration 190: Logistic Loss = 16303.770903413915\n",
            "Iteration 191: Logistic Loss = 16236.160768794574\n",
            "Iteration 192: Logistic Loss = 16258.67254469646\n",
            "Iteration 193: Logistic Loss = 16193.474023263278\n",
            "Iteration 194: Logistic Loss = 16213.39142697898\n",
            "Iteration 195: Logistic Loss = 16151.63593115493\n",
            "Iteration 196: Logistic Loss = 16166.347211347713\n",
            "Iteration 197: Logistic Loss = 16110.742767064803\n",
            "Iteration 198: Logistic Loss = 16122.685156074127\n",
            "Iteration 199: Logistic Loss = 16073.28687106286\n",
            "Iteration 200: Logistic Loss = 16074.699592356315\n",
            "Iteration 201: Logistic Loss = 16025.121542777011\n",
            "Iteration 202: Logistic Loss = 16025.476929200786\n",
            "Iteration 203: Logistic Loss = 15982.410472588075\n",
            "Iteration 204: Logistic Loss = 15986.231992584393\n",
            "Iteration 205: Logistic Loss = 15951.120972958208\n",
            "Iteration 206: Logistic Loss = 15946.608272768644\n",
            "Iteration 207: Logistic Loss = 15912.909237577833\n",
            "Iteration 208: Logistic Loss = 15903.252600127887\n",
            "Iteration 209: Logistic Loss = 15873.58128226902\n",
            "Iteration 210: Logistic Loss = 15855.635577244157\n",
            "Iteration 211: Logistic Loss = 15825.412016973845\n",
            "Iteration 212: Logistic Loss = 15802.900390456714\n",
            "Iteration 213: Logistic Loss = 15775.744232565765\n",
            "Iteration 214: Logistic Loss = 15755.465976237614\n",
            "Iteration 215: Logistic Loss = 15731.746381411685\n",
            "Iteration 216: Logistic Loss = 15715.779044433362\n",
            "Iteration 217: Logistic Loss = 15700.692216443651\n",
            "Iteration 218: Logistic Loss = 15681.667182436428\n",
            "Iteration 219: Logistic Loss = 15663.904980451061\n",
            "Iteration 220: Logistic Loss = 15639.43615773172\n",
            "Iteration 221: Logistic Loss = 15621.989150960631\n",
            "Iteration 222: Logistic Loss = 15596.796507190289\n",
            "Iteration 223: Logistic Loss = 15580.013067467946\n",
            "Iteration 224: Logistic Loss = 15543.30742643552\n",
            "Iteration 225: Logistic Loss = 15526.257132321298\n",
            "Iteration 226: Logistic Loss = 15493.534863257188\n",
            "Iteration 227: Logistic Loss = 15483.534631667975\n",
            "Iteration 228: Logistic Loss = 15452.573707871925\n",
            "Iteration 229: Logistic Loss = 15446.961344120706\n",
            "Iteration 230: Logistic Loss = 15419.255484026846\n",
            "Iteration 231: Logistic Loss = 15413.780559274255\n",
            "Iteration 232: Logistic Loss = 15390.193440692321\n",
            "Iteration 233: Logistic Loss = 15386.13374715487\n",
            "Iteration 234: Logistic Loss = 15358.887977169386\n",
            "Iteration 235: Logistic Loss = 15352.30863162084\n",
            "Iteration 236: Logistic Loss = 15313.680887595692\n",
            "Iteration 237: Logistic Loss = 15300.174731186215\n",
            "Iteration 238: Logistic Loss = 15259.383867147522\n",
            "Iteration 239: Logistic Loss = 15251.351147230997\n",
            "Iteration 240: Logistic Loss = 15215.821887373322\n",
            "Iteration 241: Logistic Loss = 15213.654829224446\n",
            "Iteration 242: Logistic Loss = 15189.817556414446\n",
            "Iteration 243: Logistic Loss = 15193.478709702966\n",
            "Iteration 244: Logistic Loss = 15172.732763319498\n",
            "Iteration 245: Logistic Loss = 15175.818740127212\n",
            "Iteration 246: Logistic Loss = 15145.465035051902\n",
            "Iteration 247: Logistic Loss = 15133.10054891777\n",
            "Iteration 248: Logistic Loss = 15092.095846376327\n",
            "Iteration 249: Logistic Loss = 15073.001691224215\n",
            "Iteration 250: Logistic Loss = 15039.426719216397\n",
            "Iteration 251: Logistic Loss = 15032.167070945543\n",
            "Iteration 252: Logistic Loss = 15004.925836216065\n",
            "Iteration 253: Logistic Loss = 15005.711994918061\n",
            "Iteration 254: Logistic Loss = 14979.168606321236\n",
            "Iteration 255: Logistic Loss = 14973.375673802397\n",
            "Iteration 256: Logistic Loss = 14946.792233278924\n",
            "Iteration 257: Logistic Loss = 14937.660861858334\n",
            "Iteration 258: Logistic Loss = 14913.42480189697\n",
            "Iteration 259: Logistic Loss = 14903.859953339146\n",
            "Iteration 260: Logistic Loss = 14879.973783376288\n",
            "Iteration 261: Logistic Loss = 14865.387766745293\n",
            "Iteration 262: Logistic Loss = 14845.612280514539\n",
            "Iteration 263: Logistic Loss = 14832.05889672634\n",
            "Iteration 264: Logistic Loss = 14816.638077093794\n",
            "Iteration 265: Logistic Loss = 14799.65845397975\n",
            "Iteration 266: Logistic Loss = 14783.659582286933\n",
            "Iteration 267: Logistic Loss = 14766.496661704901\n",
            "Iteration 268: Logistic Loss = 14755.6008917332\n",
            "Iteration 269: Logistic Loss = 14737.251576329636\n",
            "Iteration 270: Logistic Loss = 14728.206823360195\n",
            "Iteration 271: Logistic Loss = 14710.558702464166\n",
            "Iteration 272: Logistic Loss = 14705.032102821811\n",
            "Iteration 273: Logistic Loss = 14685.33426372043\n",
            "Iteration 274: Logistic Loss = 14677.093706528962\n",
            "Iteration 275: Logistic Loss = 14649.167942522121\n",
            "Iteration 276: Logistic Loss = 14639.350773293092\n",
            "Iteration 277: Logistic Loss = 14610.475839116672\n",
            "Iteration 278: Logistic Loss = 14605.118245975227\n",
            "Iteration 279: Logistic Loss = 14581.093521279872\n",
            "Iteration 280: Logistic Loss = 14575.195662919985\n",
            "Iteration 281: Logistic Loss = 14547.528141972933\n",
            "Iteration 282: Logistic Loss = 14549.035936220476\n",
            "Iteration 283: Logistic Loss = 14524.199853547176\n",
            "Iteration 284: Logistic Loss = 14526.145880955377\n",
            "Iteration 285: Logistic Loss = 14501.95594677821\n",
            "Iteration 286: Logistic Loss = 14499.520721309636\n",
            "Iteration 287: Logistic Loss = 14467.05200222723\n",
            "Iteration 288: Logistic Loss = 14464.021652704781\n",
            "Iteration 289: Logistic Loss = 14426.178057468069\n",
            "Iteration 290: Logistic Loss = 14427.651409813441\n",
            "Iteration 291: Logistic Loss = 14392.309667679428\n",
            "Iteration 292: Logistic Loss = 14402.673677794994\n",
            "Iteration 293: Logistic Loss = 14369.759290646878\n",
            "Iteration 294: Logistic Loss = 14385.67446948275\n",
            "Iteration 295: Logistic Loss = 14349.778092373668\n",
            "Iteration 296: Logistic Loss = 14368.779323192179\n",
            "Iteration 297: Logistic Loss = 14326.624711174381\n",
            "Iteration 298: Logistic Loss = 14346.44991126551\n",
            "Iteration 299: Logistic Loss = 14296.192199872277\n",
            "Iteration 300: Logistic Loss = 14312.676794422949\n",
            "Iteration 301: Logistic Loss = 14254.977228988846\n",
            "Iteration 302: Logistic Loss = 14273.14702592904\n",
            "Iteration 303: Logistic Loss = 14221.198557339932\n",
            "Iteration 304: Logistic Loss = 14248.11606828557\n",
            "Iteration 305: Logistic Loss = 14200.077082476997\n",
            "Iteration 306: Logistic Loss = 14231.081678895316\n",
            "Iteration 307: Logistic Loss = 14179.248359918718\n",
            "Iteration 308: Logistic Loss = 14213.833266133115\n",
            "Iteration 309: Logistic Loss = 14156.174719979776\n",
            "Iteration 310: Logistic Loss = 14190.217259212895\n",
            "Iteration 311: Logistic Loss = 14128.372963291957\n",
            "Iteration 312: Logistic Loss = 14160.080308199455\n",
            "Iteration 313: Logistic Loss = 14089.497742093798\n",
            "Iteration 314: Logistic Loss = 14125.109958794385\n",
            "Iteration 315: Logistic Loss = 14062.866198448346\n",
            "Iteration 316: Logistic Loss = 14103.93827580362\n",
            "Iteration 317: Logistic Loss = 14044.960005544315\n",
            "Iteration 318: Logistic Loss = 14088.003261497704\n",
            "Iteration 319: Logistic Loss = 14025.328417632412\n",
            "Iteration 320: Logistic Loss = 14072.499477731728\n",
            "Iteration 321: Logistic Loss = 14004.32440728425\n",
            "Iteration 322: Logistic Loss = 14050.013114467387\n",
            "Iteration 323: Logistic Loss = 13977.139752624447\n",
            "Iteration 324: Logistic Loss = 14028.8746193131\n",
            "Iteration 325: Logistic Loss = 13952.631670673472\n",
            "Iteration 326: Logistic Loss = 14005.575617152945\n",
            "Iteration 327: Logistic Loss = 13924.782290383457\n",
            "Iteration 328: Logistic Loss = 13980.751403540631\n",
            "Iteration 329: Logistic Loss = 13901.137585926599\n",
            "Iteration 330: Logistic Loss = 13962.486908125737\n",
            "Iteration 331: Logistic Loss = 13887.26235151516\n",
            "Iteration 332: Logistic Loss = 13950.281995175541\n",
            "Iteration 333: Logistic Loss = 13871.414107564986\n",
            "Iteration 334: Logistic Loss = 13929.343783404865\n",
            "Iteration 335: Logistic Loss = 13842.29322456057\n",
            "Iteration 336: Logistic Loss = 13894.424942044605\n",
            "Iteration 337: Logistic Loss = 13809.324925573152\n",
            "Iteration 338: Logistic Loss = 13864.216846535779\n",
            "Iteration 339: Logistic Loss = 13783.446720339058\n",
            "Iteration 340: Logistic Loss = 13842.496703070297\n",
            "Iteration 341: Logistic Loss = 13766.975593904062\n",
            "Iteration 342: Logistic Loss = 13831.800675375835\n",
            "Iteration 343: Logistic Loss = 13755.29450354776\n",
            "Iteration 344: Logistic Loss = 13819.783660237837\n",
            "Iteration 345: Logistic Loss = 13736.130557092834\n",
            "Iteration 346: Logistic Loss = 13797.14368083447\n",
            "Iteration 347: Logistic Loss = 13706.15196882491\n",
            "Iteration 348: Logistic Loss = 13764.566459303289\n",
            "Iteration 349: Logistic Loss = 13672.742782660871\n",
            "Iteration 350: Logistic Loss = 13735.14182776966\n",
            "Iteration 351: Logistic Loss = 13646.81309126554\n",
            "Iteration 352: Logistic Loss = 13714.84350157369\n",
            "Iteration 353: Logistic Loss = 13626.23734261488\n",
            "Iteration 354: Logistic Loss = 13697.280921489997\n",
            "Iteration 355: Logistic Loss = 13610.998520609348\n",
            "Iteration 356: Logistic Loss = 13683.995808134558\n",
            "Iteration 357: Logistic Loss = 13594.847993637937\n",
            "Iteration 358: Logistic Loss = 13663.392358715118\n",
            "Iteration 359: Logistic Loss = 13569.272789793555\n",
            "Iteration 360: Logistic Loss = 13641.200864474838\n",
            "Iteration 361: Logistic Loss = 13549.923388067316\n",
            "Iteration 362: Logistic Loss = 13622.419422104784\n",
            "Iteration 363: Logistic Loss = 13529.02688793849\n",
            "Iteration 364: Logistic Loss = 13600.804814893152\n",
            "Iteration 365: Logistic Loss = 13506.75589560455\n",
            "Iteration 366: Logistic Loss = 13579.84429055554\n",
            "Iteration 367: Logistic Loss = 13486.833350743884\n",
            "Iteration 368: Logistic Loss = 13562.075390291862\n",
            "Iteration 369: Logistic Loss = 13467.99588028678\n",
            "Iteration 370: Logistic Loss = 13542.961822667809\n",
            "Iteration 371: Logistic Loss = 13446.649471481858\n",
            "Iteration 372: Logistic Loss = 13520.165496842577\n",
            "Iteration 373: Logistic Loss = 13423.26626544038\n",
            "Iteration 374: Logistic Loss = 13498.835204917686\n",
            "Iteration 375: Logistic Loss = 13404.268292990198\n",
            "Iteration 376: Logistic Loss = 13483.500801167182\n",
            "Iteration 377: Logistic Loss = 13394.274550512282\n",
            "Iteration 378: Logistic Loss = 13477.614658526038\n",
            "Iteration 379: Logistic Loss = 13386.414722746913\n",
            "Iteration 380: Logistic Loss = 13468.296399810522\n",
            "Iteration 381: Logistic Loss = 13368.629043387835\n",
            "Iteration 382: Logistic Loss = 13451.048848745952\n",
            "Iteration 383: Logistic Loss = 13354.238582398519\n",
            "Iteration 384: Logistic Loss = 13435.068249835982\n",
            "Iteration 385: Logistic Loss = 13333.600711570867\n",
            "Iteration 386: Logistic Loss = 13412.0961871568\n",
            "Iteration 387: Logistic Loss = 13307.121055726635\n",
            "Iteration 388: Logistic Loss = 13390.18249092833\n",
            "Iteration 389: Logistic Loss = 13288.873842470919\n",
            "Iteration 390: Logistic Loss = 13376.518282675715\n",
            "Iteration 391: Logistic Loss = 13277.137283689684\n",
            "Iteration 392: Logistic Loss = 13366.43544176667\n",
            "Iteration 393: Logistic Loss = 13266.041772162194\n",
            "Iteration 394: Logistic Loss = 13356.264388091615\n",
            "Iteration 395: Logistic Loss = 13253.06097546541\n",
            "Iteration 396: Logistic Loss = 13334.138754065396\n",
            "Iteration 397: Logistic Loss = 13225.900928850348\n",
            "Iteration 398: Logistic Loss = 13309.962839930911\n",
            "Iteration 399: Logistic Loss = 13206.067416459584\n",
            "Iteration 400: Logistic Loss = 13292.695395140403\n",
            "Iteration 401: Logistic Loss = 13196.72520446401\n",
            "Iteration 402: Logistic Loss = 13287.227864833789\n",
            "Iteration 403: Logistic Loss = 13188.860151031367\n",
            "Iteration 404: Logistic Loss = 13276.774138572733\n",
            "Iteration 405: Logistic Loss = 13176.312547694955\n",
            "Iteration 406: Logistic Loss = 13258.828687074923\n",
            "Iteration 407: Logistic Loss = 13156.06356524595\n",
            "Iteration 408: Logistic Loss = 13236.77070309942\n",
            "Iteration 409: Logistic Loss = 13133.73172263181\n",
            "Iteration 410: Logistic Loss = 13218.004221696836\n",
            "Iteration 411: Logistic Loss = 13115.315792696896\n",
            "Iteration 412: Logistic Loss = 13204.536074601872\n",
            "Iteration 413: Logistic Loss = 13104.763540841439\n",
            "Iteration 414: Logistic Loss = 13197.788855360712\n",
            "Iteration 415: Logistic Loss = 13095.536912268748\n",
            "Iteration 416: Logistic Loss = 13183.261987242593\n",
            "Iteration 417: Logistic Loss = 13078.617922376361\n",
            "Iteration 418: Logistic Loss = 13167.357280193184\n",
            "Iteration 419: Logistic Loss = 13063.701526557501\n",
            "Iteration 420: Logistic Loss = 13151.361302259927\n",
            "Iteration 421: Logistic Loss = 13050.806190364192\n",
            "Iteration 422: Logistic Loss = 13141.404079888935\n",
            "Iteration 423: Logistic Loss = 13046.702525157802\n",
            "Iteration 424: Logistic Loss = 13138.520349175331\n",
            "Iteration 425: Logistic Loss = 13039.023734774284\n",
            "Iteration 426: Logistic Loss = 13128.701970028895\n",
            "Iteration 427: Logistic Loss = 13025.742977769512\n",
            "Iteration 428: Logistic Loss = 13112.73878987237\n",
            "Iteration 429: Logistic Loss = 13006.884350750139\n",
            "Iteration 430: Logistic Loss = 13092.757023178121\n",
            "Iteration 431: Logistic Loss = 12987.035176823796\n",
            "Iteration 432: Logistic Loss = 13073.539873933965\n",
            "Iteration 433: Logistic Loss = 12970.879415025845\n",
            "Iteration 434: Logistic Loss = 13062.076057786608\n",
            "Iteration 435: Logistic Loss = 12961.10808474637\n",
            "Iteration 436: Logistic Loss = 13053.743201490644\n",
            "Iteration 437: Logistic Loss = 12951.119780334142\n",
            "Iteration 438: Logistic Loss = 13045.277411050585\n",
            "Iteration 439: Logistic Loss = 12941.068161779065\n",
            "Iteration 440: Logistic Loss = 13031.337788297196\n",
            "Iteration 441: Logistic Loss = 12925.932334730547\n",
            "Iteration 442: Logistic Loss = 13014.261344040444\n",
            "Iteration 443: Logistic Loss = 12908.480131965709\n",
            "Iteration 444: Logistic Loss = 13000.550948633336\n",
            "Iteration 445: Logistic Loss = 12896.108244756026\n",
            "Iteration 446: Logistic Loss = 12990.905557609516\n",
            "Iteration 447: Logistic Loss = 12886.605546304927\n",
            "Iteration 448: Logistic Loss = 12979.326986952017\n",
            "Iteration 449: Logistic Loss = 12878.747178042315\n",
            "Iteration 450: Logistic Loss = 12969.73486564574\n",
            "Iteration 451: Logistic Loss = 12861.469236818299\n",
            "Iteration 452: Logistic Loss = 12950.655093841866\n",
            "Iteration 453: Logistic Loss = 12843.920469909059\n",
            "Iteration 454: Logistic Loss = 12935.046247639428\n",
            "Iteration 455: Logistic Loss = 12830.628114717798\n",
            "Iteration 456: Logistic Loss = 12921.475203223963\n",
            "Iteration 457: Logistic Loss = 12815.87940788846\n",
            "Iteration 458: Logistic Loss = 12907.23623741432\n",
            "Iteration 459: Logistic Loss = 12802.000451133965\n",
            "Iteration 460: Logistic Loss = 12893.588192170027\n",
            "Iteration 461: Logistic Loss = 12790.090980530342\n",
            "Iteration 462: Logistic Loss = 12885.988452539268\n",
            "Iteration 463: Logistic Loss = 12779.574046539732\n",
            "Iteration 464: Logistic Loss = 12867.957165461692\n",
            "Iteration 465: Logistic Loss = 12766.280929422275\n",
            "Iteration 466: Logistic Loss = 12857.62457570537\n",
            "Iteration 467: Logistic Loss = 12755.547531170723\n",
            "Iteration 468: Logistic Loss = 12846.326740403965\n",
            "Iteration 469: Logistic Loss = 12737.687475029012\n",
            "Iteration 470: Logistic Loss = 12827.093833375196\n",
            "Iteration 471: Logistic Loss = 12720.848941767563\n",
            "Iteration 472: Logistic Loss = 12813.765355874448\n",
            "Iteration 473: Logistic Loss = 12712.125371433136\n",
            "Iteration 474: Logistic Loss = 12807.163876323582\n",
            "Iteration 475: Logistic Loss = 12705.912809681568\n",
            "Iteration 476: Logistic Loss = 12803.499006553844\n",
            "Iteration 477: Logistic Loss = 12700.446472883197\n",
            "Iteration 478: Logistic Loss = 12793.870854130953\n",
            "Iteration 479: Logistic Loss = 12685.840002938236\n",
            "Iteration 480: Logistic Loss = 12776.792762113322\n",
            "Iteration 481: Logistic Loss = 12666.944752096595\n",
            "Iteration 482: Logistic Loss = 12756.819222919312\n",
            "Iteration 483: Logistic Loss = 12641.127254145675\n",
            "Iteration 484: Logistic Loss = 12736.483520074746\n",
            "Iteration 485: Logistic Loss = 12628.230070001091\n",
            "Iteration 486: Logistic Loss = 12728.997764321717\n",
            "Iteration 487: Logistic Loss = 12622.20545964963\n",
            "Iteration 488: Logistic Loss = 12722.91057257762\n",
            "Iteration 489: Logistic Loss = 12619.966204889566\n",
            "Iteration 490: Logistic Loss = 12716.248156219972\n",
            "Iteration 491: Logistic Loss = 12605.640413764002\n",
            "Iteration 492: Logistic Loss = 12702.981068400682\n",
            "Iteration 493: Logistic Loss = 12600.113540962266\n",
            "Iteration 494: Logistic Loss = 12696.036513762487\n",
            "Iteration 495: Logistic Loss = 12581.871220322533\n",
            "Iteration 496: Logistic Loss = 12676.20115063198\n",
            "Iteration 497: Logistic Loss = 12564.04710461909\n",
            "Iteration 498: Logistic Loss = 12660.967252694027\n",
            "Iteration 499: Logistic Loss = 12552.933790193307\n",
            "Iteration 500: Logistic Loss = 12651.469374898243\n",
            "Iteration 501: Logistic Loss = 12542.718413068538\n",
            "Iteration 502: Logistic Loss = 12644.764490284851\n",
            "Iteration 503: Logistic Loss = 12543.08694519557\n",
            "Iteration 504: Logistic Loss = 12645.153164662379\n",
            "Iteration 505: Logistic Loss = 12536.775710518075\n",
            "Iteration 506: Logistic Loss = 12639.790117381675\n",
            "Iteration 507: Logistic Loss = 12525.522971708588\n",
            "Iteration 508: Logistic Loss = 12627.485916441616\n",
            "Iteration 509: Logistic Loss = 12519.89029589155\n",
            "Iteration 510: Logistic Loss = 12621.6017177093\n",
            "Iteration 511: Logistic Loss = 12502.00471736046\n",
            "Iteration 512: Logistic Loss = 12600.73087055996\n",
            "Iteration 513: Logistic Loss = 12482.633552408424\n",
            "Iteration 514: Logistic Loss = 12582.055179251229\n",
            "Iteration 515: Logistic Loss = 12468.088545304221\n",
            "Iteration 516: Logistic Loss = 12565.380918259047\n",
            "Iteration 517: Logistic Loss = 12452.845754833077\n",
            "Iteration 518: Logistic Loss = 12549.909940172625\n",
            "Iteration 519: Logistic Loss = 12436.99276924573\n",
            "Iteration 520: Logistic Loss = 12534.053986075356\n",
            "Iteration 521: Logistic Loss = 12425.889987069033\n",
            "Iteration 522: Logistic Loss = 12528.650451427759\n",
            "Iteration 523: Logistic Loss = 12424.518218243431\n",
            "Iteration 524: Logistic Loss = 12529.107178765222\n",
            "Iteration 525: Logistic Loss = 12425.83060643106\n",
            "Iteration 526: Logistic Loss = 12528.671500157485\n",
            "Iteration 527: Logistic Loss = 12414.058295893325\n",
            "Iteration 528: Logistic Loss = 12510.526211428425\n",
            "Iteration 529: Logistic Loss = 12394.420338345833\n",
            "Iteration 530: Logistic Loss = 12488.622938148677\n",
            "Iteration 531: Logistic Loss = 12373.37694468255\n",
            "Iteration 532: Logistic Loss = 12467.145233573747\n",
            "Iteration 533: Logistic Loss = 12362.842049707331\n",
            "Iteration 534: Logistic Loss = 12459.160668288532\n",
            "Iteration 535: Logistic Loss = 12349.477707247275\n",
            "Iteration 536: Logistic Loss = 12446.014433785702\n",
            "Iteration 537: Logistic Loss = 12350.07260479806\n",
            "Iteration 538: Logistic Loss = 12447.540801445886\n",
            "Iteration 539: Logistic Loss = 12338.368865845454\n",
            "Iteration 540: Logistic Loss = 12435.706319595789\n",
            "Iteration 541: Logistic Loss = 12341.452547475394\n",
            "Iteration 542: Logistic Loss = 12434.246122154045\n",
            "Iteration 543: Logistic Loss = 12320.920548117529\n",
            "Iteration 544: Logistic Loss = 12415.35878924877\n",
            "Iteration 545: Logistic Loss = 12312.692265563885\n",
            "Iteration 546: Logistic Loss = 12410.536945636912\n",
            "Iteration 547: Logistic Loss = 12311.059530426084\n",
            "Iteration 548: Logistic Loss = 12409.482624820468\n",
            "Iteration 549: Logistic Loss = 12304.738087967202\n",
            "Iteration 550: Logistic Loss = 12387.893486513583\n",
            "Iteration 551: Logistic Loss = 12269.554704731927\n",
            "Iteration 552: Logistic Loss = 12357.588065080934\n",
            "Iteration 553: Logistic Loss = 12248.651252700656\n",
            "Iteration 554: Logistic Loss = 12341.989059587819\n",
            "Iteration 555: Logistic Loss = 12238.557740670487\n",
            "Iteration 556: Logistic Loss = 12334.449369683567\n",
            "Iteration 557: Logistic Loss = 12233.12749214012\n",
            "Iteration 558: Logistic Loss = 12334.219003544771\n",
            "Iteration 559: Logistic Loss = 12236.463353843188\n",
            "Iteration 560: Logistic Loss = 12326.686522906828\n",
            "Iteration 561: Logistic Loss = 12212.98278932405\n",
            "Iteration 562: Logistic Loss = 12307.374871130767\n",
            "Iteration 563: Logistic Loss = 12201.402821611417\n",
            "Iteration 564: Logistic Loss = 12294.696359743457\n",
            "Iteration 565: Logistic Loss = 12189.009253644443\n",
            "Iteration 566: Logistic Loss = 12280.453933708148\n",
            "Iteration 567: Logistic Loss = 12175.61270760302\n",
            "Iteration 568: Logistic Loss = 12270.322874438147\n",
            "Iteration 569: Logistic Loss = 12170.1542951593\n",
            "Iteration 570: Logistic Loss = 12266.218837572109\n",
            "Iteration 571: Logistic Loss = 12168.989812525539\n",
            "Iteration 572: Logistic Loss = 12260.524478240019\n",
            "Iteration 573: Logistic Loss = 12153.277884025974\n",
            "Iteration 574: Logistic Loss = 12241.294432734954\n",
            "Iteration 575: Logistic Loss = 12140.667852051753\n",
            "Iteration 576: Logistic Loss = 12232.435688217885\n",
            "Iteration 577: Logistic Loss = 12131.484118385717\n",
            "Iteration 578: Logistic Loss = 12224.160882717677\n",
            "Iteration 579: Logistic Loss = 12122.917714987787\n",
            "Iteration 580: Logistic Loss = 12212.959440475852\n",
            "Iteration 581: Logistic Loss = 12111.610135801366\n",
            "Iteration 582: Logistic Loss = 12202.62800181664\n",
            "Iteration 583: Logistic Loss = 12096.792629277568\n",
            "Iteration 584: Logistic Loss = 12186.027375966645\n",
            "Iteration 585: Logistic Loss = 12080.055323532659\n",
            "Iteration 586: Logistic Loss = 12173.449546138465\n",
            "Iteration 587: Logistic Loss = 12066.207850394738\n",
            "Iteration 588: Logistic Loss = 12157.176653493336\n",
            "Iteration 589: Logistic Loss = 12048.379609986672\n",
            "Iteration 590: Logistic Loss = 12142.842088777057\n",
            "Iteration 591: Logistic Loss = 12044.165648242102\n",
            "Iteration 592: Logistic Loss = 12144.806282688027\n",
            "Iteration 593: Logistic Loss = 12043.224091518521\n",
            "Iteration 594: Logistic Loss = 12143.072861027977\n",
            "Iteration 595: Logistic Loss = 12043.653284354677\n",
            "Iteration 596: Logistic Loss = 12144.678404259985\n",
            "Iteration 597: Logistic Loss = 12043.626842249803\n",
            "Iteration 598: Logistic Loss = 12140.691547903101\n",
            "Iteration 599: Logistic Loss = 12027.899581181591\n",
            "Iteration 600: Logistic Loss = 12120.919127182857\n",
            "Iteration 601: Logistic Loss = 12018.74727321844\n",
            "Iteration 602: Logistic Loss = 12110.292173645239\n",
            "Iteration 603: Logistic Loss = 11998.280714112627\n",
            "Iteration 604: Logistic Loss = 12091.814700108569\n",
            "Iteration 605: Logistic Loss = 11987.910172636046\n",
            "Iteration 606: Logistic Loss = 12079.772065992012\n",
            "Iteration 607: Logistic Loss = 11974.977064723067\n",
            "Iteration 608: Logistic Loss = 12068.951737152634\n",
            "Iteration 609: Logistic Loss = 11967.984531870328\n",
            "Iteration 610: Logistic Loss = 12061.365387685733\n",
            "Iteration 611: Logistic Loss = 11957.777091028609\n",
            "Iteration 612: Logistic Loss = 12056.176743596901\n",
            "Iteration 613: Logistic Loss = 11959.864574653697\n",
            "Iteration 614: Logistic Loss = 12058.039428252709\n",
            "Iteration 615: Logistic Loss = 11948.55108897009\n",
            "Iteration 616: Logistic Loss = 12041.759345189188\n",
            "Iteration 617: Logistic Loss = 11939.098297544497\n",
            "Iteration 618: Logistic Loss = 12028.516316345678\n",
            "Iteration 619: Logistic Loss = 11916.2757779517\n",
            "Iteration 620: Logistic Loss = 12007.828748566222\n",
            "Iteration 621: Logistic Loss = 11901.592493110671\n",
            "Iteration 622: Logistic Loss = 11994.083229740023\n",
            "Iteration 623: Logistic Loss = 11889.836819597731\n",
            "Iteration 624: Logistic Loss = 11982.885841394509\n",
            "Iteration 625: Logistic Loss = 11883.766527247153\n",
            "Iteration 626: Logistic Loss = 11977.901780314365\n",
            "Iteration 627: Logistic Loss = 11878.444988041314\n",
            "Iteration 628: Logistic Loss = 11974.8025427849\n",
            "Iteration 629: Logistic Loss = 11880.471870366327\n",
            "Iteration 630: Logistic Loss = 11977.21063426273\n",
            "Iteration 631: Logistic Loss = 11880.222100255076\n",
            "Iteration 632: Logistic Loss = 11974.640023821521\n",
            "Iteration 633: Logistic Loss = 11863.296685935235\n",
            "Iteration 634: Logistic Loss = 11955.71127981027\n",
            "Iteration 635: Logistic Loss = 11853.784856755618\n",
            "Iteration 636: Logistic Loss = 11944.610718664791\n",
            "Iteration 637: Logistic Loss = 11836.568960311895\n",
            "Iteration 638: Logistic Loss = 11927.246846085269\n",
            "Iteration 639: Logistic Loss = 11824.387737599138\n",
            "Iteration 640: Logistic Loss = 11916.939177895281\n",
            "Iteration 641: Logistic Loss = 11812.337042038722\n",
            "Iteration 642: Logistic Loss = 11903.422997638289\n",
            "Iteration 643: Logistic Loss = 11803.9778903588\n",
            "Iteration 644: Logistic Loss = 11894.270274076514\n",
            "Iteration 645: Logistic Loss = 11796.05610850139\n",
            "Iteration 646: Logistic Loss = 11893.383962841392\n",
            "Iteration 647: Logistic Loss = 11794.751761121079\n",
            "Iteration 648: Logistic Loss = 11889.04418880011\n",
            "Iteration 649: Logistic Loss = 11791.301660935718\n",
            "Iteration 650: Logistic Loss = 11883.95758440406\n",
            "Iteration 651: Logistic Loss = 11784.511516542618\n",
            "Iteration 652: Logistic Loss = 11873.641825962204\n",
            "Iteration 653: Logistic Loss = 11767.728903678677\n",
            "Iteration 654: Logistic Loss = 11856.542703498057\n",
            "Iteration 655: Logistic Loss = 11754.115481815816\n",
            "Iteration 656: Logistic Loss = 11840.545172908835\n",
            "Iteration 657: Logistic Loss = 11737.8808814524\n",
            "Iteration 658: Logistic Loss = 11828.671973313561\n",
            "Iteration 659: Logistic Loss = 11731.35553007758\n",
            "Iteration 660: Logistic Loss = 11825.147625419846\n",
            "Iteration 661: Logistic Loss = 11735.237325011673\n",
            "Iteration 662: Logistic Loss = 11829.697452566374\n",
            "Iteration 663: Logistic Loss = 11733.902670238036\n",
            "Iteration 664: Logistic Loss = 11829.578217371029\n",
            "Iteration 665: Logistic Loss = 11737.70748416828\n",
            "Iteration 666: Logistic Loss = 11827.405684511914\n",
            "Iteration 667: Logistic Loss = 11715.710755242155\n",
            "Iteration 668: Logistic Loss = 11804.187743429755\n",
            "Iteration 669: Logistic Loss = 11707.128730027312\n",
            "Iteration 670: Logistic Loss = 11792.653632152378\n",
            "Iteration 671: Logistic Loss = 11685.134324590754\n",
            "Iteration 672: Logistic Loss = 11770.304954415986\n",
            "Iteration 673: Logistic Loss = 11661.198659168229\n",
            "Iteration 674: Logistic Loss = 11741.219651107716\n",
            "Iteration 675: Logistic Loss = 11646.527065066297\n",
            "Iteration 676: Logistic Loss = 11730.507214795147\n",
            "Iteration 677: Logistic Loss = 11636.7066495392\n",
            "Iteration 678: Logistic Loss = 11719.401374770478\n",
            "Iteration 679: Logistic Loss = 11627.660672373227\n",
            "Iteration 680: Logistic Loss = 11714.46355941043\n",
            "Iteration 681: Logistic Loss = 11626.02522825288\n",
            "Iteration 682: Logistic Loss = 11717.0401575966\n",
            "Iteration 683: Logistic Loss = 11630.941860820241\n",
            "Iteration 684: Logistic Loss = 11726.79357301293\n",
            "Iteration 685: Logistic Loss = 11649.27970926173\n",
            "Iteration 686: Logistic Loss = 11740.518617168098\n",
            "Iteration 687: Logistic Loss = 11646.731895421488\n",
            "Iteration 688: Logistic Loss = 11728.142714244705\n",
            "Iteration 689: Logistic Loss = 11626.507760059658\n",
            "Iteration 690: Logistic Loss = 11699.268822660812\n",
            "Iteration 691: Logistic Loss = 11598.19572446253\n",
            "Iteration 692: Logistic Loss = 11676.253769627594\n",
            "Iteration 693: Logistic Loss = 11575.270898817307\n",
            "Iteration 694: Logistic Loss = 11654.67940645185\n",
            "Iteration 695: Logistic Loss = 11564.90284393884\n",
            "Iteration 696: Logistic Loss = 11644.57363662538\n",
            "Iteration 697: Logistic Loss = 11551.124323879403\n",
            "Iteration 698: Logistic Loss = 11633.115239918305\n",
            "Iteration 699: Logistic Loss = 11545.837556157065\n",
            "Iteration 700: Logistic Loss = 11626.281333451825\n",
            "Iteration 701: Logistic Loss = 11539.299883056025\n",
            "Iteration 702: Logistic Loss = 11625.650637965911\n",
            "Iteration 703: Logistic Loss = 11547.624735699079\n",
            "Iteration 704: Logistic Loss = 11632.54263994874\n",
            "Iteration 705: Logistic Loss = 11545.488099558319\n",
            "Iteration 706: Logistic Loss = 11630.84366505254\n",
            "Iteration 707: Logistic Loss = 11548.09604091056\n",
            "Iteration 708: Logistic Loss = 11630.96835491346\n",
            "Iteration 709: Logistic Loss = 11536.734432895355\n",
            "Iteration 710: Logistic Loss = 11618.333380852502\n",
            "Iteration 711: Logistic Loss = 11528.633444605559\n",
            "Iteration 712: Logistic Loss = 11604.443347334482\n",
            "Iteration 713: Logistic Loss = 11506.34724746041\n",
            "Iteration 714: Logistic Loss = 11587.917563127003\n",
            "Iteration 715: Logistic Loss = 11500.733670132046\n",
            "Iteration 716: Logistic Loss = 11572.856655105927\n",
            "Iteration 717: Logistic Loss = 11479.65592834043\n",
            "Iteration 718: Logistic Loss = 11556.363147077918\n",
            "Iteration 719: Logistic Loss = 11469.950648797443\n",
            "Iteration 720: Logistic Loss = 11542.308233845497\n",
            "Iteration 721: Logistic Loss = 11457.010469461993\n",
            "Iteration 722: Logistic Loss = 11534.820696685221\n",
            "Iteration 723: Logistic Loss = 11453.172346450221\n",
            "Iteration 724: Logistic Loss = 11528.246162909265\n",
            "Iteration 725: Logistic Loss = 11450.113939400775\n",
            "Iteration 726: Logistic Loss = 11536.224596153032\n",
            "Iteration 727: Logistic Loss = 11460.771954999702\n",
            "Iteration 728: Logistic Loss = 11540.305411627589\n",
            "Iteration 729: Logistic Loss = 11459.384173859315\n",
            "Iteration 730: Logistic Loss = 11531.427030251944\n",
            "Iteration 731: Logistic Loss = 11446.538721649169\n",
            "Iteration 732: Logistic Loss = 11514.912434877222\n",
            "Iteration 733: Logistic Loss = 11423.300367089529\n",
            "Iteration 734: Logistic Loss = 11494.637648505472\n",
            "Iteration 735: Logistic Loss = 11412.662363695912\n",
            "Iteration 736: Logistic Loss = 11481.577247669116\n",
            "Iteration 737: Logistic Loss = 11386.648815130153\n",
            "Iteration 738: Logistic Loss = 11457.808688297999\n",
            "Iteration 739: Logistic Loss = 11373.4566621636\n",
            "Iteration 740: Logistic Loss = 11443.249240656174\n",
            "Iteration 741: Logistic Loss = 11355.775605319985\n",
            "Iteration 742: Logistic Loss = 11424.506166655832\n",
            "Iteration 743: Logistic Loss = 11342.761365633445\n",
            "Iteration 744: Logistic Loss = 11417.499574432291\n",
            "Iteration 745: Logistic Loss = 11333.0577134459\n",
            "Iteration 746: Logistic Loss = 11412.25689573653\n",
            "Iteration 747: Logistic Loss = 11336.52946824329\n",
            "Iteration 748: Logistic Loss = 11421.132436115624\n",
            "Iteration 749: Logistic Loss = 11357.14825200494\n",
            "Iteration 750: Logistic Loss = 11445.834831036062\n",
            "Iteration 751: Logistic Loss = 11375.978823479341\n",
            "Iteration 752: Logistic Loss = 11460.843164286143\n",
            "Iteration 753: Logistic Loss = 11382.432786481804\n",
            "Iteration 754: Logistic Loss = 11459.79345592062\n",
            "Iteration 755: Logistic Loss = 11368.324928812002\n",
            "Iteration 756: Logistic Loss = 11438.654910039899\n",
            "Iteration 757: Logistic Loss = 11345.176596374742\n",
            "Iteration 758: Logistic Loss = 11421.61657116557\n",
            "Iteration 759: Logistic Loss = 11330.834817614454\n",
            "Iteration 760: Logistic Loss = 11400.905097046334\n",
            "Iteration 761: Logistic Loss = 11303.160251914507\n",
            "Iteration 762: Logistic Loss = 11380.585189685364\n",
            "Iteration 763: Logistic Loss = 11292.504767834223\n",
            "Iteration 764: Logistic Loss = 11370.404340999854\n",
            "Iteration 765: Logistic Loss = 11274.249480120696\n",
            "Iteration 766: Logistic Loss = 11352.62409655572\n",
            "Iteration 767: Logistic Loss = 11267.770405227224\n",
            "Iteration 768: Logistic Loss = 11345.282495965595\n",
            "Iteration 769: Logistic Loss = 11260.451479927573\n",
            "Iteration 770: Logistic Loss = 11342.471105286653\n",
            "Iteration 771: Logistic Loss = 11263.74717798484\n",
            "Iteration 772: Logistic Loss = 11346.804757571303\n",
            "Iteration 773: Logistic Loss = 11271.648992647095\n",
            "Iteration 774: Logistic Loss = 11351.198471137704\n",
            "Iteration 775: Logistic Loss = 11279.756087034952\n",
            "Iteration 776: Logistic Loss = 11365.34932846036\n",
            "Iteration 777: Logistic Loss = 11279.404959402998\n",
            "Iteration 778: Logistic Loss = 11360.83060621213\n",
            "Iteration 779: Logistic Loss = 11284.144229678004\n",
            "Iteration 780: Logistic Loss = 11364.489458190932\n",
            "Iteration 781: Logistic Loss = 11273.828631081007\n",
            "Iteration 782: Logistic Loss = 11349.629093433869\n",
            "Iteration 783: Logistic Loss = 11260.237391877155\n",
            "Iteration 784: Logistic Loss = 11331.568695248756\n",
            "Iteration 785: Logistic Loss = 11238.416456183868\n",
            "Iteration 786: Logistic Loss = 11312.188888417697\n",
            "Iteration 787: Logistic Loss = 11224.864064695419\n",
            "Iteration 788: Logistic Loss = 11300.313122863423\n",
            "Iteration 789: Logistic Loss = 11210.44230069535\n",
            "Iteration 790: Logistic Loss = 11283.792965153305\n",
            "Iteration 791: Logistic Loss = 11199.03537597771\n",
            "Iteration 792: Logistic Loss = 11275.369107979845\n",
            "Iteration 793: Logistic Loss = 11180.988145706822\n",
            "Iteration 794: Logistic Loss = 11253.104579966805\n",
            "Iteration 795: Logistic Loss = 11167.878469645579\n",
            "Iteration 796: Logistic Loss = 11239.398144532719\n",
            "Iteration 797: Logistic Loss = 11145.049403058161\n",
            "Iteration 798: Logistic Loss = 11215.247767808665\n",
            "Iteration 799: Logistic Loss = 11126.273149123324\n",
            "Iteration 800: Logistic Loss = 11190.521856428086\n",
            "Iteration 801: Logistic Loss = 11095.68812473211\n",
            "Iteration 802: Logistic Loss = 11157.888768370736\n",
            "Iteration 803: Logistic Loss = 11079.079402116899\n",
            "Iteration 804: Logistic Loss = 11150.476785664021\n",
            "Iteration 805: Logistic Loss = 11074.494832526712\n",
            "Iteration 806: Logistic Loss = 11147.383190967466\n",
            "Iteration 807: Logistic Loss = 11088.917462465783\n",
            "Iteration 808: Logistic Loss = 11175.321821443109\n",
            "Iteration 809: Logistic Loss = 11113.247134754783\n",
            "Iteration 810: Logistic Loss = 11209.43660348056\n",
            "Iteration 811: Logistic Loss = 11166.303711933313\n",
            "Iteration 812: Logistic Loss = 11260.210037821937\n",
            "Iteration 813: Logistic Loss = 11182.311896228868\n",
            "Iteration 814: Logistic Loss = 11262.739579212473\n",
            "Iteration 815: Logistic Loss = 11192.189116669706\n",
            "Iteration 816: Logistic Loss = 11264.942863817088\n",
            "Iteration 817: Logistic Loss = 11152.139679395017\n",
            "Iteration 818: Logistic Loss = 11208.822328091646\n",
            "Iteration 819: Logistic Loss = 11133.388306918903\n",
            "Iteration 820: Logistic Loss = 11197.8933407033\n",
            "Iteration 821: Logistic Loss = 11094.511287194942\n",
            "Iteration 822: Logistic Loss = 11153.318064227391\n",
            "Iteration 823: Logistic Loss = 11085.541887922063\n",
            "Iteration 824: Logistic Loss = 11152.46447581875\n",
            "Iteration 825: Logistic Loss = 11050.282665915682\n",
            "Iteration 826: Logistic Loss = 11110.887427229614\n",
            "Iteration 827: Logistic Loss = 11045.24402415249\n",
            "Iteration 828: Logistic Loss = 11107.860968658053\n",
            "Iteration 829: Logistic Loss = 11020.018197477042\n",
            "Iteration 830: Logistic Loss = 11082.538632897078\n",
            "Iteration 831: Logistic Loss = 11013.966817345525\n",
            "Iteration 832: Logistic Loss = 11073.957220633974\n",
            "Iteration 833: Logistic Loss = 11002.014585316372\n",
            "Iteration 834: Logistic Loss = 11068.449314462297\n",
            "Iteration 835: Logistic Loss = 11004.811862236993\n",
            "Iteration 836: Logistic Loss = 11069.945131271888\n",
            "Iteration 837: Logistic Loss = 11001.01522334516\n",
            "Iteration 838: Logistic Loss = 11074.95797175076\n",
            "Iteration 839: Logistic Loss = 11023.61051814697\n",
            "Iteration 840: Logistic Loss = 11103.665532653264\n",
            "Iteration 841: Logistic Loss = 11050.206020980911\n",
            "Iteration 842: Logistic Loss = 11124.13602175986\n",
            "Iteration 843: Logistic Loss = 11059.203375080686\n",
            "Iteration 844: Logistic Loss = 11127.608051623094\n",
            "Iteration 845: Logistic Loss = 11055.461111333665\n",
            "Iteration 846: Logistic Loss = 11114.10945045881\n",
            "Iteration 847: Logistic Loss = 11034.47908135647\n",
            "Iteration 848: Logistic Loss = 11086.7662458702\n",
            "Iteration 849: Logistic Loss = 10995.55864300664\n",
            "Iteration 850: Logistic Loss = 11057.642807674652\n",
            "Iteration 851: Logistic Loss = 10977.549801869009\n",
            "Iteration 852: Logistic Loss = 11035.571026279922\n",
            "Iteration 853: Logistic Loss = 10949.774812224174\n",
            "Iteration 854: Logistic Loss = 11014.61885518576\n",
            "Iteration 855: Logistic Loss = 10941.46054923458\n",
            "Iteration 856: Logistic Loss = 11004.700471764618\n",
            "Iteration 857: Logistic Loss = 10928.21759314129\n",
            "Iteration 858: Logistic Loss = 10993.246672381778\n",
            "Iteration 859: Logistic Loss = 10922.627404536026\n",
            "Iteration 860: Logistic Loss = 10986.440130182391\n",
            "Iteration 861: Logistic Loss = 10911.243302072548\n",
            "Iteration 862: Logistic Loss = 10977.10522852674\n",
            "Iteration 863: Logistic Loss = 10908.064522508574\n",
            "Iteration 864: Logistic Loss = 10974.093512925872\n",
            "Iteration 865: Logistic Loss = 10910.228888806942\n",
            "Iteration 866: Logistic Loss = 10983.863382547468\n",
            "Iteration 867: Logistic Loss = 10931.245379333533\n",
            "Iteration 868: Logistic Loss = 11006.355982902262\n",
            "Iteration 869: Logistic Loss = 10944.392358914356\n",
            "Iteration 870: Logistic Loss = 11012.919697299734\n",
            "Iteration 871: Logistic Loss = 10943.699167592169\n",
            "Iteration 872: Logistic Loss = 11013.36414406899\n",
            "Iteration 873: Logistic Loss = 10946.040706544982\n",
            "Iteration 874: Logistic Loss = 11005.076786336624\n",
            "Iteration 875: Logistic Loss = 10930.03103247599\n",
            "Iteration 876: Logistic Loss = 10990.349721684146\n",
            "Iteration 877: Logistic Loss = 10911.132402495066\n",
            "Iteration 878: Logistic Loss = 10964.515236973783\n",
            "Iteration 879: Logistic Loss = 10884.547418601083\n",
            "Iteration 880: Logistic Loss = 10935.847332507816\n",
            "Iteration 881: Logistic Loss = 10846.136700306024\n",
            "Iteration 882: Logistic Loss = 10898.965683370092\n",
            "Iteration 883: Logistic Loss = 10820.598393726239\n",
            "Iteration 884: Logistic Loss = 10866.573252261642\n",
            "Iteration 885: Logistic Loss = 10776.202473899359\n",
            "Iteration 886: Logistic Loss = 10830.23906888612\n",
            "Iteration 887: Logistic Loss = 10758.62297449368\n",
            "Iteration 888: Logistic Loss = 10808.979401748566\n",
            "Iteration 889: Logistic Loss = 10733.965862487239\n",
            "Iteration 890: Logistic Loss = 10788.343188470411\n",
            "Iteration 891: Logistic Loss = 10730.69239787664\n",
            "Iteration 892: Logistic Loss = 10787.55535322306\n",
            "Iteration 893: Logistic Loss = 10725.335379875927\n",
            "Iteration 894: Logistic Loss = 10794.500034708566\n",
            "Iteration 895: Logistic Loss = 10751.703950367191\n",
            "Iteration 896: Logistic Loss = 10826.473715653829\n",
            "Iteration 897: Logistic Loss = 10789.95955816421\n",
            "Iteration 898: Logistic Loss = 10882.409897637219\n",
            "Iteration 899: Logistic Loss = 10859.631616698764\n",
            "Iteration 900: Logistic Loss = 10947.07284587994\n",
            "Iteration 901: Logistic Loss = 10902.127732779247\n",
            "Iteration 902: Logistic Loss = 10970.933776885679\n",
            "Iteration 903: Logistic Loss = 10888.622775842743\n",
            "Iteration 904: Logistic Loss = 10940.23439303814\n",
            "Iteration 905: Logistic Loss = 10871.419112173076\n",
            "Iteration 906: Logistic Loss = 10907.004710759375\n",
            "Iteration 907: Logistic Loss = 10817.76462820137\n",
            "Iteration 908: Logistic Loss = 10866.45383715076\n",
            "Iteration 909: Logistic Loss = 10805.351678909523\n",
            "Iteration 910: Logistic Loss = 10853.781738685644\n",
            "Iteration 911: Logistic Loss = 10780.810683446829\n",
            "Iteration 912: Logistic Loss = 10840.989784986044\n",
            "Iteration 913: Logistic Loss = 10766.347863695306\n",
            "Iteration 914: Logistic Loss = 10824.602658294427\n",
            "Iteration 915: Logistic Loss = 10757.023868709457\n",
            "Iteration 916: Logistic Loss = 10819.723266893572\n",
            "Iteration 917: Logistic Loss = 10749.23883885663\n",
            "Iteration 918: Logistic Loss = 10804.854956581114\n",
            "Iteration 919: Logistic Loss = 10743.50013901237\n",
            "Iteration 920: Logistic Loss = 10802.383013945913\n",
            "Iteration 921: Logistic Loss = 10733.320397596037\n",
            "Iteration 922: Logistic Loss = 10788.949339679419\n",
            "Iteration 923: Logistic Loss = 10730.307698805625\n",
            "Iteration 924: Logistic Loss = 10785.832956188066\n",
            "Iteration 925: Logistic Loss = 10724.324100246225\n",
            "Iteration 926: Logistic Loss = 10783.702995136236\n",
            "Iteration 927: Logistic Loss = 10729.225196020372\n",
            "Iteration 928: Logistic Loss = 10788.626631534913\n",
            "Iteration 929: Logistic Loss = 10728.206480282015\n",
            "Iteration 930: Logistic Loss = 10791.730108564745\n",
            "Iteration 931: Logistic Loss = 10742.00103418473\n",
            "Iteration 932: Logistic Loss = 10801.752383871553\n",
            "Iteration 933: Logistic Loss = 10741.779118085653\n",
            "Iteration 934: Logistic Loss = 10806.232107608426\n",
            "Iteration 935: Logistic Loss = 10753.722189499926\n",
            "Iteration 936: Logistic Loss = 10810.314255128736\n",
            "Iteration 937: Logistic Loss = 10748.377750800846\n",
            "Iteration 938: Logistic Loss = 10801.665124643176\n",
            "Iteration 939: Logistic Loss = 10736.530441567671\n",
            "Iteration 940: Logistic Loss = 10782.945297513077\n",
            "Iteration 941: Logistic Loss = 10720.668588646155\n",
            "Iteration 942: Logistic Loss = 10765.508997718734\n",
            "Iteration 943: Logistic Loss = 10682.224169151656\n",
            "Iteration 944: Logistic Loss = 10720.14105109904\n",
            "Iteration 945: Logistic Loss = 10665.128414174034\n",
            "Iteration 946: Logistic Loss = 10702.698560279554\n",
            "Iteration 947: Logistic Loss = 10624.099116570795\n",
            "Iteration 948: Logistic Loss = 10658.588232805098\n",
            "Iteration 949: Logistic Loss = 10592.194064191437\n",
            "Iteration 950: Logistic Loss = 10628.848299624071\n",
            "Iteration 951: Logistic Loss = 10556.079210920198\n",
            "Iteration 952: Logistic Loss = 10587.285571678562\n",
            "Iteration 953: Logistic Loss = 10523.395379765647\n",
            "Iteration 954: Logistic Loss = 10562.476325190715\n",
            "Iteration 955: Logistic Loss = 10509.95509480984\n",
            "Iteration 956: Logistic Loss = 10549.407327350611\n",
            "Iteration 957: Logistic Loss = 10502.976327106264\n",
            "Iteration 958: Logistic Loss = 10549.202541429575\n",
            "Iteration 959: Logistic Loss = 10509.218614217312\n",
            "Iteration 960: Logistic Loss = 10560.99590695717\n",
            "Iteration 961: Logistic Loss = 10527.90788274331\n",
            "Iteration 962: Logistic Loss = 10594.225743842777\n",
            "Iteration 963: Logistic Loss = 10581.6632525674\n",
            "Iteration 964: Logistic Loss = 10660.619353265989\n",
            "Iteration 965: Logistic Loss = 10652.889835038488\n",
            "Iteration 966: Logistic Loss = 10727.84723795043\n",
            "Iteration 967: Logistic Loss = 10694.422130113573\n",
            "Iteration 968: Logistic Loss = 10763.547738708285\n",
            "Iteration 969: Logistic Loss = 10723.224926846131\n",
            "Iteration 970: Logistic Loss = 10746.53414028194\n",
            "Iteration 971: Logistic Loss = 10661.871445715296\n",
            "Iteration 972: Logistic Loss = 10690.670016983811\n",
            "Iteration 973: Logistic Loss = 10634.046402586126\n",
            "Iteration 974: Logistic Loss = 10659.85499288268\n",
            "Iteration 975: Logistic Loss = 10593.311658159586\n",
            "Iteration 976: Logistic Loss = 10637.120714113282\n",
            "Iteration 977: Logistic Loss = 10576.455570338541\n",
            "Iteration 978: Logistic Loss = 10624.873006835032\n",
            "Iteration 979: Logistic Loss = 10573.536041673666\n",
            "Iteration 980: Logistic Loss = 10625.556363753069\n",
            "Iteration 981: Logistic Loss = 10574.38431700425\n",
            "Iteration 982: Logistic Loss = 10618.133237938378\n",
            "Iteration 983: Logistic Loss = 10566.132255092069\n",
            "Iteration 984: Logistic Loss = 10611.081668583629\n",
            "Iteration 985: Logistic Loss = 10558.813991034953\n",
            "Iteration 986: Logistic Loss = 10595.146104078443\n",
            "Iteration 987: Logistic Loss = 10540.23417521641\n",
            "Iteration 988: Logistic Loss = 10585.463337287627\n",
            "Iteration 989: Logistic Loss = 10527.13267848787\n",
            "Iteration 990: Logistic Loss = 10561.994533045046\n",
            "Iteration 991: Logistic Loss = 10506.437467075666\n",
            "Iteration 992: Logistic Loss = 10545.669958791896\n",
            "Iteration 993: Logistic Loss = 10487.096714278323\n",
            "Iteration 994: Logistic Loss = 10529.82780904712\n",
            "Iteration 995: Logistic Loss = 10477.137571208772\n",
            "Iteration 996: Logistic Loss = 10516.612915736547\n",
            "Iteration 997: Logistic Loss = 10462.20844698072\n",
            "Iteration 998: Logistic Loss = 10506.845785133552\n",
            "Iteration 999: Logistic Loss = 10462.474175665797\n",
            "Iteration 1000: Logistic Loss = 10500.275391881343\n"
          ]
        }
      ],
      "source": [
        "# Add more layers for hte multilayer perceptron\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "learning_rate = 0.2\n",
        "\n",
        "num_features= M_train.shape[1]\n",
        "hidden_size = 32\n",
        "\n",
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).reshape(-1,1)\n",
        "np.random.seed(1)\n",
        "# Initialize weights_0_1 with positive values to encourage ReLU activation\n",
        "weights_0_1 = np.random.rand(num_features,hidden_size) * np.sqrt(2 / num_features)\n",
        "weights_1_2 = np.random.randn(hidden_size,1) * np.sqrt(2 / hidden_size)\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "n_iters=1000\n",
        "N = M_train.shape[0] # Number of training samples\n",
        "\n",
        "for iteration in range(n_iters):\n",
        "\n",
        "    layer_2_error = 0\n",
        "    layer_0 = M_train\n",
        "\n",
        "    ## Add forward pass\n",
        "    layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "    layer_2 = np.dot(layer_1,weights_1_2)\n",
        "   \n",
        "\n",
        "\n",
        "    # Then apply sigmoid\n",
        "    layer_2_s = 1/(1+np.exp(-layer_2))\n",
        "\n",
        "    # # Helped by AI chatgpt\n",
        "    eps = 1e-8\n",
        "    # # ----- BCE loss report -----\n",
        "    q = np.clip(layer_2_s, eps, 1 - eps)\n",
        "    # bce = -np.mean(true_labels * np.log(q) + (1 - true_labels) * np.log(1 - q))\n",
        "    # loss_history.append(bce)\n",
        "    # print(f\"Iteration {iteration+1}: BCE Loss = {bce}\")\n",
        "    loss = -np.sum(true_labels * np.log2(q) + (1 - true_labels) * np.log2(1 - q))\n",
        "    loss_history.append(loss)\n",
        "    print(f\"Iteration {iteration+1}: Logistic Loss = {loss}\")\n",
        "\n",
        "    ## Add backward pass and update weights\n",
        "    layer_2_diff = (layer_2_s - true_labels)\n",
        "\n",
        "    z1 = np.dot(layer_0, weights_0_1)\n",
        "    relu_grad = (z1 > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "    hidden_delta = np.dot(layer_2_diff, weights_1_2.T) * relu_grad\n",
        "\n",
        "    # Normalize weight updates by N\n",
        "    weights_1_2 -= learning_rate * (np.dot(layer_1.T, layer_2_diff) / N)\n",
        "    weights_0_1 -= learning_rate * (np.dot(layer_0.T, hidden_delta) / N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "23dca344",
        "outputId": "cd418c49-40b1-4835-b417-944a6f393e19"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'weights_0_1' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform forward pass on the final test data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m layer_0_final_test \u001b[38;5;241m=\u001b[39m M_final_test\n\u001b[0;32m----> 3\u001b[0m layer_1_final_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(np\u001b[38;5;241m.\u001b[39mdot(layer_0_final_test, \u001b[43mweights_0_1\u001b[49m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m layer_2_final_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(layer_1_final_test, weights_1_2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply sigmoid activation to get probabilities\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'weights_0_1' is not defined"
          ]
        }
      ],
      "source": [
        "# Perform forward pass on the final test data\n",
        "layer_0_final_test = M_final_test\n",
        "layer_1_final_test = np.maximum(np.dot(layer_0_final_test, weights_0_1), 0)\n",
        "layer_2_final_test = np.dot(layer_1_final_test, weights_1_2)\n",
        "\n",
        "# Apply sigmoid activation to get probabilities\n",
        "layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "# Convert probabilities to binary predictions (0 or 1)\n",
        "y_final_test_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "\n",
        "# Prepare true labels for the final test set\n",
        "true_labels_final_test = np.array([int(l == 'positive') for l in sentiment_labels_final_test]).reshape(-1, 1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_final_test_pred == true_labels_final_test)\n",
        "\n",
        "print(f\"Accuracy on the final test set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot the loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_history)\n",
        "plt.title('BCE Loss during Training')\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('BCE Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "t71Y0zmW-YwP",
        "outputId": "0c36c1e1-2e5e-44c8-ff5e-b514f29464ca"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'M_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-225728772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mM_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'M_train' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "learning_rate = 0.001\n",
        "\n",
        "num_features= M_train.shape[1]\n",
        "hidden_size = 32\n",
        "\n",
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).reshape(-1,1)\n",
        "np.random.seed(1)\n",
        "# Initialize weights_0_1 with positive values to encourage ReLU activation\n",
        "weights_0_1 = np.random.rand(num_features,hidden_size) * np.sqrt(2 / num_features)\n",
        "weights_1_2 = np.random.randn(hidden_size,1) * np.sqrt(2 / hidden_size)\n",
        "\n",
        "loss_history = []\n",
        "n_iters=1000\n",
        "N = M_train.shape[0] # Number of training samples\n",
        "\n",
        "for iteration in range(n_iters):\n",
        "\n",
        "    layer_2_error = 0\n",
        "\n",
        "    layer_0 = M_train\n",
        "\n",
        "    ## Add forward pass\n",
        "    layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "    layer_2 = np.dot(layer_1,weights_1_2)\n",
        "\n",
        "    # Then apply sigmoid\n",
        "    layer_2_s = 1/(1+np.exp(-layer_2))\n",
        "\n",
        "\n",
        "    eps = 1e-8\n",
        "    # ----- BCE loss report -----\n",
        "    q = np.clip(layer_2_s, eps, 1 - eps)\n",
        "    bce = -np.mean(true_labels * np.log(q) + (1 - true_labels) * np.log(1 - q))\n",
        "    loss_history.append(bce)\n",
        "    print(f\"Iteration {iteration+1}: BCE Loss = {bce}\")\n",
        "\n",
        "\n",
        "    ## Add backward pass and update weights\n",
        "    layer_2_diff = (layer_2_s - true_labels)\n",
        "\n",
        "\n",
        "    z1 = np.dot(layer_0, weights_0_1)\n",
        "    relu_grad = (z1 > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "    hidden_delta = np.dot(layer_2_diff, weights_1_2.T) * relu_grad\n",
        "\n",
        "    # Normalize weight updates by N\n",
        "    weights_1_2 -= learning_rate * (np.dot(layer_1.T, layer_2_diff) / N)\n",
        "    weights_0_1 -= learning_rate * (np.dot(layer_0.T, hidden_delta) / N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "eplGudosz53X"
      },
      "outputs": [],
      "source": [
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpfqtooD0JFV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Helpfulness Analysis** \n",
        "<br>\n",
        "- One Hot Encoding \n",
        "- Multi-Layer Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**One Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M4aBFxUK0GlK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('',\n",
              " 'the',\n",
              " 'I',\n",
              " 'and',\n",
              " 'to',\n",
              " 'a',\n",
              " 'it',\n",
              " 'of',\n",
              " 'is',\n",
              " 'this',\n",
              " 'in',\n",
              " 'for',\n",
              " 'that',\n",
              " 'you',\n",
              " 'with',\n",
              " 'on',\n",
              " 's',\n",
              " 'have',\n",
              " 'was',\n",
              " 'The',\n",
              " 'my',\n",
              " 'not',\n",
              " 't',\n",
              " 'but',\n",
              " 'are',\n",
              " 'as',\n",
              " 'be',\n",
              " 'It',\n",
              " 'one',\n",
              " 'so',\n",
              " 'or',\n",
              " 'can',\n",
              " 'they',\n",
              " 'all',\n",
              " 'at',\n",
              " 'like',\n",
              " 'This',\n",
              " 'very',\n",
              " 'from',\n",
              " 'just',\n",
              " 'out',\n",
              " 'would',\n",
              " 'has',\n",
              " 'an',\n",
              " 'up',\n",
              " 'about',\n",
              " 'had',\n",
              " 'more',\n",
              " 'your',\n",
              " 'get',\n",
              " 'me',\n",
              " 'if',\n",
              " 'will',\n",
              " 'good',\n",
              " 'great',\n",
              " 'when',\n",
              " 'them',\n",
              " 'only',\n",
              " 'time',\n",
              " 'use',\n",
              " 'than',\n",
              " 'by',\n",
              " 'no',\n",
              " 'do',\n",
              " 'which',\n",
              " 'other',\n",
              " 'some',\n",
              " 'what',\n",
              " 'product',\n",
              " 'really',\n",
              " 'there',\n",
              " 'because',\n",
              " 'well',\n",
              " 'much',\n",
              " 'these',\n",
              " 'even',\n",
              " 'don',\n",
              " 'he',\n",
              " 'also',\n",
              " 'too',\n",
              " 'game',\n",
              " 'were',\n",
              " 'been',\n",
              " 'who',\n",
              " 'we',\n",
              " 'his',\n",
              " 'If',\n",
              " 'their',\n",
              " 'first',\n",
              " 'any',\n",
              " 've',\n",
              " 'am',\n",
              " 'bought',\n",
              " 'after',\n",
              " 'i',\n",
              " 'better',\n",
              " 'book',\n",
              " 'over',\n",
              " 'little',\n",
              " 'm',\n",
              " 'back',\n",
              " 'work',\n",
              " 'My',\n",
              " 'could',\n",
              " 'her',\n",
              " 'does',\n",
              " 'then',\n",
              " 'off',\n",
              " 'buy',\n",
              " 'make',\n",
              " 'used',\n",
              " 'into',\n",
              " 'how',\n",
              " 'way',\n",
              " 'love',\n",
              " 'two',\n",
              " 'still',\n",
              " 'got',\n",
              " 'now',\n",
              " 'new',\n",
              " 'did',\n",
              " 'its',\n",
              " 'They',\n",
              " 'most',\n",
              " 'best',\n",
              " 'many',\n",
              " 'camera',\n",
              " 'You',\n",
              " 'find',\n",
              " 'think',\n",
              " 'want',\n",
              " 'quality',\n",
              " 'old',\n",
              " 'go',\n",
              " 'see',\n",
              " 'made',\n",
              " 'she',\n",
              " 'never',\n",
              " 'know',\n",
              " 'But',\n",
              " 'years',\n",
              " 'price',\n",
              " 'A',\n",
              " 'easy',\n",
              " 're',\n",
              " 'magazine',\n",
              " 'thing',\n",
              " 'same',\n",
              " 'people',\n",
              " 'read',\n",
              " 'few',\n",
              " 'We',\n",
              " '$',\n",
              " 'money',\n",
              " 'before',\n",
              " 'put',\n",
              " 'say',\n",
              " 'There',\n",
              " 'using',\n",
              " 'year',\n",
              " 'need',\n",
              " 'doesn',\n",
              " 'recommend',\n",
              " 'movie',\n",
              " 'found',\n",
              " 'should',\n",
              " 'down',\n",
              " 'didn',\n",
              " 'look',\n",
              " 'nice',\n",
              " 'right',\n",
              " 'every',\n",
              " 'again',\n",
              " 'take',\n",
              " 'through',\n",
              " 'long',\n",
              " 'enough',\n",
              " 'our',\n",
              " 'In',\n",
              " 'lot',\n",
              " 'day',\n",
              " 'And',\n",
              " 'while',\n",
              " 'another',\n",
              " 'around',\n",
              " 'without',\n",
              " 'problem',\n",
              " 'something',\n",
              " 'ever',\n",
              " 'set',\n",
              " 'since',\n",
              " 'those',\n",
              " 'where',\n",
              " 'So',\n",
              " 'small',\n",
              " 'll',\n",
              " 'works',\n",
              " 'being',\n",
              " 'play',\n",
              " 'different',\n",
              " 'going',\n",
              " 'looking',\n",
              " 'though',\n",
              " 'bad',\n",
              " 'hard',\n",
              " 'music',\n",
              " 'Amazon',\n",
              " 'makes',\n",
              " 'worth',\n",
              " 'times',\n",
              " 'phone',\n",
              " 'tried',\n",
              " 'purchased',\n",
              " 'keep',\n",
              " 'When',\n",
              " 'things',\n",
              " 'far',\n",
              " 'film',\n",
              " 'actually',\n",
              " 'months',\n",
              " 'give',\n",
              " 'last',\n",
              " 'always',\n",
              " 'bit',\n",
              " 'may',\n",
              " 'thought',\n",
              " 'pretty',\n",
              " 'both',\n",
              " 'own',\n",
              " 'such',\n",
              " 'watch',\n",
              " 'big',\n",
              " 'here',\n",
              " 'album',\n",
              " 'feel',\n",
              " 'light',\n",
              " 'try',\n",
              " 'him',\n",
              " 'item',\n",
              " 'come',\n",
              " 'd',\n",
              " 'size',\n",
              " 'fun',\n",
              " 'life',\n",
              " 'As',\n",
              " 'He',\n",
              " 'sure',\n",
              " 'hair',\n",
              " 'For',\n",
              " 'reviews',\n",
              " 'case',\n",
              " 'seems',\n",
              " 'each',\n",
              " 'sound',\n",
              " 'less',\n",
              " 'quite',\n",
              " 'version',\n",
              " 'able',\n",
              " 'anything',\n",
              " '&',\n",
              " 'After',\n",
              " 'getting',\n",
              " 'high',\n",
              " 'end',\n",
              " 'almost',\n",
              " 'came',\n",
              " 'However',\n",
              " 'story',\n",
              " 'software',\n",
              " 'several',\n",
              " 'must',\n",
              " 'away',\n",
              " 'water',\n",
              " 'box',\n",
              " 'said',\n",
              " 'son',\n",
              " 'CD',\n",
              " 'nothing',\n",
              " 'real',\n",
              " 'why',\n",
              " 'received',\n",
              " 'part',\n",
              " 'top',\n",
              " 'These',\n",
              " 'fit',\n",
              " 'battery',\n",
              " 'looks',\n",
              " 'perfect',\n",
              " 'yet',\n",
              " 'purchase',\n",
              " 'having',\n",
              " 'might',\n",
              " 'problems',\n",
              " 'once',\n",
              " 'days',\n",
              " 'comes',\n",
              " 'fine',\n",
              " 'Not',\n",
              " 'anyone',\n",
              " 'order',\n",
              " 'side',\n",
              " 'daughter',\n",
              " 'fact',\n",
              " 'three',\n",
              " 'What',\n",
              " 'minutes',\n",
              " 'unit',\n",
              " 'took',\n",
              " 'won',\n",
              " 'games',\n",
              " 'probably',\n",
              " 'reading',\n",
              " 'help',\n",
              " 'baby',\n",
              " 'Also',\n",
              " 'products',\n",
              " 'trying',\n",
              " 'pictures',\n",
              " 'least',\n",
              " 'car',\n",
              " 'wanted',\n",
              " 'home',\n",
              " 'excellent',\n",
              " 'DVD',\n",
              " 'isn',\n",
              " 'us',\n",
              " 'THE',\n",
              " 'easily',\n",
              " 'point',\n",
              " 'place',\n",
              " 'until',\n",
              " 'together',\n",
              " 'system',\n",
              " 'everything',\n",
              " 'kids',\n",
              " 'second',\n",
              " 'disappointed',\n",
              " 'ordered',\n",
              " 'computer',\n",
              " 'etc',\n",
              " 'All',\n",
              " 'One',\n",
              " 'That',\n",
              " 'ago',\n",
              " 'review',\n",
              " 'worked',\n",
              " 'buying',\n",
              " 'No',\n",
              " 'happy',\n",
              " 'songs',\n",
              " 'stars',\n",
              " 'full',\n",
              " 'next',\n",
              " 'hours',\n",
              " 'cover',\n",
              " 'store',\n",
              " 'issue',\n",
              " 'instead',\n",
              " 'world',\n",
              " 'whole',\n",
              " 'done',\n",
              " 'especially',\n",
              " 'program',\n",
              " 'lens',\n",
              " 'making',\n",
              " 'free',\n",
              " 'highly',\n",
              " 'either',\n",
              " 'original',\n",
              " 'between',\n",
              " 'takes',\n",
              " 'plastic',\n",
              " 'picture',\n",
              " 'went',\n",
              " 'gift',\n",
              " 'skin',\n",
              " 'else',\n",
              " 'She',\n",
              " 'NOT',\n",
              " 'weeks',\n",
              " 'Now',\n",
              " 'screen',\n",
              " 'color',\n",
              " 'couple',\n",
              " 'song',\n",
              " 'low',\n",
              " 'however',\n",
              " 'let',\n",
              " 'already',\n",
              " 'support',\n",
              " 'card',\n",
              " 'bag',\n",
              " 'hold',\n",
              " 'others',\n",
              " 'month',\n",
              " 'characters',\n",
              " 'piece',\n",
              " 'fan',\n",
              " 'started',\n",
              " 'under',\n",
              " 'stuff',\n",
              " 'video',\n",
              " 'seen',\n",
              " 'return',\n",
              " 'hand',\n",
              " 'short',\n",
              " 'someone',\n",
              " 'seem',\n",
              " 'week',\n",
              " 'service',\n",
              " 'playing',\n",
              " 'features',\n",
              " 'tell',\n",
              " 'reason',\n",
              " 'rather',\n",
              " 'large',\n",
              " 'waste',\n",
              " 'half',\n",
              " 'company',\n",
              " 'gets',\n",
              " 'job',\n",
              " 'line',\n",
              " 'cheap',\n",
              " 'kind',\n",
              " 'wrong',\n",
              " 'myself',\n",
              " 'loves',\n",
              " 'Don',\n",
              " 'turn',\n",
              " 'wonderful',\n",
              " 'left',\n",
              " 'enjoy',\n",
              " 'working',\n",
              " 'favorite',\n",
              " 'start',\n",
              " 'definitely',\n",
              " 'itself',\n",
              " 'couldn',\n",
              " 'loved',\n",
              " 'simply',\n",
              " 'room',\n",
              " 'tea',\n",
              " 'design',\n",
              " 'believe',\n",
              " 'Very',\n",
              " 'wish',\n",
              " 'toy',\n",
              " 'run',\n",
              " 'course',\n",
              " 'wear',\n",
              " 'difficult',\n",
              " 'Great',\n",
              " 'show',\n",
              " 'called',\n",
              " 'completely',\n",
              " 'shipping',\n",
              " 'expensive',\n",
              " '&quot',\n",
              " 'often',\n",
              " 'wasn',\n",
              " 'open',\n",
              " 'gave',\n",
              " 'needed',\n",
              " 'seat',\n",
              " 'Just',\n",
              " 'taste',\n",
              " 'simple',\n",
              " 'experience',\n",
              " 'coffee',\n",
              " 'person',\n",
              " 'played',\n",
              " 'clean',\n",
              " 'cost',\n",
              " 'later',\n",
              " 'feature',\n",
              " 'longer',\n",
              " 'night',\n",
              " 'series',\n",
              " 'wouldn',\n",
              " 'power',\n",
              " 'doing',\n",
              " 'Even',\n",
              " 'quickly',\n",
              " 'books',\n",
              " 'Christmas',\n",
              " 'idea',\n",
              " 'comfortable',\n",
              " 'interesting',\n",
              " 'cannot',\n",
              " 'extra',\n",
              " 'child',\n",
              " 'ones',\n",
              " 'player',\n",
              " 'type',\n",
              " 'style',\n",
              " 'family',\n",
              " 'batteries',\n",
              " 'close',\n",
              " 'goes',\n",
              " 'deal',\n",
              " 'change',\n",
              " 'during',\n",
              " 'Well',\n",
              " 'live',\n",
              " 'character',\n",
              " 'decided',\n",
              " 'machine',\n",
              " 'true',\n",
              " 'beautiful',\n",
              " 'head',\n",
              " 'house',\n",
              " 'husband',\n",
              " 'fast',\n",
              " 'children',\n",
              " 'maybe',\n",
              " 'customer',\n",
              " 'absolutely',\n",
              " 'AND',\n",
              " 'pay',\n",
              " 'black',\n",
              " 'On',\n",
              " 'clear',\n",
              " 'heavy',\n",
              " 'To',\n",
              " 'At',\n",
              " 'available',\n",
              " 'finally',\n",
              " 'star',\n",
              " 'looked',\n",
              " 'level',\n",
              " 'band',\n",
              " 'With',\n",
              " 'information',\n",
              " 'issues',\n",
              " 'extremely',\n",
              " 'digital',\n",
              " 'needs',\n",
              " 'face',\n",
              " 'poor',\n",
              " 'written',\n",
              " 'past',\n",
              " 'everyone',\n",
              " 'number',\n",
              " 'says',\n",
              " 'Then',\n",
              " 'yourself',\n",
              " 'model',\n",
              " 'cool',\n",
              " 'along',\n",
              " 'liked',\n",
              " 'given',\n",
              " 'IT',\n",
              " 'told',\n",
              " 'haven',\n",
              " 'inside',\n",
              " 'hot',\n",
              " 'mind',\n",
              " '%',\n",
              " 'call',\n",
              " 'add',\n",
              " 'saw',\n",
              " 'four',\n",
              " 'heard',\n",
              " 'within',\n",
              " 'brand',\n",
              " 'articles',\n",
              " 'man',\n",
              " 'While',\n",
              " 'cut',\n",
              " 'gives',\n",
              " 'D',\n",
              " 'mine',\n",
              " 'amazing',\n",
              " 'button',\n",
              " 'stay',\n",
              " 'Overall',\n",
              " 'weight',\n",
              " 'hear',\n",
              " 'guess',\n",
              " 'taking',\n",
              " 'although',\n",
              " 'learn',\n",
              " 'movies',\n",
              " 'friend',\n",
              " 'writing',\n",
              " 'pieces',\n",
              " 'understand',\n",
              " 'hands',\n",
              " 'rest',\n",
              " 'single',\n",
              " 'control',\n",
              " 'name',\n",
              " 'online',\n",
              " 'sent',\n",
              " 'sometimes',\n",
              " 'expected',\n",
              " 'front',\n",
              " 'First',\n",
              " 'entire',\n",
              " 'material',\n",
              " 'move',\n",
              " 'wife',\n",
              " 'strong',\n",
              " 'friends',\n",
              " 'today',\n",
              " 'unless',\n",
              " 'expect',\n",
              " 'broke',\n",
              " 'pleased',\n",
              " 'white',\n",
              " 'value',\n",
              " 'care',\n",
              " 'running',\n",
              " 'felt',\n",
              " 'voice',\n",
              " 'body',\n",
              " 'regular',\n",
              " 'paper',\n",
              " 'items',\n",
              " 'feet',\n",
              " 'lost',\n",
              " 'arrived',\n",
              " 'bottom',\n",
              " 'soft',\n",
              " 'wait',\n",
              " 'thinking',\n",
              " 'huge',\n",
              " 'pages',\n",
              " 'previous',\n",
              " 'replacement',\n",
              " 'amount',\n",
              " 'hope',\n",
              " 'sounds',\n",
              " 'TV',\n",
              " 'usually',\n",
              " 'become',\n",
              " 'main',\n",
              " 'easier',\n",
              " 'returned',\n",
              " 'food',\n",
              " 'save',\n",
              " 'smell',\n",
              " 'matter',\n",
              " 'exactly',\n",
              " 'page',\n",
              " 'based',\n",
              " 'seemed',\n",
              " 'dry',\n",
              " 'quick',\n",
              " 'S',\n",
              " 'non',\n",
              " 'charge',\n",
              " 'perfectly',\n",
              " 'spend',\n",
              " 'pick',\n",
              " 'due',\n",
              " 'worst',\n",
              " 'IS',\n",
              " 'THIS',\n",
              " 'fits',\n",
              " 'local',\n",
              " 'plus',\n",
              " 'handle',\n",
              " 'device',\n",
              " 'parts',\n",
              " 'recommended',\n",
              " 'package',\n",
              " 'results',\n",
              " 'hit',\n",
              " 'instructions',\n",
              " 'Canon',\n",
              " 'flavor',\n",
              " 'five',\n",
              " 'Some',\n",
              " 'business',\n",
              " 'subscription',\n",
              " 'included',\n",
              " 'stop',\n",
              " 'difference',\n",
              " 'write',\n",
              " 'supposed',\n",
              " 'turned',\n",
              " 'track',\n",
              " 'stick',\n",
              " 'photos',\n",
              " 'memory',\n",
              " 'send',\n",
              " 'stand',\n",
              " 'dark',\n",
              " 'leave',\n",
              " 'Sony',\n",
              " 'PC',\n",
              " 'Good',\n",
              " 'young',\n",
              " 'kept',\n",
              " 'truly',\n",
              " 'TO',\n",
              " 'drive',\n",
              " 'pair',\n",
              " 'soon',\n",
              " 'lots',\n",
              " 'outside',\n",
              " 'author',\n",
              " 'user',\n",
              " 'check',\n",
              " 'complete',\n",
              " 'totally',\n",
              " 'hour',\n",
              " 'cards',\n",
              " 'space',\n",
              " 'graphics',\n",
              " 'American',\n",
              " 'including',\n",
              " 'opinion',\n",
              " 'T',\n",
              " 'area',\n",
              " 'slow',\n",
              " 'means',\n",
              " 'mean',\n",
              " 'older',\n",
              " 'recently',\n",
              " 'grill',\n",
              " 'similar',\n",
              " 'shoes',\n",
              " 'cable',\n",
              " 'shows',\n",
              " 'image',\n",
              " 'uses',\n",
              " 'performance',\n",
              " 'e',\n",
              " 'listen',\n",
              " 'awesome',\n",
              " 'aren',\n",
              " 'flash',\n",
              " 'certainly',\n",
              " 'paid',\n",
              " 'figure',\n",
              " 'air',\n",
              " 'break',\n",
              " 'setting',\n",
              " 'sturdy',\n",
              " 'special',\n",
              " 'smaller',\n",
              " 'against',\n",
              " 'sense',\n",
              " 'important',\n",
              " 'useful',\n",
              " 'taken',\n",
              " 'enjoyed',\n",
              " 'overall',\n",
              " 'spent',\n",
              " 'manual',\n",
              " 'fall',\n",
              " 'market',\n",
              " 'bottle',\n",
              " 'cup',\n",
              " 'classic',\n",
              " 'school',\n",
              " 'choice',\n",
              " 'eat',\n",
              " 'colors',\n",
              " 'metal',\n",
              " 'collection',\n",
              " 'guy',\n",
              " 'Its',\n",
              " 'Windows',\n",
              " 'example',\n",
              " 'wide',\n",
              " 'annoying',\n",
              " 'plot',\n",
              " 'feels',\n",
              " 'women',\n",
              " 'decent',\n",
              " 'Of',\n",
              " 'table',\n",
              " 'carry',\n",
              " 'larger',\n",
              " 'major',\n",
              " 'range',\n",
              " 'except',\n",
              " 'red',\n",
              " 'agree',\n",
              " 'nearly',\n",
              " 'clock',\n",
              " 'magazines',\n",
              " 'worse',\n",
              " 'action',\n",
              " 'boring',\n",
              " 'designed',\n",
              " 'sweet',\n",
              " 'terrible',\n",
              " 'bike',\n",
              " 'accurate',\n",
              " 'replace',\n",
              " 'install',\n",
              " 'feeling',\n",
              " 'Do',\n",
              " 'owned',\n",
              " 'beat',\n",
              " 'watching',\n",
              " 'website',\n",
              " 'apart',\n",
              " 'keeps',\n",
              " 'cd',\n",
              " 'Unfortunately',\n",
              " 'words',\n",
              " 'giving',\n",
              " 'release',\n",
              " 'pack',\n",
              " 'mode',\n",
              " 'coming',\n",
              " 'standard',\n",
              " 'oil',\n",
              " 'twice',\n",
              " 'Although',\n",
              " 'cute',\n",
              " 'gone',\n",
              " 'possible',\n",
              " 'above',\n",
              " 'age',\n",
              " 'across',\n",
              " 'How',\n",
              " 'warm',\n",
              " 'forward',\n",
              " 'rock',\n",
              " 'addition',\n",
              " 'saying',\n",
              " 'stopped',\n",
              " 'happened',\n",
              " 'funny',\n",
              " 'dont',\n",
              " 'print',\n",
              " 'x',\n",
              " 'glad',\n",
              " 'immediately',\n",
              " 'Since',\n",
              " 'th',\n",
              " 'normal',\n",
              " 'files',\n",
              " 'slightly',\n",
              " 'radio',\n",
              " 'fairly',\n",
              " 'list',\n",
              " 'stories',\n",
              " 'history',\n",
              " 'built',\n",
              " 'trouble',\n",
              " 'bar',\n",
              " 'ended',\n",
              " 'pop',\n",
              " 'New',\n",
              " 'Yes',\n",
              " 'horrible',\n",
              " 'self',\n",
              " 'focus',\n",
              " 'buttons',\n",
              " 'reviewers',\n",
              " 'remove',\n",
              " 'scent',\n",
              " 'mm',\n",
              " 'view',\n",
              " 'fresh',\n",
              " 'site',\n",
              " 'wearing',\n",
              " 'display',\n",
              " 'thin',\n",
              " 'straight',\n",
              " 'copy',\n",
              " 'reader',\n",
              " 'suggest',\n",
              " 'OF',\n",
              " 'broken',\n",
              " 'noise',\n",
              " 'bed',\n",
              " 'putting',\n",
              " 'installed',\n",
              " 'various',\n",
              " 'early',\n",
              " 'Maybe',\n",
              " 'word',\n",
              " 'tracks',\n",
              " 'remember',\n",
              " 'reviewer',\n",
              " 'pocket',\n",
              " 'tool',\n",
              " 'Like',\n",
              " 'mention',\n",
              " 'known',\n",
              " 'Most',\n",
              " 'heat',\n",
              " 'near',\n",
              " 'date',\n",
              " 'process',\n",
              " 'web',\n",
              " 'third',\n",
              " 'smooth',\n",
              " 'alone',\n",
              " 'solid',\n",
              " 'pull',\n",
              " 'Why',\n",
              " 'added',\n",
              " 'wants',\n",
              " 'guitar',\n",
              " 'touch',\n",
              " 'fans',\n",
              " 'sit',\n",
              " 'plan',\n",
              " 'X',\n",
              " 'average',\n",
              " 'useless',\n",
              " 'rate',\n",
              " 'VERY',\n",
              " 'stroller',\n",
              " 'natural',\n",
              " 'serious',\n",
              " 'consider',\n",
              " 'kit',\n",
              " 'option',\n",
              " 'onto',\n",
              " 'speed',\n",
              " 'learning',\n",
              " 'surprised',\n",
              " 'cause',\n",
              " 'record',\n",
              " 'Microsoft',\n",
              " 'super',\n",
              " 'girl',\n",
              " 'cheaper',\n",
              " 'upgrade',\n",
              " 'fantastic',\n",
              " 'plays',\n",
              " 'noticed',\n",
              " 'behind',\n",
              " 'personal',\n",
              " 'base',\n",
              " 'note',\n",
              " 'scene',\n",
              " 'interested',\n",
              " 'middle',\n",
              " 'impressed',\n",
              " 'form',\n",
              " 'basic',\n",
              " 'properly',\n",
              " 'office',\n",
              " 'images',\n",
              " 'morning',\n",
              " 'heart',\n",
              " 'knew',\n",
              " 'Once',\n",
              " 'stuck',\n",
              " 'volume',\n",
              " 'bright',\n",
              " 'sleep',\n",
              " 'photo',\n",
              " 'ability',\n",
              " 'thick',\n",
              " 'opened',\n",
              " 'mentioned',\n",
              " 'per',\n",
              " 'scenes',\n",
              " 'Love',\n",
              " 'compared',\n",
              " 'changed',\n",
              " 'travel',\n",
              " 'cameras',\n",
              " 'fix',\n",
              " 'helps',\n",
              " 'charger',\n",
              " 'mostly',\n",
              " 'interest',\n",
              " 'present',\n",
              " 'purchasing',\n",
              " 'floor',\n",
              " 'woman',\n",
              " 'helpful',\n",
              " 'future',\n",
              " 'advice',\n",
              " 'chocolate',\n",
              " 'offer',\n",
              " 'title',\n",
              " 'lack',\n",
              " 'chair',\n",
              " 'beginning',\n",
              " 'whether',\n",
              " 'certain',\n",
              " 'seconds',\n",
              " 'particular',\n",
              " 'sort',\n",
              " 'pain',\n",
              " 'somewhat',\n",
              " 'refund',\n",
              " 'acting',\n",
              " 'men',\n",
              " 'cold',\n",
              " 'durable',\n",
              " 'excited',\n",
              " 'anyway',\n",
              " 'com',\n",
              " 'chance',\n",
              " 'Another',\n",
              " ...)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_list = so[0:8000]\n",
        "M = np.zeros((len(reviews)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort to ensure consistent ordering\n",
        "unique_labels = sorted(list(set(helpfulness_ratings)))\n",
        "unique_one_hot = np.diag(np.ones(len(unique_labels)))\n",
        "\n",
        "\n",
        "labels_train = [helpfulness_ratings[i] for i in train_ints]\n",
        "labels_test = [helpfulness_ratings[i] for i in test_ints]\n",
        "labels_dev = [helpfulness_ratings[i] for i in final_test_ints]\n",
        "\n",
        "# Make sure you're using the right label variables\n",
        "y_train = np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_train]]).T\n",
        "y_test = np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_test]]).T\n",
        "y_dev = np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_dev]]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21928, 8000)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 21928)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 21928)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21928, 8000)\n",
            "(21928, 3)\n",
            "(21928, 3)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 2000\n",
        "\n",
        "\n",
        "\n",
        "num_features = M_train.shape[1]\n",
        "num_classes = y_train.shape[0]\n",
        "num_samples = M_train.shape[0]\n",
        "\n",
        "# Initialize weights with correct dimensions: (num_features, num_classes)\n",
        "weights = np.random.rand(num_features, num_classes) # (8000,3)\n",
        "bias = np.zeros(num_classes) # (3,)\n",
        "lr=0.1\n",
        "logistic_loss=[]\n",
        "\n",
        "\n",
        "\n",
        "# x and y now refer to the training data for intent classification\n",
        "x_train_data = M_train  # Shape (num_samples, num_features) (21928, 8000)\n",
        "y_train_targets = y_train.T # Shape (num_samples, num_classes) (3, 21928)\n",
        "print(x_train_data.shape)\n",
        "print(y_train_targets.shape)\n",
        "z=np.zeros((num_samples,num_classes)) # (21928, 3)\n",
        "q=np.zeros((num_samples,num_classes)) # (21928, 3)\n",
        "\n",
        "print(q.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss at  0 is 5.410927540164162\n",
            "loss at  1 is 3.2143479016356924\n",
            "loss at  2 is 3.176381492879163\n",
            "loss at  3 is 3.1577437042206853\n",
            "loss at  4 is 3.1406677426946996\n",
            "loss at  5 is 3.1247888225706246\n",
            "loss at  6 is 3.109958318744848\n",
            "loss at  7 is 3.0960693615297705\n",
            "loss at  8 is 3.0830295372411376\n",
            "loss at  9 is 3.0707561533018284\n",
            "loss at  10 is 3.059175069095574\n",
            "loss at  11 is 3.0482198955580913\n",
            "loss at  12 is 3.037831177148084\n",
            "loss at  13 is 3.027955605474674\n",
            "loss at  14 is 3.0185453156971955\n",
            "loss at  15 is 3.009557277072809\n",
            "loss at  16 is 3.000952768412277\n",
            "loss at  17 is 2.992696923816532\n",
            "loss at  18 is 2.984758335216663\n",
            "loss at  19 is 2.977108701146809\n",
            "loss at  20 is 2.969722513950655\n",
            "loss at  21 is 2.9625767797346056\n",
            "loss at  22 is 2.9556507668362495\n",
            "loss at  23 is 2.94892577952971\n",
            "loss at  24 is 2.9423849543025504\n",
            "loss at  25 is 2.936013076440359\n",
            "loss at  26 is 2.9297964149317393\n",
            "loss at  27 is 2.9237225739132686\n",
            "loss at  28 is 2.9177803590421516\n",
            "loss at  29 is 2.911959657330673\n",
            "loss at  30 is 2.9062513291092937\n",
            "loss at  31 is 2.900647110907786\n",
            "loss at  32 is 2.8951395281574963\n",
            "loss at  33 is 2.889721816723008\n",
            "loss at  34 is 2.8843878523682798\n",
            "loss at  35 is 2.879132087350949\n",
            "loss at  36 is 2.873949493419138\n",
            "loss at  37 is 2.8688355105583097\n",
            "loss at  38 is 2.863786000901806\n",
            "loss at  39 is 2.8587972072784775\n",
            "loss at  40 is 2.8538657159244765\n",
            "loss at  41 is 2.8489884229346987\n",
            "loss at  42 is 2.8441625040728633\n",
            "loss at  43 is 2.8393853875982393\n",
            "loss at  44 is 2.8346547298022053\n",
            "loss at  45 is 2.829968392979267\n",
            "loss at  46 is 2.8253244255855336\n",
            "loss at  47 is 2.8207210443629642\n",
            "loss at  48 is 2.816156618230549\n",
            "loss at  49 is 2.8116296537639616\n",
            "loss at  50 is 2.8071387821035985\n",
            "loss at  51 is 2.802682747147301\n",
            "loss at  52 is 2.7982603948988163\n",
            "loss at  53 is 2.7938706638562545\n",
            "loss at  54 is 2.789512576336605\n",
            "loss at  55 is 2.785185230643021\n",
            "loss at  56 is 2.780887793991046\n",
            "loss at  57 is 2.7766194961185064\n",
            "loss at  58 is 2.7723796235114153\n",
            "loss at  59 is 2.768167514185088\n",
            "loss at  60 is 2.763982552965796\n",
            "loss at  61 is 2.7598241672237958\n",
            "loss at  62 is 2.7556918230135397\n",
            "loss at  63 is 2.751585021581232\n",
            "loss at  64 is 2.7475032962039725\n",
            "loss at  65 is 2.743446209328179\n",
            "loss at  66 is 2.739413349978317\n",
            "loss at  67 is 2.7354043314097223\n",
            "loss at  68 is 2.731418788981985\n",
            "loss at  69 is 2.727456378231614\n",
            "loss at  70 is 2.7235167731248615\n",
            "loss at  71 is 2.719599664473402\n",
            "loss at  72 is 2.7157047584973006\n",
            "loss at  73 is 2.711831775521199\n",
            "loss at  74 is 2.707980448791038\n",
            "loss at  75 is 2.704150523399845\n",
            "loss at  76 is 2.700341755312257\n",
            "loss at  77 is 2.696553910478427\n",
            "loss at  78 is 2.692786764028868\n",
            "loss at  79 is 2.689040099542619\n",
            "loss at  80 is 2.6853137083818295\n",
            "loss at  81 is 2.6816073890865364\n",
            "loss at  82 is 2.6779209468239884\n",
            "loss at  83 is 2.6742541928874424\n",
            "loss at  84 is 2.670606944239796\n",
            "loss at  85 is 2.6669790230979094\n",
            "loss at  86 is 2.6633702565538244\n",
            "loss at  87 is 2.6597804762294786\n",
            "loss at  88 is 2.656209517961815\n",
            "loss at  89 is 2.6526572215154847\n",
            "loss at  90 is 2.649123430320614\n",
            "loss at  91 is 2.64560799123334\n",
            "loss at  92 is 2.6421107543170157\n",
            "loss at  93 is 2.6386315726422285\n",
            "loss at  94 is 2.6351703021038917\n",
            "loss at  95 is 2.631726801253885\n",
            "loss at  96 is 2.628300931147818\n",
            "loss at  97 is 2.624892555204659\n",
            "loss at  98 is 2.621501539078062\n",
            "loss at  99 is 2.618127750538341\n",
            "loss at  100 is 2.6147710593641573\n",
            "loss at  101 is 2.6114313372430256\n",
            "loss at  102 is 2.608108457679884\n",
            "loss at  103 is 2.6048022959129957\n",
            "loss at  104 is 2.601512728836538\n",
            "loss at  105 is 2.598239634929298\n",
            "loss at  106 is 2.5949828941889326\n",
            "loss at  107 is 2.5917423880713026\n",
            "loss at  108 is 2.588517999434453\n",
            "loss at  109 is 2.585309612486823\n",
            "loss at  110 is 2.5821171127393225\n",
            "loss at  111 is 2.5789403869609537\n",
            "loss at  112 is 2.5757793231376516\n",
            "loss at  113 is 2.5726338104340956\n",
            "loss at  114 is 2.56950373915821\n",
            "loss at  115 is 2.566389000728157\n",
            "loss at  116 is 2.563289487641579\n",
            "loss at  117 is 2.5602050934469305\n",
            "loss at  118 is 2.5571357127167067\n",
            "loss at  119 is 2.5540812410224185\n",
            "loss at  120 is 2.551041574911164\n",
            "loss at  121 is 2.5480166118836696\n",
            "loss at  122 is 2.5450062503736874\n",
            "loss at  123 is 2.5420103897286164\n",
            "loss at  124 is 2.5390289301912716\n",
            "loss at  125 is 2.536061772882697\n",
            "loss at  126 is 2.5331088197859395\n",
            "loss at  127 is 2.530169973730713\n",
            "loss at  128 is 2.5272451383788734\n",
            "loss at  129 is 2.524334218210651\n",
            "loss at  130 is 2.521437118511574\n",
            "loss at  131 is 2.518553745360033\n",
            "loss at  132 is 2.515684005615436\n",
            "loss at  133 is 2.5128278069069103\n",
            "loss at  134 is 2.5099850576225142\n",
            "loss at  135 is 2.507155666898905\n",
            "loss at  136 is 2.504339544611454\n",
            "loss at  137 is 2.501536601364754\n",
            "loss at  138 is 2.498746748483501\n",
            "loss at  139 is 2.4959698980037217\n",
            "loss at  140 is 2.4932059626643257\n",
            "loss at  141 is 2.4904548558989483\n",
            "loss at  142 is 2.4877164918280843\n",
            "loss at  143 is 2.4849907852514628\n",
            "loss at  144 is 2.4822776516406817\n",
            "loss at  145 is 2.4795770071320535\n",
            "loss at  146 is 2.4768887685196725\n",
            "loss at  147 is 2.4742128532486674\n",
            "loss at  148 is 2.4715491794086453\n",
            "loss at  149 is 2.468897665727307\n",
            "loss at  150 is 2.466258231564222\n",
            "loss at  151 is 2.4636307969047566\n",
            "loss at  152 is 2.461015282354139\n",
            "loss at  153 is 2.4584116091316663\n",
            "loss at  154 is 2.4558196990650303\n",
            "loss at  155 is 2.4532394745847648\n",
            "loss at  156 is 2.4506708587188\n",
            "loss at  157 is 2.4481137750871325\n",
            "loss at  158 is 2.4455681478965756\n",
            "loss at  159 is 2.443033901935627\n",
            "loss at  160 is 2.440510962569401\n",
            "loss at  161 is 2.4379992557346633\n",
            "loss at  162 is 2.4354987079349364\n",
            "loss at  163 is 2.43300924623568\n",
            "loss at  164 is 2.4305307982595457\n",
            "loss at  165 is 2.4280632921816943\n",
            "loss at  166 is 2.425606656725187\n",
            "loss at  167 is 2.423160821156429\n",
            "loss at  168 is 2.420725715280673\n",
            "loss at  169 is 2.418301269437586\n",
            "loss at  170 is 2.4158874144968565\n",
            "loss at  171 is 2.4134840818538668\n",
            "loss at  172 is 2.4110912034254\n",
            "loss at  173 is 2.4087087116454056\n",
            "loss at  174 is 2.406336539460803\n",
            "loss at  175 is 2.4039746203273307\n",
            "loss at  176 is 2.4016228882054316\n",
            "loss at  177 is 2.3992812775561942\n",
            "loss at  178 is 2.396949723337306\n",
            "loss at  179 is 2.394628160999071\n",
            "loss at  180 is 2.392316526480444\n",
            "loss at  181 is 2.3900147562051095\n",
            "loss at  182 is 2.3877227870775926\n",
            "loss at  183 is 2.3854405564793995\n",
            "loss at  184 is 2.3831680022651955\n",
            "loss at  185 is 2.380905062759008\n",
            "loss at  186 is 2.3786516767504664\n",
            "loss at  187 is 2.376407783491065\n",
            "loss at  188 is 2.3741733226904613\n",
            "loss at  189 is 2.371948234512801\n",
            "loss at  190 is 2.369732459573066\n",
            "loss at  191 is 2.3675259389334604\n",
            "loss at  192 is 2.3653286140998113\n",
            "loss at  193 is 2.36314042701801\n",
            "loss at  194 is 2.360961320070465\n",
            "loss at  195 is 2.358791236072593\n",
            "loss at  196 is 2.3566301182693343\n",
            "loss at  197 is 2.3544779103316844\n",
            "loss at  198 is 2.3523345563532647\n",
            "loss at  199 is 2.3502000008469044\n",
            "loss at  200 is 2.3480741887412626\n",
            "loss at  201 is 2.3459570653774606\n",
            "loss at  202 is 2.343848576505751\n",
            "loss at  203 is 2.3417486682822024\n",
            "loss at  204 is 2.339657287265419\n",
            "loss at  205 is 2.337574380413271\n",
            "loss at  206 is 2.335499895079659\n",
            "loss at  207 is 2.3334337790113078\n",
            "loss at  208 is 2.3313759803445686\n",
            "loss at  209 is 2.32932644760226\n",
            "loss at  210 is 2.3272851296905235\n",
            "loss at  211 is 2.3252519758957133\n",
            "loss at  212 is 2.3232269358812996\n",
            "loss at  213 is 2.3212099596848046\n",
            "loss at  214 is 2.3192009977147556\n",
            "loss at  215 is 2.317200000747671\n",
            "loss at  216 is 2.3152069199250596\n",
            "loss at  217 is 2.3132217067504506\n",
            "loss at  218 is 2.3112443130864526\n",
            "loss at  219 is 2.309274691151822\n",
            "loss at  220 is 2.307312793518571\n",
            "loss at  221 is 2.305358573109087\n",
            "loss at  222 is 2.3034119831932873\n",
            "loss at  223 is 2.301472977385786\n",
            "loss at  224 is 2.2995415096430936\n",
            "loss at  225 is 2.297617534260839\n",
            "loss at  226 is 2.2957010058710083\n",
            "loss at  227 is 2.293791879439218\n",
            "loss at  228 is 2.2918901102620017\n",
            "loss at  229 is 2.289995653964125\n",
            "loss at  230 is 2.28810846649593\n",
            "loss at  231 is 2.2862285041306856\n",
            "loss at  232 is 2.2843557234619793\n",
            "loss at  233 is 2.282490081401126\n",
            "loss at  234 is 2.2806315351745905\n",
            "loss at  235 is 2.2787800423214497\n",
            "loss at  236 is 2.276935560690862\n",
            "loss at  237 is 2.2750980484395704\n",
            "loss at  238 is 2.2732674640294217\n",
            "loss at  239 is 2.271443766224909\n",
            "loss at  240 is 2.269626914090739\n",
            "loss at  241 is 2.267816866989419\n",
            "loss at  242 is 2.266013584578868\n",
            "loss at  243 is 2.2642170268100448\n",
            "loss at  244 is 2.262427153924603\n",
            "loss at  245 is 2.2606439264525684\n",
            "loss at  246 is 2.258867305210028\n",
            "loss at  247 is 2.257097251296848\n",
            "loss at  248 is 2.255333726094418\n",
            "loss at  249 is 2.253576691263402\n",
            "loss at  250 is 2.251826108741518\n",
            "loss at  251 is 2.250081940741338\n",
            "loss at  252 is 2.24834414974811\n",
            "loss at  253 is 2.24661269851759\n",
            "loss at  254 is 2.2448875500739067\n",
            "loss at  255 is 2.24316866770744\n",
            "loss at  256 is 2.2414560149727096\n",
            "loss at  257 is 2.2397495556863025\n",
            "loss at  258 is 2.238049253924803\n",
            "loss at  259 is 2.2363550740227485\n",
            "loss at  260 is 2.2346669805705974\n",
            "loss at  261 is 2.232984938412727\n",
            "loss at  262 is 2.231308912645439\n",
            "loss at  263 is 2.229638868614984\n",
            "loss at  264 is 2.2279747719156093\n",
            "loss at  265 is 2.226316588387621\n",
            "loss at  266 is 2.2246642841154616\n",
            "loss at  267 is 2.223017825425801\n",
            "loss at  268 is 2.221377178885661\n",
            "loss at  269 is 2.219742311300534\n",
            "loss at  270 is 2.218113189712535\n",
            "loss at  271 is 2.2164897813985576\n",
            "loss at  272 is 2.2148720538684623\n",
            "loss at  273 is 2.213259974863257\n",
            "loss at  274 is 2.211653512353318\n",
            "loss at  275 is 2.2100526345366083\n",
            "loss at  276 is 2.208457309836917\n",
            "loss at  277 is 2.2068675069021197\n",
            "loss at  278 is 2.205283194602443\n",
            "loss at  279 is 2.203704342028751\n",
            "loss at  280 is 2.202130918490844\n",
            "loss at  281 is 2.2005628935157726\n",
            "loss at  282 is 2.1990002368461603\n",
            "loss at  283 is 2.1974429184385516\n",
            "loss at  284 is 2.195890908461762\n",
            "loss at  285 is 2.194344177295247\n",
            "loss at  286 is 2.1928026955274835\n",
            "loss at  287 is 2.1912664339543673\n",
            "loss at  288 is 2.18973536357762\n",
            "loss at  289 is 2.1882094556032077\n",
            "loss at  290 is 2.1866886814397772\n",
            "loss at  291 is 2.185173012697102\n",
            "loss at  292 is 2.1836624211845406\n",
            "loss at  293 is 2.1821568789095065\n",
            "loss at  294 is 2.180656358075956\n",
            "loss at  295 is 2.1791608310828763\n",
            "loss at  296 is 2.1776702705227993\n",
            "loss at  297 is 2.1761846491803185\n",
            "loss at  298 is 2.174703940030616\n",
            "loss at  299 is 2.173228116238014\n",
            "loss at  300 is 2.171757151154514\n",
            "loss at  301 is 2.170291018318377\n",
            "loss at  302 is 2.1688296914526886\n",
            "loss at  303 is 2.1673731444639506\n",
            "loss at  304 is 2.165921351440678\n",
            "loss at  305 is 2.1644742866520077\n",
            "loss at  306 is 2.1630319245463205\n",
            "loss at  307 is 2.1615942397498666\n",
            "loss at  308 is 2.160161207065408\n",
            "loss at  309 is 2.158732801470871\n",
            "loss at  310 is 2.1573089981180087\n",
            "loss at  311 is 2.155889772331067\n",
            "loss at  312 is 2.1544750996054716\n",
            "loss at  313 is 2.1530649556065202\n",
            "loss at  314 is 2.151659316168077\n",
            "loss at  315 is 2.150258157291295\n",
            "loss at  316 is 2.1488614551433285\n",
            "loss at  317 is 2.1474691860560693\n",
            "loss at  318 is 2.146081326524884\n",
            "loss at  319 is 2.1446978532073673\n",
            "loss at  320 is 2.1433187429221006\n",
            "loss at  321 is 2.141943972647422\n",
            "loss at  322 is 2.1405735195202036\n",
            "loss at  323 is 2.13920736083464\n",
            "loss at  324 is 2.1378454740410446\n",
            "loss at  325 is 2.136487836744658\n",
            "loss at  326 is 2.135134426704461\n",
            "loss at  327 is 2.133785221832001\n",
            "loss at  328 is 2.132440200190223\n",
            "loss at  329 is 2.131099339992312\n",
            "loss at  330 is 2.1297626196005464\n",
            "loss at  331 is 2.128430017525155\n",
            "loss at  332 is 2.127101512423187\n",
            "loss at  333 is 2.1257770830973883\n",
            "loss at  334 is 2.124456708495087\n",
            "loss at  335 is 2.12314036770709\n",
            "loss at  336 is 2.1218280399665796\n",
            "loss at  337 is 2.1205197046480344\n",
            "loss at  338 is 2.119215341266137\n",
            "loss at  339 is 2.1179149294747135\n",
            "loss at  340 is 2.1166184490656583\n",
            "loss at  341 is 2.1153258799678887\n",
            "loss at  342 is 2.1140372022462883\n",
            "loss at  343 is 2.1127523961006767\n",
            "loss at  344 is 2.1114714418647673\n",
            "loss at  345 is 2.1101943200051547\n",
            "loss at  346 is 2.108921011120291\n",
            "loss at  347 is 2.1076514959394825\n",
            "loss at  348 is 2.1063857553218877\n",
            "loss at  349 is 2.105123770255528\n",
            "loss at  350 is 2.103865521856301\n",
            "loss at  351 is 2.102610991367006\n",
            "loss at  352 is 2.1013601601563714\n",
            "loss at  353 is 2.1001130097180973\n",
            "loss at  354 is 2.0988695216699034\n",
            "loss at  355 is 2.097629677752576\n",
            "loss at  356 is 2.096393459829034\n",
            "loss at  357 is 2.0951608498833987\n",
            "loss at  358 is 2.093931830020066\n",
            "loss at  359 is 2.0927063824627927\n",
            "loss at  360 is 2.091484489553784\n",
            "loss at  361 is 2.090266133752795\n",
            "loss at  362 is 2.089051297636233\n",
            "loss at  363 is 2.087839963896269\n",
            "loss at  364 is 2.0866321153399596\n",
            "loss at  365 is 2.085427734888371\n",
            "loss at  366 is 2.0842268055757125\n",
            "loss at  367 is 2.083029310548475\n",
            "loss at  368 is 2.0818352330645813\n",
            "loss at  369 is 2.080644556492535\n",
            "loss at  370 is 2.079457264310586\n",
            "loss at  371 is 2.0782733401058926\n",
            "loss at  372 is 2.0770927675736988\n",
            "loss at  373 is 2.0759155305165136\n",
            "loss at  374 is 2.074741612843297\n",
            "loss at  375 is 2.0735709985686537\n",
            "loss at  376 is 2.072403671812034\n",
            "loss at  377 is 2.0712396167969396\n",
            "loss at  378 is 2.070078817850136\n",
            "loss at  379 is 2.0689212594008706\n",
            "loss at  380 is 2.0677669259801\n",
            "loss at  381 is 2.06661580221972\n",
            "loss at  382 is 2.0654678728518028\n",
            "loss at  383 is 2.0643231227078433\n",
            "loss at  384 is 2.0631815367180035\n",
            "loss at  385 is 2.0620430999103756\n",
            "loss at  386 is 2.060907797410238\n",
            "loss at  387 is 2.0597756144393258\n",
            "loss at  388 is 2.0586465363151016\n",
            "loss at  389 is 2.0575205484500407\n",
            "loss at  390 is 2.0563976363509084\n",
            "loss at  391 is 2.055277785618059\n",
            "loss at  392 is 2.0541609819447233\n",
            "loss at  393 is 2.053047211116321\n",
            "loss at  394 is 2.05193645900976\n",
            "loss at  395 is 2.0508287115927555\n",
            "loss at  396 is 2.049723954923145\n",
            "loss at  397 is 2.0486221751482128\n",
            "loss at  398 is 2.047523358504023\n",
            "loss at  399 is 2.0464274913147515\n",
            "loss at  400 is 2.045334559992028\n",
            "loss at  401 is 2.0442445510342795\n",
            "loss at  402 is 2.0431574510260826\n",
            "loss at  403 is 2.0420732466375195\n",
            "loss at  404 is 2.040991924623539\n",
            "loss at  405 is 2.039913471823321\n",
            "loss at  406 is 2.0388378751596505\n",
            "loss at  407 is 2.0377651216382913\n",
            "loss at  408 is 2.0366951983473682\n",
            "loss at  409 is 2.0356280924567534\n",
            "loss at  410 is 2.034563791217454\n",
            "loss at  411 is 2.033502281961014\n",
            "loss at  412 is 2.032443552098908\n",
            "loss at  413 is 2.0313875891219513\n",
            "loss at  414 is 2.0303343805997054\n",
            "loss at  415 is 2.0292839141798984\n",
            "loss at  416 is 2.028236177587836\n",
            "loss at  417 is 2.027191158625831\n",
            "loss at  418 is 2.02614884517263\n",
            "loss at  419 is 2.025109225182847\n",
            "loss at  420 is 2.0240722866863967\n",
            "loss at  421 is 2.0230380177879392\n",
            "loss at  422 is 2.022006406666325\n",
            "loss at  423 is 2.0209774415740442\n",
            "loss at  424 is 2.019951110836683\n",
            "loss at  425 is 2.0189274028523805\n",
            "loss at  426 is 2.017906306091288\n",
            "loss at  427 is 2.0168878090950453\n",
            "loss at  428 is 2.015871900476242\n",
            "loss at  429 is 2.0148585689178993\n",
            "loss at  430 is 2.0138478031729448\n",
            "loss at  431 is 2.012839592063698\n",
            "loss at  432 is 2.011833924481358\n",
            "loss at  433 is 2.010830789385492\n",
            "loss at  434 is 2.0098301758035344\n",
            "loss at  435 is 2.0088320728302813\n",
            "loss at  436 is 2.0078364696273976\n",
            "loss at  437 is 2.0068433554229177\n",
            "loss at  438 is 2.005852719510763\n",
            "loss at  439 is 2.0048645512502525\n",
            "loss at  440 is 2.003878840065618\n",
            "loss at  441 is 2.0028955754455304\n",
            "loss at  442 is 2.001914746942624\n",
            "loss at  443 is 2.000936344173021\n",
            "loss at  444 is 1.9999603568158686\n",
            "loss at  445 is 1.9989867746128738\n",
            "loss at  446 is 1.998015587367843\n",
            "loss at  447 is 1.997046784946222\n",
            "loss at  448 is 1.996080357274646\n",
            "loss at  449 is 1.9951162943404894\n",
            "loss at  450 is 1.994154586191418\n",
            "loss at  451 is 1.9931952229349434\n",
            "loss at  452 is 1.9922381947379881\n",
            "loss at  453 is 1.991283491826445\n",
            "loss at  454 is 1.9903311044847458\n",
            "loss at  455 is 1.989381023055429\n",
            "loss at  456 is 1.988433237938717\n",
            "loss at  457 is 1.9874877395920891\n",
            "loss at  458 is 1.9865445185298614\n",
            "loss at  459 is 1.9856035653227717\n",
            "loss at  460 is 1.9846648705975627\n",
            "loss at  461 is 1.9837284250365759\n",
            "loss at  462 is 1.9827942193773374\n",
            "loss at  463 is 1.9818622444121574\n",
            "loss at  464 is 1.9809324909877286\n",
            "loss at  465 is 1.980004950004726\n",
            "loss at  466 is 1.9790796124174113\n",
            "loss at  467 is 1.9781564692332418\n",
            "loss at  468 is 1.9772355115124791\n",
            "loss at  469 is 1.9763167303678055\n",
            "loss at  470 is 1.9754001169639348\n",
            "loss at  471 is 1.9744856625172365\n",
            "loss at  472 is 1.973573358295354\n",
            "loss at  473 is 1.9726631956168343\n",
            "loss at  474 is 1.971755165850751\n",
            "loss at  475 is 1.9708492604163363\n",
            "loss at  476 is 1.9699454707826158\n",
            "loss at  477 is 1.969043788468042\n",
            "loss at  478 is 1.9681442050401363\n",
            "loss at  479 is 1.967246712115127\n",
            "loss at  480 is 1.966351301357597\n",
            "loss at  481 is 1.9654579644801282\n",
            "loss at  482 is 1.9645666932429537\n",
            "loss at  483 is 1.963677479453606\n",
            "loss at  484 is 1.9627903149665775\n",
            "loss at  485 is 1.9619051916829746\n",
            "loss at  486 is 1.9610221015501776\n",
            "loss at  487 is 1.9601410365615053\n",
            "loss at  488 is 1.9592619887558786\n",
            "loss at  489 is 1.958384950217492\n",
            "loss at  490 is 1.9575099130754776\n",
            "loss at  491 is 1.9566368695035858\n",
            "loss at  492 is 1.9557658117198544\n",
            "loss at  493 is 1.954896731986289\n",
            "loss at  494 is 1.9540296226085463\n",
            "loss at  495 is 1.9531644759356088\n",
            "loss at  496 is 1.9523012843594814\n",
            "loss at  497 is 1.9514400403148693\n",
            "loss at  498 is 1.9505807362788725\n",
            "loss at  499 is 1.949723364770679\n",
            "loss at  500 is 1.9488679183512567\n",
            "loss at  501 is 1.9480143896230544\n",
            "loss at  502 is 1.9471627712296964\n",
            "loss at  503 is 1.9463130558556898\n",
            "loss at  504 is 1.9454652362261247\n",
            "loss at  505 is 1.9446193051063807\n",
            "loss at  506 is 1.9437752553018413\n",
            "loss at  507 is 1.9429330796575948\n",
            "loss at  508 is 1.9420927710581561\n",
            "loss at  509 is 1.94125432242718\n",
            "loss at  510 is 1.940417726727178\n",
            "loss at  511 is 1.9395829769592345\n",
            "loss at  512 is 1.9387500661627373\n",
            "loss at  513 is 1.9379189874150937\n",
            "loss at  514 is 1.937089733831463\n",
            "loss at  515 is 1.9362622985644793\n",
            "loss at  516 is 1.9354366748039862\n",
            "loss at  517 is 1.9346128557767692\n",
            "loss at  518 is 1.9337908347462878\n",
            "loss at  519 is 1.9329706050124158\n",
            "loss at  520 is 1.9321521599111768\n",
            "loss at  521 is 1.9313354928144897\n",
            "loss at  522 is 1.9305205971299055\n",
            "loss at  523 is 1.9297074663003588\n",
            "loss at  524 is 1.928896093803911\n",
            "loss at  525 is 1.9280864731535006\n",
            "loss at  526 is 1.9272785978966929\n",
            "loss at  527 is 1.926472461615435\n",
            "loss at  528 is 1.9256680579258112\n",
            "loss at  529 is 1.9248653804777955\n",
            "loss at  530 is 1.9240644229550177\n",
            "loss at  531 is 1.9232651790745146\n",
            "loss at  532 is 1.9224676425865046\n",
            "loss at  533 is 1.9216718072741388\n",
            "loss at  534 is 1.92087766695328\n",
            "loss at  535 is 1.9200852154722634\n",
            "loss at  536 is 1.919294446711667\n",
            "loss at  537 is 1.918505354584088\n",
            "loss at  538 is 1.9177179330339116\n",
            "loss at  539 is 1.9169321760370899\n",
            "loss at  540 is 1.9161480776009179\n",
            "loss at  541 is 1.9153656317638146\n",
            "loss at  542 is 1.9145848325951\n",
            "loss at  543 is 1.9138056741947804\n",
            "loss at  544 is 1.9130281506933349\n",
            "loss at  545 is 1.9122522562514972\n",
            "loss at  546 is 1.911477985060047\n",
            "loss at  547 is 1.9107053313395979\n",
            "loss at  548 is 1.9099342893403874\n",
            "loss at  549 is 1.9091648533420758\n",
            "loss at  550 is 1.908397017653532\n",
            "loss at  551 is 1.907630776612639\n",
            "loss at  552 is 1.9068661245860832\n",
            "loss at  553 is 1.9061030559691625\n",
            "loss at  554 is 1.90534156518558\n",
            "loss at  555 is 1.9045816466872525\n",
            "loss at  556 is 1.9038232949541125\n",
            "loss at  557 is 1.9030665044939123\n",
            "loss at  558 is 1.9023112698420372\n",
            "loss at  559 is 1.9015575855613096\n",
            "loss at  560 is 1.9008054462418014\n",
            "loss at  561 is 1.9000548465006466\n",
            "loss at  562 is 1.8993057809818563\n",
            "loss at  563 is 1.8985582443561309\n",
            "loss at  564 is 1.8978122313206793\n",
            "loss at  565 is 1.897067736599038\n",
            "loss at  566 is 1.89632475494089\n",
            "loss at  567 is 1.895583281121882\n",
            "loss at  568 is 1.894843309943457\n",
            "loss at  569 is 1.894104836232669\n",
            "loss at  570 is 1.8933678548420123\n",
            "loss at  571 is 1.8926323606492512\n",
            "loss at  572 is 1.8918983485572423\n",
            "loss at  573 is 1.8911658134937726\n",
            "loss at  574 is 1.890434750411383\n",
            "loss at  575 is 1.8897051542872065\n",
            "loss at  576 is 1.8889770201228\n",
            "loss at  577 is 1.8882503429439799\n",
            "loss at  578 is 1.887525117800659\n",
            "loss at  579 is 1.8868013397666854\n",
            "loss at  580 is 1.8860790039396818\n",
            "loss at  581 is 1.8853581054408866\n",
            "loss at  582 is 1.8846386394149937\n",
            "loss at  583 is 1.8839206010300005\n",
            "loss at  584 is 1.8832039854770484\n",
            "loss at  585 is 1.8824887879702716\n",
            "loss at  586 is 1.8817750037466423\n",
            "loss at  587 is 1.8810626280658211\n",
            "loss at  588 is 1.8803516562100084\n",
            "loss at  589 is 1.8796420834837904\n",
            "loss at  590 is 1.8789339052139937\n",
            "loss at  591 is 1.8782271167495441\n",
            "loss at  592 is 1.8775217134613138\n",
            "loss at  593 is 1.8768176907419793\n",
            "loss at  594 is 1.876115044005884\n",
            "loss at  595 is 1.8754137686888888\n",
            "loss at  596 is 1.8747138602482367\n",
            "loss at  597 is 1.874015314162412\n",
            "loss at  598 is 1.873318125931003\n",
            "loss at  599 is 1.8726222910745633\n",
            "loss at  600 is 1.871927805134479\n",
            "loss at  601 is 1.8712346636728285\n",
            "loss at  602 is 1.8705428622722577\n",
            "loss at  603 is 1.8698523965358378\n",
            "loss at  604 is 1.8691632620869425\n",
            "loss at  605 is 1.86847545456911\n",
            "loss at  606 is 1.8677889696459213\n",
            "loss at  607 is 1.8671038030008666\n",
            "loss at  608 is 1.8664199503372205\n",
            "loss at  609 is 1.8657374073779165\n",
            "loss at  610 is 1.8650561698654216\n",
            "loss at  611 is 1.8643762335616114\n",
            "loss at  612 is 1.863697594247648\n",
            "loss at  613 is 1.863020247723859\n",
            "loss at  614 is 1.8623441898096176\n",
            "loss at  615 is 1.8616694163432204\n",
            "loss at  616 is 1.8609959231817668\n",
            "loss at  617 is 1.8603237062010487\n",
            "loss at  618 is 1.8596527612954254\n",
            "loss at  619 is 1.8589830843777133\n",
            "loss at  620 is 1.8583146713790697\n",
            "loss at  621 is 1.857647518248877\n",
            "loss at  622 is 1.8569816209546313\n",
            "loss at  623 is 1.8563169754818318\n",
            "loss at  624 is 1.8556535778338652\n",
            "loss at  625 is 1.854991424031902\n",
            "loss at  626 is 1.854330510114783\n",
            "loss at  627 is 1.85367083213891\n",
            "loss at  628 is 1.8530123861781418\n",
            "loss at  629 is 1.8523551683236854\n",
            "loss at  630 is 1.8516991746839917\n",
            "loss at  631 is 1.8510444013846503\n",
            "loss at  632 is 1.8503908445682844\n",
            "loss at  633 is 1.8497385003944493\n",
            "loss at  634 is 1.8490873650395303\n",
            "loss at  635 is 1.8484374346966401\n",
            "loss at  636 is 1.8477887055755204\n",
            "loss at  637 is 1.847141173902438\n",
            "loss at  638 is 1.8464948359200923\n",
            "loss at  639 is 1.8458496878875106\n",
            "loss at  640 is 1.845205726079957\n",
            "loss at  641 is 1.844562946788832\n",
            "loss at  642 is 1.8439213463215771\n",
            "loss at  643 is 1.843280921001583\n",
            "loss at  644 is 1.8426416671680916\n",
            "loss at  645 is 1.8420035811761053\n",
            "loss at  646 is 1.8413666593962947\n",
            "loss at  647 is 1.8407308982149044\n",
            "loss at  648 is 1.8400962940336643\n",
            "loss at  649 is 1.8394628432696998\n",
            "loss at  650 is 1.8388305423554387\n",
            "loss at  651 is 1.8381993877385279\n",
            "loss at  652 is 1.8375693758817389\n",
            "loss at  653 is 1.8369405032628876\n",
            "loss at  654 is 1.8363127663747416\n",
            "loss at  655 is 1.8356861617249374\n",
            "loss at  656 is 1.8350606858358927\n",
            "loss at  657 is 1.8344363352447264\n",
            "loss at  658 is 1.83381310650317\n",
            "loss at  659 is 1.833190996177488\n",
            "loss at  660 is 1.8325700008483918\n",
            "loss at  661 is 1.831950117110961\n",
            "loss at  662 is 1.8313313415745605\n",
            "loss at  663 is 1.8307136708627612\n",
            "loss at  664 is 1.8300971016132592\n",
            "loss at  665 is 1.8294816304777965\n",
            "loss at  666 is 1.828867254122082\n",
            "loss at  667 is 1.828253969225715\n",
            "loss at  668 is 1.827641772482108\n",
            "loss at  669 is 1.827030660598406\n",
            "loss at  670 is 1.8264206302954156\n",
            "loss at  671 is 1.8258116783075269\n",
            "loss at  672 is 1.8252038013826382\n",
            "loss at  673 is 1.8245969962820818\n",
            "loss at  674 is 1.8239912597805523\n",
            "loss at  675 is 1.8233865886660292\n",
            "loss at  676 is 1.8227829797397082\n",
            "loss at  677 is 1.8221804298159283\n",
            "loss at  678 is 1.8215789357220997\n",
            "loss at  679 is 1.8209784942986293\n",
            "loss at  680 is 1.8203791023988596\n",
            "loss at  681 is 1.8197807568889885\n",
            "loss at  682 is 1.8191834546480077\n",
            "loss at  683 is 1.8185871925676267\n",
            "loss at  684 is 1.817991967552214\n",
            "loss at  685 is 1.8173977765187197\n",
            "loss at  686 is 1.816804616396614\n",
            "loss at  687 is 1.816212484127819\n",
            "loss at  688 is 1.8156213766666436\n",
            "loss at  689 is 1.8150312909797148\n",
            "loss at  690 is 1.8144422240459144\n",
            "loss at  691 is 1.8138541728563178\n",
            "loss at  692 is 1.8132671344141227\n",
            "loss at  693 is 1.8126811057345886\n",
            "loss at  694 is 1.8120960838449776\n",
            "loss at  695 is 1.8115120657844834\n",
            "loss at  696 is 1.8109290486041767\n",
            "loss at  697 is 1.8103470293669393\n",
            "loss at  698 is 1.8097660051474018\n",
            "loss at  699 is 1.8091859730318869\n",
            "loss at  700 is 1.808606930118344\n",
            "loss at  701 is 1.8080288735162926\n",
            "loss at  702 is 1.8074518003467606\n",
            "loss at  703 is 1.8068757077422273\n",
            "loss at  704 is 1.8063005928465616\n",
            "loss at  705 is 1.8057264528149664\n",
            "loss at  706 is 1.8051532848139185\n",
            "loss at  707 is 1.804581086021115\n",
            "loss at  708 is 1.804009853625409\n",
            "loss at  709 is 1.8034395848267606\n",
            "loss at  710 is 1.802870276836176\n",
            "loss at  711 is 1.8023019268756533\n",
            "loss at  712 is 1.8017345321781257\n",
            "loss at  713 is 1.8011680899874092\n",
            "loss at  714 is 1.8006025975581441\n",
            "loss at  715 is 1.8000380521557435\n",
            "loss at  716 is 1.7994744510563379\n",
            "loss at  717 is 1.7989117915467217\n",
            "loss at  718 is 1.7983500709243017\n",
            "loss at  719 is 1.7977892864970415\n",
            "loss at  720 is 1.7972294355834122\n",
            "loss at  721 is 1.7966705155123368\n",
            "loss at  722 is 1.7961125236231414\n",
            "loss at  723 is 1.7955554572655008\n",
            "loss at  724 is 1.7949993137993916\n",
            "loss at  725 is 1.7944440905950392\n",
            "loss at  726 is 1.7938897850328643\n",
            "loss at  727 is 1.7933363945034382\n",
            "loss at  728 is 1.7927839164074302\n",
            "loss at  729 is 1.7922323481555598\n",
            "loss at  730 is 1.7916816871685455\n",
            "loss at  731 is 1.7911319308770584\n",
            "loss at  732 is 1.790583076721672\n",
            "loss at  733 is 1.7900351221528148\n",
            "loss at  734 is 1.7894880646307223\n",
            "loss at  735 is 1.7889419016253922\n",
            "loss at  736 is 1.7883966306165318\n",
            "loss at  737 is 1.7878522490935158\n",
            "loss at  738 is 1.787308754555339\n",
            "loss at  739 is 1.7867661445105685\n",
            "loss at  740 is 1.7862244164772993\n",
            "loss at  741 is 1.7856835679831062\n",
            "loss at  742 is 1.7851435965650035\n",
            "loss at  743 is 1.7846044997693942\n",
            "loss at  744 is 1.7840662751520293\n",
            "loss at  745 is 1.783528920277961\n",
            "loss at  746 is 1.7829924327214992\n",
            "loss at  747 is 1.782456810066171\n",
            "loss at  748 is 1.7819220499046684\n",
            "loss at  749 is 1.7813881498388147\n",
            "loss at  750 is 1.7808551074795154\n",
            "loss at  751 is 1.7803229204467172\n",
            "loss at  752 is 1.7797915863693656\n",
            "loss at  753 is 1.7792611028853627\n",
            "loss at  754 is 1.7787314676415238\n",
            "loss at  755 is 1.7782026782935372\n",
            "loss at  756 is 1.7776747325059234\n",
            "loss at  757 is 1.77714762795199\n",
            "loss at  758 is 1.7766213623137954\n",
            "loss at  759 is 1.7760959332821042\n",
            "loss at  760 is 1.775571338556351\n",
            "loss at  761 is 1.7750475758445945\n",
            "loss at  762 is 1.7745246428634829\n",
            "loss at  763 is 1.7740025373382093\n",
            "loss at  764 is 1.7734812570024776\n",
            "loss at  765 is 1.7729607995984578\n",
            "loss at  766 is 1.7724411628767498\n",
            "loss at  767 is 1.7719223445963441\n",
            "loss at  768 is 1.7714043425245836\n",
            "loss at  769 is 1.7708871544371243\n",
            "loss at  770 is 1.7703707781178986\n",
            "loss at  771 is 1.7698552113590742\n",
            "loss at  772 is 1.76934045196102\n",
            "loss at  773 is 1.7688264977322672\n",
            "loss at  774 is 1.7683133464894716\n",
            "loss at  775 is 1.7678009960573766\n",
            "loss at  776 is 1.7672894442687777\n",
            "loss at  777 is 1.7667786889644834\n",
            "loss at  778 is 1.7662687279932816\n",
            "loss at  779 is 1.7657595592119002\n",
            "loss at  780 is 1.7652511804849762\n",
            "loss at  781 is 1.7647435896850125\n",
            "loss at  782 is 1.7642367846923492\n",
            "loss at  783 is 1.7637307633951242\n",
            "loss at  784 is 1.7632255236892405\n",
            "loss at  785 is 1.7627210634783275\n",
            "loss at  786 is 1.7622173806737123\n",
            "loss at  787 is 1.761714473194378\n",
            "loss at  788 is 1.7612123389669347\n",
            "loss at  789 is 1.760710975925583\n",
            "loss at  790 is 1.7602103820120798\n",
            "loss at  791 is 1.7597105551757064\n",
            "loss at  792 is 1.7592114933732312\n",
            "loss at  793 is 1.758713194568881\n",
            "loss at  794 is 1.7582156567343048\n",
            "loss at  795 is 1.7577188778485389\n",
            "loss at  796 is 1.7572228558979803\n",
            "loss at  797 is 1.756727588876346\n",
            "loss at  798 is 1.7562330747846475\n",
            "loss at  799 is 1.7557393116311544\n",
            "loss at  800 is 1.7552462974313623\n",
            "loss at  801 is 1.7547540302079627\n",
            "loss at  802 is 1.7542625079908114\n",
            "loss at  803 is 1.7537717288168937\n",
            "loss at  804 is 1.7532816907302962\n",
            "loss at  805 is 1.7527923917821735\n",
            "loss at  806 is 1.752303830030719\n",
            "loss at  807 is 1.7518160035411303\n",
            "loss at  808 is 1.7513289103855834\n",
            "loss at  809 is 1.7508425486431989\n",
            "loss at  810 is 1.750356916400012\n",
            "loss at  811 is 1.7498720117489408\n",
            "loss at  812 is 1.7493878327897605\n",
            "loss at  813 is 1.7489043776290685\n",
            "loss at  814 is 1.7484216443802578\n",
            "loss at  815 is 1.7479396311634858\n",
            "loss at  816 is 1.7474583361056457\n",
            "loss at  817 is 1.746977757340336\n",
            "loss at  818 is 1.7464978930078339\n",
            "loss at  819 is 1.7460187412550605\n",
            "loss at  820 is 1.7455403002355616\n",
            "loss at  821 is 1.745062568109467\n",
            "loss at  822 is 1.7445855430434738\n",
            "loss at  823 is 1.7441092232108086\n",
            "loss at  824 is 1.7436336067912048\n",
            "loss at  825 is 1.7431586919708708\n",
            "loss at  826 is 1.7426844769424665\n",
            "loss at  827 is 1.7422109599050715\n",
            "loss at  828 is 1.741738139064158\n",
            "loss at  829 is 1.741266012631565\n",
            "loss at  830 is 1.7407945788254706\n",
            "loss at  831 is 1.740323835870363\n",
            "loss at  832 is 1.7398537819970155\n",
            "loss at  833 is 1.7393844154424574\n",
            "loss at  834 is 1.7389157344499502\n",
            "loss at  835 is 1.7384477372689564\n",
            "loss at  836 is 1.7379804221551178\n",
            "loss at  837 is 1.7375137873702264\n",
            "loss at  838 is 1.7370478311821982\n",
            "loss at  839 is 1.736582551865048\n",
            "loss at  840 is 1.7361179476988637\n",
            "loss at  841 is 1.7356540169697772\n",
            "loss at  842 is 1.7351907579699442\n",
            "loss at  843 is 1.7347281689975127\n",
            "loss at  844 is 1.7342662483566025\n",
            "loss at  845 is 1.733804994357277\n",
            "loss at  846 is 1.733344405315518\n",
            "loss at  847 is 1.7328844795532032\n",
            "loss at  848 is 1.7324252153980775\n",
            "loss at  849 is 1.7319666111837315\n",
            "loss at  850 is 1.7315086652495741\n",
            "loss at  851 is 1.7310513759408102\n",
            "loss at  852 is 1.730594741608415\n",
            "loss at  853 is 1.7301387606091099\n",
            "loss at  854 is 1.7296834313053384\n",
            "loss at  855 is 1.7292287520652425\n",
            "loss at  856 is 1.728774721262637\n",
            "loss at  857 is 1.7283213372769886\n",
            "loss at  858 is 1.7278685984933884\n",
            "loss at  859 is 1.7274165033025326\n",
            "loss at  860 is 1.7269650501006957\n",
            "loss at  861 is 1.726514237289709\n",
            "loss at  862 is 1.7260640632769346\n",
            "loss at  863 is 1.725614526475248\n",
            "loss at  864 is 1.7251656253030074\n",
            "loss at  865 is 1.7247173581840378\n",
            "loss at  866 is 1.724269723547604\n",
            "loss at  867 is 1.7238227198283893\n",
            "loss at  868 is 1.7233763454664726\n",
            "loss at  869 is 1.7229305989073076\n",
            "loss at  870 is 1.7224854786016974\n",
            "loss at  871 is 1.7220409830057755\n",
            "loss at  872 is 1.7215971105809804\n",
            "loss at  873 is 1.721153859794037\n",
            "loss at  874 is 1.7207112291169322\n",
            "loss at  875 is 1.7202692170268947\n",
            "loss at  876 is 1.719827822006372\n",
            "loss at  877 is 1.7193870425430104\n",
            "loss at  878 is 1.7189468771296308\n",
            "loss at  879 is 1.718507324264212\n",
            "loss at  880 is 1.718068382449864\n",
            "loss at  881 is 1.7176300501948103\n",
            "loss at  882 is 1.7171923260123678\n",
            "loss at  883 is 1.7167552084209214\n",
            "loss at  884 is 1.7163186959439074\n",
            "loss at  885 is 1.7158827871097908\n",
            "loss at  886 is 1.7154474804520454\n",
            "loss at  887 is 1.715012774509134\n",
            "loss at  888 is 1.7145786678244845\n",
            "loss at  889 is 1.7141451589464725\n",
            "loss at  890 is 1.7137122464284034\n",
            "loss at  891 is 1.713279928828487\n",
            "loss at  892 is 1.7128482047098195\n",
            "loss at  893 is 1.7124170726403656\n",
            "loss at  894 is 1.7119865311929354\n",
            "loss at  895 is 1.7115565789451679\n",
            "loss at  896 is 1.7111272144795084\n",
            "loss at  897 is 1.7106984363831903\n",
            "loss at  898 is 1.7102702432482162\n",
            "loss at  899 is 1.7098426336713368\n",
            "loss at  900 is 1.709415606254034\n",
            "loss at  901 is 1.7089891596025002\n",
            "loss at  902 is 1.7085632923276177\n",
            "loss at  903 is 1.7081380030449427\n",
            "loss at  904 is 1.7077132903746848\n",
            "loss at  905 is 1.707289152941688\n",
            "loss at  906 is 1.7068655893754134\n",
            "loss at  907 is 1.706442598309918\n",
            "loss at  908 is 1.7060201783838382\n",
            "loss at  909 is 1.7055983282403706\n",
            "loss at  910 is 1.705177046527254\n",
            "loss at  911 is 1.7047563318967507\n",
            "loss at  912 is 1.7043361830056258\n",
            "loss at  913 is 1.7039165985151363\n",
            "loss at  914 is 1.7034975770910055\n",
            "loss at  915 is 1.703079117403406\n",
            "loss at  916 is 1.7026612181269476\n",
            "loss at  917 is 1.7022438779406524\n",
            "loss at  918 is 1.7018270955279413\n",
            "loss at  919 is 1.7014108695766157\n",
            "loss at  920 is 1.700995198778838\n",
            "loss at  921 is 1.7005800818311165\n",
            "loss at  922 is 1.7001655174342871\n",
            "loss at  923 is 1.699751504293494\n",
            "loss at  924 is 1.699338041118176\n",
            "loss at  925 is 1.6989251266220486\n",
            "loss at  926 is 1.6985127595230838\n",
            "loss at  927 is 1.6981009385434944\n",
            "loss at  928 is 1.6976896624097202\n",
            "loss at  929 is 1.6972789298524078\n",
            "loss at  930 is 1.6968687396063942\n",
            "loss at  931 is 1.69645909041069\n",
            "loss at  932 is 1.6960499810084653\n",
            "loss at  933 is 1.6956414101470292\n",
            "loss at  934 is 1.6952333765778151\n",
            "loss at  935 is 1.694825879056367\n",
            "loss at  936 is 1.6944189163423162\n",
            "loss at  937 is 1.694012487199372\n",
            "loss at  938 is 1.693606590395303\n",
            "loss at  939 is 1.69320122470192\n",
            "loss at  940 is 1.692796388895059\n",
            "loss at  941 is 1.6923920817545688\n",
            "loss at  942 is 1.6919883020642923\n",
            "loss at  943 is 1.6915850486120516\n",
            "loss at  944 is 1.6911823201896319\n",
            "loss at  945 is 1.6907801155927642\n",
            "loss at  946 is 1.6903784336211127\n",
            "loss at  947 is 1.6899772730782578\n",
            "loss at  948 is 1.6895766327716786\n",
            "loss at  949 is 1.6891765115127415\n",
            "loss at  950 is 1.6887769081166808\n",
            "loss at  951 is 1.688377821402585\n",
            "loss at  952 is 1.6879792501933821\n",
            "loss at  953 is 1.6875811933158236\n",
            "loss at  954 is 1.6871836496004697\n",
            "loss at  955 is 1.6867866178816733\n",
            "loss at  956 is 1.6863900969975654\n",
            "loss at  957 is 1.6859940857900415\n",
            "loss at  958 is 1.6855985831047444\n",
            "loss at  959 is 1.6852035877910514\n",
            "loss at  960 is 1.6848090987020574\n",
            "loss at  961 is 1.6844151146945616\n",
            "loss at  962 is 1.6840216346290529\n",
            "loss at  963 is 1.6836286573696937\n",
            "loss at  964 is 1.6832361817843091\n",
            "loss at  965 is 1.6828442067443667\n",
            "loss at  966 is 1.6824527311249662\n",
            "loss at  967 is 1.6820617538048246\n",
            "loss at  968 is 1.6816712736662611\n",
            "loss at  969 is 1.6812812895951816\n",
            "loss at  970 is 1.6808918004810678\n",
            "loss at  971 is 1.6805028052169593\n",
            "loss at  972 is 1.6801143026994427\n",
            "loss at  973 is 1.6797262918286346\n",
            "loss at  974 is 1.6793387715081693\n",
            "loss at  975 is 1.6789517406451857\n",
            "loss at  976 is 1.6785651981503105\n",
            "loss at  977 is 1.6781791429376476\n",
            "loss at  978 is 1.6777935739247627\n",
            "loss at  979 is 1.6774084900326667\n",
            "loss at  980 is 1.6770238901858092\n",
            "loss at  981 is 1.6766397733120575\n",
            "loss at  982 is 1.6762561383426873\n",
            "loss at  983 is 1.6758729842123676\n",
            "loss at  984 is 1.6754903098591472\n",
            "loss at  985 is 1.6751081142244426\n",
            "loss at  986 is 1.6747263962530214\n",
            "loss at  987 is 1.6743451548929937\n",
            "loss at  988 is 1.6739643890957936\n",
            "loss at  989 is 1.67358409781617\n",
            "loss at  990 is 1.673204280012172\n",
            "loss at  991 is 1.6728249346451347\n",
            "loss at  992 is 1.6724460606796687\n",
            "loss at  993 is 1.6720676570836424\n",
            "loss at  994 is 1.671689722828175\n",
            "loss at  995 is 1.671312256887619\n",
            "loss at  996 is 1.6709352582395482\n",
            "loss at  997 is 1.6705587258647467\n",
            "loss at  998 is 1.6701826587471935\n",
            "loss at  999 is 1.6698070558740512\n",
            "loss at  1000 is 1.6694319162356541\n",
            "loss at  1001 is 1.6690572388254934\n",
            "loss at  1002 is 1.6686830226402043\n",
            "loss at  1003 is 1.668309266679557\n",
            "loss at  1004 is 1.6679359699464398\n",
            "loss at  1005 is 1.6675631314468498\n",
            "loss at  1006 is 1.667190750189878\n",
            "loss at  1007 is 1.6668188251876987\n",
            "loss at  1008 is 1.6664473554555563\n",
            "loss at  1009 is 1.666076340011753\n",
            "loss at  1010 is 1.6657057778776365\n",
            "loss at  1011 is 1.6653356680775877\n",
            "loss at  1012 is 1.6649660096390084\n",
            "loss at  1013 is 1.6645968015923098\n",
            "loss at  1014 is 1.664228042970897\n",
            "loss at  1015 is 1.6638597328111657\n",
            "loss at  1016 is 1.6634918701524781\n",
            "loss at  1017 is 1.6631244540371601\n",
            "loss at  1018 is 1.6627574835104861\n",
            "loss at  1019 is 1.662390957620665\n",
            "loss at  1020 is 1.6620248754188323\n",
            "loss at  1021 is 1.6616592359590365\n",
            "loss at  1022 is 1.6612940382982262\n",
            "loss at  1023 is 1.6609292814962404\n",
            "loss at  1024 is 1.6605649646157936\n",
            "loss at  1025 is 1.6602010867224684\n",
            "loss at  1026 is 1.6598376468847014\n",
            "loss at  1027 is 1.6594746441737702\n",
            "loss at  1028 is 1.659112077663785\n",
            "loss at  1029 is 1.6587499464316748\n",
            "loss at  1030 is 1.6583882495571771\n",
            "loss at  1031 is 1.6580269861228256\n",
            "loss at  1032 is 1.6576661552139396\n",
            "loss at  1033 is 1.6573057559186113\n",
            "loss at  1034 is 1.6569457873276958\n",
            "loss at  1035 is 1.656586248534802\n",
            "loss at  1036 is 1.656227138636274\n",
            "loss at  1037 is 1.6558684567311872\n",
            "loss at  1038 is 1.655510201921336\n",
            "loss at  1039 is 1.6551523733112175\n",
            "loss at  1040 is 1.654794970008028\n",
            "loss at  1041 is 1.6544379911216458\n",
            "loss at  1042 is 1.654081435764623\n",
            "loss at  1043 is 1.653725303052174\n",
            "loss at  1044 is 1.6533695921021652\n",
            "loss at  1045 is 1.6530143020351027\n",
            "loss at  1046 is 1.6526594319741221\n",
            "loss at  1047 is 1.6523049810449786\n",
            "loss at  1048 is 1.6519509483760357\n",
            "loss at  1049 is 1.6515973330982532\n",
            "loss at  1050 is 1.6512441343451776\n",
            "loss at  1051 is 1.6508913512529315\n",
            "loss at  1052 is 1.6505389829602037\n",
            "loss at  1053 is 1.6501870286082372\n",
            "loss at  1054 is 1.6498354873408185\n",
            "loss at  1055 is 1.6494843583042693\n",
            "loss at  1056 is 1.6491336406474344\n",
            "loss at  1057 is 1.6487833335216702\n",
            "loss at  1058 is 1.6484334360808355\n",
            "loss at  1059 is 1.6480839474812843\n",
            "loss at  1060 is 1.6477348668818474\n",
            "loss at  1061 is 1.6473861934438336\n",
            "loss at  1062 is 1.647037926331008\n",
            "loss at  1063 is 1.6466900647095886\n",
            "loss at  1064 is 1.6463426077482348\n",
            "loss at  1065 is 1.6459955546180371\n",
            "loss at  1066 is 1.6456489044925076\n",
            "loss at  1067 is 1.6453026565475672\n",
            "loss at  1068 is 1.6449568099615404\n",
            "loss at  1069 is 1.644611363915141\n",
            "loss at  1070 is 1.6442663175914651\n",
            "loss at  1071 is 1.64392167017598\n",
            "loss at  1072 is 1.6435774208565135\n",
            "loss at  1073 is 1.6432335688232471\n",
            "loss at  1074 is 1.6428901132687033\n",
            "loss at  1075 is 1.6425470533877375\n",
            "loss at  1076 is 1.6422043883775275\n",
            "loss at  1077 is 1.641862117437564\n",
            "loss at  1078 is 1.6415202397696433\n",
            "loss at  1079 is 1.6411787545778536\n",
            "loss at  1080 is 1.6408376610685698\n",
            "loss at  1081 is 1.6404969584504403\n",
            "loss at  1082 is 1.6401566459343808\n",
            "loss at  1083 is 1.6398167227335638\n",
            "loss at  1084 is 1.6394771880634074\n",
            "loss at  1085 is 1.6391380411415706\n",
            "loss at  1086 is 1.6387992811879395\n",
            "loss at  1087 is 1.6384609074246195\n",
            "loss at  1088 is 1.6381229190759283\n",
            "loss at  1089 is 1.6377853153683841\n",
            "loss at  1090 is 1.6374480955306985\n",
            "loss at  1091 is 1.637111258793766\n",
            "loss at  1092 is 1.6367748043906567\n",
            "loss at  1093 is 1.6364387315566047\n",
            "loss at  1094 is 1.6361030395290033\n",
            "loss at  1095 is 1.6357677275473932\n",
            "loss at  1096 is 1.635432794853453\n",
            "loss at  1097 is 1.6350982406909946\n",
            "loss at  1098 is 1.6347640643059502\n",
            "loss at  1099 is 1.6344302649463673\n",
            "loss at  1100 is 1.6340968418623947\n",
            "loss at  1101 is 1.6337637943062815\n",
            "loss at  1102 is 1.6334311215323638\n",
            "loss at  1103 is 1.6330988227970542\n",
            "loss at  1104 is 1.6327668973588403\n",
            "loss at  1105 is 1.6324353444782698\n",
            "loss at  1106 is 1.6321041634179467\n",
            "loss at  1107 is 1.6317733534425205\n",
            "loss at  1108 is 1.6314429138186777\n",
            "loss at  1109 is 1.6311128438151363\n",
            "loss at  1110 is 1.6307831427026345\n",
            "loss at  1111 is 1.6304538097539252\n",
            "loss at  1112 is 1.6301248442437668\n",
            "loss at  1113 is 1.6297962454489152\n",
            "loss at  1114 is 1.6294680126481167\n",
            "loss at  1115 is 1.629140145122098\n",
            "loss at  1116 is 1.6288126421535616\n",
            "loss at  1117 is 1.628485503027175\n",
            "loss at  1118 is 1.6281587270295659\n",
            "loss at  1119 is 1.62783231344931\n",
            "loss at  1120 is 1.6275062615769291\n",
            "loss at  1121 is 1.6271805707048792\n",
            "loss at  1122 is 1.626855240127545\n",
            "loss at  1123 is 1.6265302691412302\n",
            "loss at  1124 is 1.6262056570441545\n",
            "loss at  1125 is 1.62588140313644\n",
            "loss at  1126 is 1.62555750672011\n",
            "loss at  1127 is 1.625233967099077\n",
            "loss at  1128 is 1.6249107835791383\n",
            "loss at  1129 is 1.624587955467966\n",
            "loss at  1130 is 1.6242654820751037\n",
            "loss at  1131 is 1.6239433627119562\n",
            "loss at  1132 is 1.623621596691784\n",
            "loss at  1133 is 1.6233001833296936\n",
            "loss at  1134 is 1.6229791219426342\n",
            "loss at  1135 is 1.6226584118493914\n",
            "loss at  1136 is 1.622338052370573\n",
            "loss at  1137 is 1.6220180428286104\n",
            "loss at  1138 is 1.6216983825477487\n",
            "loss at  1139 is 1.6213790708540377\n",
            "loss at  1140 is 1.6210601070753299\n",
            "loss at  1141 is 1.6207414905412685\n",
            "loss at  1142 is 1.6204232205832856\n",
            "loss at  1143 is 1.620105296534591\n",
            "loss at  1144 is 1.6197877177301718\n",
            "loss at  1145 is 1.6194704835067788\n",
            "loss at  1146 is 1.6191535932029246\n",
            "loss at  1147 is 1.6188370461588757\n",
            "loss at  1148 is 1.618520841716647\n",
            "loss at  1149 is 1.6182049792199946\n",
            "loss at  1150 is 1.6178894580144094\n",
            "loss at  1151 is 1.6175742774471127\n",
            "loss at  1152 is 1.617259436867047\n",
            "loss at  1153 is 1.6169449356248713\n",
            "loss at  1154 is 1.6166307730729559\n",
            "loss at  1155 is 1.6163169485653759\n",
            "loss at  1156 is 1.616003461457904\n",
            "loss at  1157 is 1.6156903111080057\n",
            "loss at  1158 is 1.6153774968748318\n",
            "loss at  1159 is 1.615065018119216\n",
            "loss at  1160 is 1.614752874203664\n",
            "loss at  1161 is 1.6144410644923528\n",
            "loss at  1162 is 1.6141295883511206\n",
            "loss at  1163 is 1.6138184451474633\n",
            "loss at  1164 is 1.6135076342505283\n",
            "loss at  1165 is 1.6131971550311097\n",
            "loss at  1166 is 1.6128870068616405\n",
            "loss at  1167 is 1.6125771891161904\n",
            "loss at  1168 is 1.6122677011704563\n",
            "loss at  1169 is 1.6119585424017595\n",
            "loss at  1170 is 1.6116497121890399\n",
            "loss at  1171 is 1.611341209912849\n",
            "loss at  1172 is 1.6110330349553454\n",
            "loss at  1173 is 1.6107251867002923\n",
            "loss at  1174 is 1.6104176645330464\n",
            "loss at  1175 is 1.610110467840556\n",
            "loss at  1176 is 1.609803596011357\n",
            "loss at  1177 is 1.6094970484355648\n",
            "loss at  1178 is 1.6091908245048716\n",
            "loss at  1179 is 1.6088849236125375\n",
            "loss at  1180 is 1.6085793451533916\n",
            "loss at  1181 is 1.6082740885238207\n",
            "loss at  1182 is 1.6079691531217661\n",
            "loss at  1183 is 1.607664538346722\n",
            "loss at  1184 is 1.6073602435997245\n",
            "loss at  1185 is 1.6070562682833534\n",
            "loss at  1186 is 1.6067526118017212\n",
            "loss at  1187 is 1.6064492735604707\n",
            "loss at  1188 is 1.6061462529667712\n",
            "loss at  1189 is 1.605843549429312\n",
            "loss at  1190 is 1.6055411623582998\n",
            "loss at  1191 is 1.6052390911654502\n",
            "loss at  1192 is 1.6049373352639864\n",
            "loss at  1193 is 1.6046358940686327\n",
            "loss at  1194 is 1.6043347669956112\n",
            "loss at  1195 is 1.6040339534626344\n",
            "loss at  1196 is 1.6037334528889047\n",
            "loss at  1197 is 1.6034332646951055\n",
            "loss at  1198 is 1.6031333883034014\n",
            "loss at  1199 is 1.6028338231374273\n",
            "loss at  1200 is 1.6025345686222903\n",
            "loss at  1201 is 1.6022356241845614\n",
            "loss at  1202 is 1.6019369892522723\n",
            "loss at  1203 is 1.6016386632549096\n",
            "loss at  1204 is 1.601340645623414\n",
            "loss at  1205 is 1.6010429357901708\n",
            "loss at  1206 is 1.600745533189009\n",
            "loss at  1207 is 1.6004484372551968\n",
            "loss at  1208 is 1.6001516474254363\n",
            "loss at  1209 is 1.5998551631378584\n",
            "loss at  1210 is 1.599558983832021\n",
            "loss at  1211 is 1.599263108948902\n",
            "loss at  1212 is 1.5989675379308983\n",
            "loss at  1213 is 1.5986722702218177\n",
            "loss at  1214 is 1.598377305266878\n",
            "loss at  1215 is 1.5980826425127013\n",
            "loss at  1216 is 1.5977882814073108\n",
            "loss at  1217 is 1.5974942214001242\n",
            "loss at  1218 is 1.5972004619419529\n",
            "loss at  1219 is 1.5969070024849976\n",
            "loss at  1220 is 1.5966138424828402\n",
            "loss at  1221 is 1.5963209813904442\n",
            "loss at  1222 is 1.5960284186641496\n",
            "loss at  1223 is 1.5957361537616677\n",
            "loss at  1224 is 1.5954441861420787\n",
            "loss at  1225 is 1.5951525152658261\n",
            "loss at  1226 is 1.594861140594713\n",
            "loss at  1227 is 1.5945700615919014\n",
            "loss at  1228 is 1.5942792777219021\n",
            "loss at  1229 is 1.5939887884505777\n",
            "loss at  1230 is 1.5936985932451324\n",
            "loss at  1231 is 1.5934086915741141\n",
            "loss at  1232 is 1.5931190829074038\n",
            "loss at  1233 is 1.5928297667162201\n",
            "loss at  1234 is 1.5925407424731075\n",
            "loss at  1235 is 1.5922520096519373\n",
            "loss at  1236 is 1.5919635677279023\n",
            "loss at  1237 is 1.5916754161775128\n",
            "loss at  1238 is 1.591387554478594\n",
            "loss at  1239 is 1.5910999821102814\n",
            "loss at  1240 is 1.5908126985530162\n",
            "loss at  1241 is 1.590525703288545\n",
            "loss at  1242 is 1.5902389957999115\n",
            "loss at  1243 is 1.5899525755714556\n",
            "loss at  1244 is 1.58966644208881\n",
            "loss at  1245 is 1.5893805948388955\n",
            "loss at  1246 is 1.5890950333099159\n",
            "loss at  1247 is 1.5888097569913595\n",
            "loss at  1248 is 1.5885247653739896\n",
            "loss at  1249 is 1.588240057949845\n",
            "loss at  1250 is 1.5879556342122314\n",
            "loss at  1251 is 1.5876714936557264\n",
            "loss at  1252 is 1.5873876357761671\n",
            "loss at  1253 is 1.5871040600706512\n",
            "loss at  1254 is 1.5868207660375335\n",
            "loss at  1255 is 1.58653775317642\n",
            "loss at  1256 is 1.5862550209881665\n",
            "loss at  1257 is 1.5859725689748747\n",
            "loss at  1258 is 1.5856903966398876\n",
            "loss at  1259 is 1.5854085034877876\n",
            "loss at  1260 is 1.5851268890243917\n",
            "loss at  1261 is 1.5848455527567489\n",
            "loss at  1262 is 1.584564494193136\n",
            "loss at  1263 is 1.5842837128430551\n",
            "loss at  1264 is 1.5840032082172308\n",
            "loss at  1265 is 1.583722979827602\n",
            "loss at  1266 is 1.5834430271873265\n",
            "loss at  1267 is 1.5831633498107707\n",
            "loss at  1268 is 1.5828839472135094\n",
            "loss at  1269 is 1.582604818912322\n",
            "loss at  1270 is 1.5823259644251888\n",
            "loss at  1271 is 1.5820473832712876\n",
            "loss at  1272 is 1.5817690749709918\n",
            "loss at  1273 is 1.5814910390458619\n",
            "loss at  1274 is 1.5812132750186507\n",
            "loss at  1275 is 1.5809357824132941\n",
            "loss at  1276 is 1.580658560754907\n",
            "loss at  1277 is 1.5803816095697836\n",
            "loss at  1278 is 1.580104928385392\n",
            "loss at  1279 is 1.5798285167303725\n",
            "loss at  1280 is 1.579552374134531\n",
            "loss at  1281 is 1.57927650012884\n",
            "loss at  1282 is 1.579000894245433\n",
            "loss at  1283 is 1.5787255560176003\n",
            "loss at  1284 is 1.5784504849797885\n",
            "loss at  1285 is 1.5781756806675944\n",
            "loss at  1286 is 1.5779011426177645\n",
            "loss at  1287 is 1.5776268703681893\n",
            "loss at  1288 is 1.5773528634579008\n",
            "loss at  1289 is 1.5770791214270716\n",
            "loss at  1290 is 1.5768056438170075\n",
            "loss at  1291 is 1.5765324301701482\n",
            "loss at  1292 is 1.576259480030061\n",
            "loss at  1293 is 1.5759867929414413\n",
            "loss at  1294 is 1.5757143684501054\n",
            "loss at  1295 is 1.5754422061029885\n",
            "loss at  1296 is 1.575170305448145\n",
            "loss at  1297 is 1.57489866603474\n",
            "loss at  1298 is 1.5746272874130494\n",
            "loss at  1299 is 1.5743561691344565\n",
            "loss at  1300 is 1.5740853107514483\n",
            "loss at  1301 is 1.5738147118176122\n",
            "loss at  1302 is 1.5735443718876339\n",
            "loss at  1303 is 1.5732742905172927\n",
            "loss at  1304 is 1.57300446726346\n",
            "loss at  1305 is 1.572734901684095\n",
            "loss at  1306 is 1.572465593338242\n",
            "loss at  1307 is 1.5721965417860293\n",
            "loss at  1308 is 1.5719277465886619\n",
            "loss at  1309 is 1.5716592073084212\n",
            "loss at  1310 is 1.5713909235086623\n",
            "loss at  1311 is 1.5711228947538105\n",
            "loss at  1312 is 1.570855120609357\n",
            "loss at  1313 is 1.570587600641857\n",
            "loss at  1314 is 1.5703203344189276\n",
            "loss at  1315 is 1.5700533215092423\n",
            "loss at  1316 is 1.569786561482529\n",
            "loss at  1317 is 1.5695200539095706\n",
            "loss at  1318 is 1.569253798362193\n",
            "loss at  1319 is 1.5689877944132733\n",
            "loss at  1320 is 1.5687220416367291\n",
            "loss at  1321 is 1.5684565396075176\n",
            "loss at  1322 is 1.568191287901633\n",
            "loss at  1323 is 1.5679262860961038\n",
            "loss at  1324 is 1.5676615337689879\n",
            "loss at  1325 is 1.5673970304993734\n",
            "loss at  1326 is 1.5671327758673712\n",
            "loss at  1327 is 1.5668687694541155\n",
            "loss at  1328 is 1.5666050108417595\n",
            "loss at  1329 is 1.5663414996134724\n",
            "loss at  1330 is 1.566078235353436\n",
            "loss at  1331 is 1.5658152176468436\n",
            "loss at  1332 is 1.5655524460798935\n",
            "loss at  1333 is 1.5652899202397927\n",
            "loss at  1334 is 1.565027639714746\n",
            "loss at  1335 is 1.5647656040939584\n",
            "loss at  1336 is 1.564503812967631\n",
            "loss at  1337 is 1.5642422659269568\n",
            "loss at  1338 is 1.5639809625641203\n",
            "loss at  1339 is 1.5637199024722925\n",
            "loss at  1340 is 1.5634590852456287\n",
            "loss at  1341 is 1.5631985104792663\n",
            "loss at  1342 is 1.5629381777693203\n",
            "loss at  1343 is 1.5626780867128827\n",
            "loss at  1344 is 1.562418236908018\n",
            "loss at  1345 is 1.5621586279537614\n",
            "loss at  1346 is 1.5618992594501149\n",
            "loss at  1347 is 1.5616401309980448\n",
            "loss at  1348 is 1.561381242199481\n",
            "loss at  1349 is 1.5611225926573107\n",
            "loss at  1350 is 1.5608641819753772\n",
            "loss at  1351 is 1.5606060097584793\n",
            "loss at  1352 is 1.5603480756123633\n",
            "loss at  1353 is 1.560090379143727\n",
            "loss at  1354 is 1.5598329199602103\n",
            "loss at  1355 is 1.5595756976703974\n",
            "loss at  1356 is 1.559318711883811\n",
            "loss at  1357 is 1.5590619622109116\n",
            "loss at  1358 is 1.558805448263094\n",
            "loss at  1359 is 1.558549169652684\n",
            "loss at  1360 is 1.5582931259929356\n",
            "loss at  1361 is 1.5580373168980297\n",
            "loss at  1362 is 1.55778174198307\n",
            "loss at  1363 is 1.557526400864082\n",
            "loss at  1364 is 1.5572712931580084\n",
            "loss at  1365 is 1.5570164184827058\n",
            "loss at  1366 is 1.5567617764569457\n",
            "loss at  1367 is 1.556507366700409\n",
            "loss at  1368 is 1.5562531888336832\n",
            "loss at  1369 is 1.5559992424782605\n",
            "loss at  1370 is 1.5557455272565357\n",
            "loss at  1371 is 1.5554920427918018\n",
            "loss at  1372 is 1.55523878870825\n",
            "loss at  1373 is 1.5549857646309648\n",
            "loss at  1374 is 1.5547329701859218\n",
            "loss at  1375 is 1.5544804049999867\n",
            "loss at  1376 is 1.5542280687009102\n",
            "loss at  1377 is 1.5539759609173276\n",
            "loss at  1378 is 1.5537240812787543\n",
            "loss at  1379 is 1.553472429415585\n",
            "loss at  1380 is 1.5532210049590898\n",
            "loss at  1381 is 1.5529698075414131\n",
            "loss at  1382 is 1.5527188367955687\n",
            "loss at  1383 is 1.5524680923554395\n",
            "loss at  1384 is 1.5522175738557742\n",
            "loss at  1385 is 1.5519672809321834\n",
            "loss at  1386 is 1.5517172132211405\n",
            "loss at  1387 is 1.551467370359976\n",
            "loss at  1388 is 1.5512177519868746\n",
            "loss at  1389 is 1.550968357740876\n",
            "loss at  1390 is 1.5507191872618695\n",
            "loss at  1391 is 1.5504702401905928\n",
            "loss at  1392 is 1.550221516168628\n",
            "loss at  1393 is 1.5499730148384023\n",
            "loss at  1394 is 1.5497247358431825\n",
            "loss at  1395 is 1.5494766788270737\n",
            "loss at  1396 is 1.5492288434350157\n",
            "loss at  1397 is 1.5489812293127831\n",
            "loss at  1398 is 1.5487338361069811\n",
            "loss at  1399 is 1.5484866634650418\n",
            "loss at  1400 is 1.5482397110352235\n",
            "loss at  1401 is 1.5479929784666104\n",
            "loss at  1402 is 1.5477464654091047\n",
            "loss at  1403 is 1.5475001715134287\n",
            "loss at  1404 is 1.5472540964311214\n",
            "loss at  1405 is 1.5470082398145342\n",
            "loss at  1406 is 1.5467626013168312\n",
            "loss at  1407 is 1.546517180591985\n",
            "loss at  1408 is 1.5462719772947755\n",
            "loss at  1409 is 1.5460269910807865\n",
            "loss at  1410 is 1.545782221606403\n",
            "loss at  1411 is 1.5455376685288111\n",
            "loss at  1412 is 1.5452933315059934\n",
            "loss at  1413 is 1.5450492101967277\n",
            "loss at  1414 is 1.5448053042605834\n",
            "loss at  1415 is 1.5445616133579227\n",
            "loss at  1416 is 1.5443181371498917\n",
            "loss at  1417 is 1.544074875298426\n",
            "loss at  1418 is 1.5438318274662437\n",
            "loss at  1419 is 1.5435889933168412\n",
            "loss at  1420 is 1.543346372514497\n",
            "loss at  1421 is 1.5431039647242646\n",
            "loss at  1422 is 1.542861769611972\n",
            "loss at  1423 is 1.5426197868442197\n",
            "loss at  1424 is 1.5423780160883755\n",
            "loss at  1425 is 1.5421364570125777\n",
            "loss at  1426 is 1.541895109285727\n",
            "loss at  1427 is 1.5416539725774898\n",
            "loss at  1428 is 1.5414130465582905\n",
            "loss at  1429 is 1.541172330899314\n",
            "loss at  1430 is 1.5409318252725002\n",
            "loss at  1431 is 1.5406915293505432\n",
            "loss at  1432 is 1.540451442806889\n",
            "loss at  1433 is 1.5402115653157344\n",
            "loss at  1434 is 1.539971896552021\n",
            "loss at  1435 is 1.5397324361914388\n",
            "loss at  1436 is 1.5394931839104182\n",
            "loss at  1437 is 1.5392541393861319\n",
            "loss at  1438 is 1.5390153022964908\n",
            "loss at  1439 is 1.5387766723201421\n",
            "loss at  1440 is 1.538538249136469\n",
            "loss at  1441 is 1.5383000324255842\n",
            "loss at  1442 is 1.5380620218683336\n",
            "loss at  1443 is 1.537824217146288\n",
            "loss at  1444 is 1.537586617941747\n",
            "loss at  1445 is 1.5373492239377327\n",
            "loss at  1446 is 1.5371120348179876\n",
            "loss at  1447 is 1.5368750502669766\n",
            "loss at  1448 is 1.5366382699698784\n",
            "loss at  1449 is 1.5364016936125904\n",
            "loss at  1450 is 1.5361653208817219\n",
            "loss at  1451 is 1.5359291514645927\n",
            "loss at  1452 is 1.5356931850492335\n",
            "loss at  1453 is 1.53545742132438\n",
            "loss at  1454 is 1.5352218599794747\n",
            "loss at  1455 is 1.5349865007046615\n",
            "loss at  1456 is 1.5347513431907864\n",
            "loss at  1457 is 1.5345163871293943\n",
            "loss at  1458 is 1.534281632212726\n",
            "loss at  1459 is 1.5340470781337183\n",
            "loss at  1460 is 1.533812724586\n",
            "loss at  1461 is 1.5335785712638905\n",
            "loss at  1462 is 1.5333446178623986\n",
            "loss at  1463 is 1.5331108640772209\n",
            "loss at  1464 is 1.5328773096047363\n",
            "loss at  1465 is 1.532643954142009\n",
            "loss at  1466 is 1.5324107973867818\n",
            "loss at  1467 is 1.5321778390374787\n",
            "loss at  1468 is 1.531945078793198\n",
            "loss at  1469 is 1.531712516353717\n",
            "loss at  1470 is 1.5314801514194818\n",
            "loss at  1471 is 1.5312479836916124\n",
            "loss at  1472 is 1.531016012871895\n",
            "loss at  1473 is 1.5307842386627866\n",
            "loss at  1474 is 1.5305526607674074\n",
            "loss at  1475 is 1.530321278889541\n",
            "loss at  1476 is 1.5300900927336338\n",
            "loss at  1477 is 1.5298591020047894\n",
            "loss at  1478 is 1.5296283064087715\n",
            "loss at  1479 is 1.5293977056519978\n",
            "loss at  1480 is 1.529167299441542\n",
            "loss at  1481 is 1.5289370874851265\n",
            "loss at  1482 is 1.5287070694911271\n",
            "loss at  1483 is 1.528477245168566\n",
            "loss at  1484 is 1.5282476142271126\n",
            "loss at  1485 is 1.5280181763770793\n",
            "loss at  1486 is 1.5277889313294242\n",
            "loss at  1487 is 1.5275598787957438\n",
            "loss at  1488 is 1.5273310184882745\n",
            "loss at  1489 is 1.5271023501198895\n",
            "loss at  1490 is 1.5268738734040979\n",
            "loss at  1491 is 1.5266455880550418\n",
            "loss at  1492 is 1.5264174937874944\n",
            "loss at  1493 is 1.5261895903168616\n",
            "loss at  1494 is 1.525961877359174\n",
            "loss at  1495 is 1.5257343546310915\n",
            "loss at  1496 is 1.525507021849896\n",
            "loss at  1497 is 1.5252798787334936\n",
            "loss at  1498 is 1.525052925000411\n",
            "loss at  1499 is 1.5248261603697943\n",
            "loss at  1500 is 1.5245995845614084\n",
            "loss at  1501 is 1.5243731972956307\n",
            "loss at  1502 is 1.5241469982934557\n",
            "loss at  1503 is 1.5239209872764876\n",
            "loss at  1504 is 1.5236951639669427\n",
            "loss at  1505 is 1.5234695280876467\n",
            "loss at  1506 is 1.5232440793620303\n",
            "loss at  1507 is 1.5230188175141308\n",
            "loss at  1508 is 1.5227937422685882\n",
            "loss at  1509 is 1.5225688533506458\n",
            "loss at  1510 is 1.5223441504861455\n",
            "loss at  1511 is 1.5221196334015288\n",
            "loss at  1512 is 1.5218953018238328\n",
            "loss at  1513 is 1.5216711554806912\n",
            "loss at  1514 is 1.52144719410033\n",
            "loss at  1515 is 1.521223417411567\n",
            "loss at  1516 is 1.5209998251438113\n",
            "loss at  1517 is 1.5207764170270577\n",
            "loss at  1518 is 1.5205531927918914\n",
            "loss at  1519 is 1.5203301521694796\n",
            "loss at  1520 is 1.5201072948915741\n",
            "loss at  1521 is 1.5198846206905086\n",
            "loss at  1522 is 1.5196621292991974\n",
            "loss at  1523 is 1.519439820451132\n",
            "loss at  1524 is 1.5192176938803812\n",
            "loss at  1525 is 1.5189957493215909\n",
            "loss at  1526 is 1.5187739865099783\n",
            "loss at  1527 is 1.518552405181334\n",
            "loss at  1528 is 1.518331005072019\n",
            "loss at  1529 is 1.5181097859189616\n",
            "loss at  1530 is 1.5178887474596596\n",
            "loss at  1531 is 1.5176678894321747\n",
            "loss at  1532 is 1.517447211575135\n",
            "loss at  1533 is 1.5172267136277278\n",
            "loss at  1534 is 1.5170063953297055\n",
            "loss at  1535 is 1.516786256421375\n",
            "loss at  1536 is 1.5165662966436055\n",
            "loss at  1537 is 1.51634651573782\n",
            "loss at  1538 is 1.5161269134459967\n",
            "loss at  1539 is 1.515907489510667\n",
            "loss at  1540 is 1.5156882436749155\n",
            "loss at  1541 is 1.5154691756823733\n",
            "loss at  1542 is 1.515250285277224\n",
            "loss at  1543 is 1.5150315722041958\n",
            "loss at  1544 is 1.5148130362085632\n",
            "loss at  1545 is 1.5145946770361451\n",
            "loss at  1546 is 1.5143764944333025\n",
            "loss at  1547 is 1.5141584881469368\n",
            "loss at  1548 is 1.5139406579244907\n",
            "loss at  1549 is 1.513723003513943\n",
            "loss at  1550 is 1.5135055246638105\n",
            "loss at  1551 is 1.5132882211231442\n",
            "loss at  1552 is 1.5130710926415287\n",
            "loss at  1553 is 1.5128541389690815\n",
            "loss at  1554 is 1.5126373598564489\n",
            "loss at  1555 is 1.5124207550548094\n",
            "loss at  1556 is 1.512204324315867\n",
            "loss at  1557 is 1.511988067391852\n",
            "loss at  1558 is 1.5117719840355197\n",
            "loss at  1559 is 1.5115560740001504\n",
            "loss at  1560 is 1.5113403370395442\n",
            "loss at  1561 is 1.5111247729080224\n",
            "loss at  1562 is 1.5109093813604264\n",
            "loss at  1563 is 1.510694162152113\n",
            "loss at  1564 is 1.5104791150389585\n",
            "loss at  1565 is 1.5102642397773502\n",
            "loss at  1566 is 1.5100495361241921\n",
            "loss at  1567 is 1.5098350038368986\n",
            "loss at  1568 is 1.5096206426733951\n",
            "loss at  1569 is 1.509406452392116\n",
            "loss at  1570 is 1.5091924327520039\n",
            "loss at  1571 is 1.5089785835125078\n",
            "loss at  1572 is 1.50876490443358\n",
            "loss at  1573 is 1.5085513952756804\n",
            "loss at  1574 is 1.5083380557997674\n",
            "loss at  1575 is 1.5081248857673029\n",
            "loss at  1576 is 1.5079118849402462\n",
            "loss at  1577 is 1.5076990530810566\n",
            "loss at  1578 is 1.50748638995269\n",
            "loss at  1579 is 1.5072738953185965\n",
            "loss at  1580 is 1.507061568942722\n",
            "loss at  1581 is 1.5068494105895047\n",
            "loss at  1582 is 1.506637420023874\n",
            "loss at  1583 is 1.5064255970112497\n",
            "loss at  1584 is 1.5062139413175406\n",
            "loss at  1585 is 1.5060024527091431\n",
            "loss at  1586 is 1.5057911309529382\n",
            "loss at  1587 is 1.505579975816295\n",
            "loss at  1588 is 1.5053689870670637\n",
            "loss at  1589 is 1.505158164473577\n",
            "loss at  1590 is 1.5049475078046495\n",
            "loss at  1591 is 1.5047370168295746\n",
            "loss at  1592 is 1.5045266913181254\n",
            "loss at  1593 is 1.5043165310405506\n",
            "loss at  1594 is 1.5041065357675754\n",
            "loss at  1595 is 1.5038967052704\n",
            "loss at  1596 is 1.5036870393206967\n",
            "loss at  1597 is 1.5034775376906109\n",
            "loss at  1598 is 1.503268200152758\n",
            "loss at  1599 is 1.5030590264802235\n",
            "loss at  1600 is 1.5028500164465617\n",
            "loss at  1601 is 1.502641169825791\n",
            "loss at  1602 is 1.5024324863923997\n",
            "loss at  1603 is 1.502223965921337\n",
            "loss at  1604 is 1.5020156081880172\n",
            "loss at  1605 is 1.5018074129683157\n",
            "loss at  1606 is 1.5015993800385699\n",
            "loss at  1607 is 1.5013915091755758\n",
            "loss at  1608 is 1.5011838001565867\n",
            "loss at  1609 is 1.5009762527593156\n",
            "loss at  1610 is 1.5007688667619288\n",
            "loss at  1611 is 1.5005616419430488\n",
            "loss at  1612 is 1.500354578081751\n",
            "loss at  1613 is 1.500147674957563\n",
            "loss at  1614 is 1.4999409323504642\n",
            "loss at  1615 is 1.4997343500408822\n",
            "loss at  1616 is 1.4995279278096958\n",
            "loss at  1617 is 1.4993216654382275\n",
            "loss at  1618 is 1.4991155627082504\n",
            "loss at  1619 is 1.4989096194019795\n",
            "loss at  1620 is 1.4987038353020758\n",
            "loss at  1621 is 1.4984982101916406\n",
            "loss at  1622 is 1.4982927438542197\n",
            "loss at  1623 is 1.4980874360737966\n",
            "loss at  1624 is 1.497882286634797\n",
            "loss at  1625 is 1.4976772953220807\n",
            "loss at  1626 is 1.4974724619209483\n",
            "loss at  1627 is 1.4972677862171342\n",
            "loss at  1628 is 1.4970632679968077\n",
            "loss at  1629 is 1.4968589070465723\n",
            "loss at  1630 is 1.4966547031534634\n",
            "loss at  1631 is 1.4964506561049462\n",
            "loss at  1632 is 1.4962467656889185\n",
            "loss at  1633 is 1.4960430316937054\n",
            "loss at  1634 is 1.4958394539080606\n",
            "loss at  1635 is 1.495636032121164\n",
            "loss at  1636 is 1.495432766122621\n",
            "loss at  1637 is 1.4952296557024616\n",
            "loss at  1638 is 1.4950267006511393\n",
            "loss at  1639 is 1.4948239007595308\n",
            "loss at  1640 is 1.494621255818933\n",
            "loss at  1641 is 1.494418765621061\n",
            "loss at  1642 is 1.4942164299580527\n",
            "loss at  1643 is 1.4940142486224612\n",
            "loss at  1644 is 1.493812221407257\n",
            "loss at  1645 is 1.4936103481058258\n",
            "loss at  1646 is 1.4934086285119692\n",
            "loss at  1647 is 1.4932070624199019\n",
            "loss at  1648 is 1.4930056496242496\n",
            "loss at  1649 is 1.4928043899200514\n",
            "loss at  1650 is 1.4926032831027554\n",
            "loss at  1651 is 1.4924023289682196\n",
            "loss at  1652 is 1.4922015273127092\n",
            "loss at  1653 is 1.4920008779328988\n",
            "loss at  1654 is 1.4918003806258657\n",
            "loss at  1655 is 1.491600035189095\n",
            "loss at  1656 is 1.4913998414204745\n",
            "loss at  1657 is 1.4911997991182955\n",
            "loss at  1658 is 1.4909999080812506\n",
            "loss at  1659 is 1.4908001681084342\n",
            "loss at  1660 is 1.4906005789993386\n",
            "loss at  1661 is 1.4904011405538564\n",
            "loss at  1662 is 1.4902018525722789\n",
            "loss at  1663 is 1.4900027148552912\n",
            "loss at  1664 is 1.4898037272039766\n",
            "loss at  1665 is 1.4896048894198124\n",
            "loss at  1666 is 1.4894062013046685\n",
            "loss at  1667 is 1.489207662660809\n",
            "loss at  1668 is 1.489009273290889\n",
            "loss at  1669 is 1.4888110329979534\n",
            "loss at  1670 is 1.4886129415854386\n",
            "loss at  1671 is 1.488414998857167\n",
            "loss at  1672 is 1.4882172046173512\n",
            "loss at  1673 is 1.4880195586705898\n",
            "loss at  1674 is 1.4878220608218655\n",
            "loss at  1675 is 1.4876247108765472\n",
            "loss at  1676 is 1.4874275086403865\n",
            "loss at  1677 is 1.48723045391952\n",
            "loss at  1678 is 1.4870335465204618\n",
            "loss at  1679 is 1.4868367862501102\n",
            "loss at  1680 is 1.4866401729157426\n",
            "loss at  1681 is 1.4864437063250142\n",
            "loss at  1682 is 1.4862473862859584\n",
            "loss at  1683 is 1.4860512126069865\n",
            "loss at  1684 is 1.4858551850968835\n",
            "loss at  1685 is 1.4856593035648125\n",
            "loss at  1686 is 1.485463567820307\n",
            "loss at  1687 is 1.485267977673276\n",
            "loss at  1688 is 1.4850725329340002\n",
            "loss at  1689 is 1.48487723341313\n",
            "loss at  1690 is 1.4846820789216881\n",
            "loss at  1691 is 1.484487069271065\n",
            "loss at  1692 is 1.4842922042730202\n",
            "loss at  1693 is 1.4840974837396799\n",
            "loss at  1694 is 1.483902907483537\n",
            "loss at  1695 is 1.4837084753174508\n",
            "loss at  1696 is 1.4835141870546438\n",
            "loss at  1697 is 1.4833200425087032\n",
            "loss at  1698 is 1.483126041493578\n",
            "loss at  1699 is 1.4829321838235805\n",
            "loss at  1700 is 1.4827384693133818\n",
            "loss at  1701 is 1.4825448977780156\n",
            "loss at  1702 is 1.4823514690328716\n",
            "loss at  1703 is 1.482158182893701\n",
            "loss at  1704 is 1.4819650391766095\n",
            "loss at  1705 is 1.4817720376980608\n",
            "loss at  1706 is 1.4815791782748733\n",
            "loss at  1707 is 1.4813864607242202\n",
            "loss at  1708 is 1.4811938848636295\n",
            "loss at  1709 is 1.4810014505109794\n",
            "loss at  1710 is 1.4808091574845026\n",
            "loss at  1711 is 1.4806170056027814\n",
            "loss at  1712 is 1.4804249946847485\n",
            "loss at  1713 is 1.4802331245496863\n",
            "loss at  1714 is 1.4800413950172249\n",
            "loss at  1715 is 1.4798498059073428\n",
            "loss at  1716 is 1.4796583570403643\n",
            "loss at  1717 is 1.4794670482369598\n",
            "loss at  1718 is 1.479275879318145\n",
            "loss at  1719 is 1.4790848501052776\n",
            "loss at  1720 is 1.478893960420063\n",
            "loss at  1721 is 1.478703210084543\n",
            "loss at  1722 is 1.4785125989211054\n",
            "loss at  1723 is 1.4783221267524767\n",
            "loss at  1724 is 1.4781317934017237\n",
            "loss at  1725 is 1.4779415986922522\n",
            "loss at  1726 is 1.4777515424478045\n",
            "loss at  1727 is 1.477561624492463\n",
            "loss at  1728 is 1.477371844650644\n",
            "loss at  1729 is 1.4771822027471004\n",
            "loss at  1730 is 1.4769926986069193\n",
            "loss at  1731 is 1.476803332055522\n",
            "loss at  1732 is 1.4766141029186632\n",
            "loss at  1733 is 1.4764250110224293\n",
            "loss at  1734 is 1.4762360561932375\n",
            "loss at  1735 is 1.4760472382578365\n",
            "loss at  1736 is 1.4758585570433045\n",
            "loss at  1737 is 1.4756700123770485\n",
            "loss at  1738 is 1.4754816040868035\n",
            "loss at  1739 is 1.4752933320006318\n",
            "loss at  1740 is 1.4751051959469226\n",
            "loss at  1741 is 1.4749171957543896\n",
            "loss at  1742 is 1.474729331252073\n",
            "loss at  1743 is 1.4745416022693356\n",
            "loss at  1744 is 1.474354008635864\n",
            "loss at  1745 is 1.4741665501816674\n",
            "loss at  1746 is 1.4739792267370768\n",
            "loss at  1747 is 1.4737920381327427\n",
            "loss at  1748 is 1.4736049841996373\n",
            "loss at  1749 is 1.4734180647690513\n",
            "loss at  1750 is 1.4732312796725937\n",
            "loss at  1751 is 1.4730446287421914\n",
            "loss at  1752 is 1.4728581118100885\n",
            "loss at  1753 is 1.4726717287088447\n",
            "loss at  1754 is 1.4724854792713353\n",
            "loss at  1755 is 1.4722993633307495\n",
            "loss at  1756 is 1.4721133807205917\n",
            "loss at  1757 is 1.4719275312746778\n",
            "loss at  1758 is 1.4717418148271364\n",
            "loss at  1759 is 1.4715562312124084\n",
            "loss at  1760 is 1.4713707802652443\n",
            "loss at  1761 is 1.4711854618207052\n",
            "loss at  1762 is 1.4710002757141607\n",
            "loss at  1763 is 1.4708152217812889\n",
            "loss at  1764 is 1.470630299858076\n",
            "loss at  1765 is 1.4704455097808156\n",
            "loss at  1766 is 1.470260851386106\n",
            "loss at  1767 is 1.4700763245108512\n",
            "loss at  1768 is 1.4698919289922614\n",
            "loss at  1769 is 1.4697076646678486\n",
            "loss at  1770 is 1.4695235313754291\n",
            "loss at  1771 is 1.4693395289531215\n",
            "loss at  1772 is 1.4691556572393458\n",
            "loss at  1773 is 1.4689719160728223\n",
            "loss at  1774 is 1.4687883052925728\n",
            "loss at  1775 is 1.4686048247379175\n",
            "loss at  1776 is 1.4684214742484762\n",
            "loss at  1777 is 1.468238253664166\n",
            "loss at  1778 is 1.4680551628252005\n",
            "loss at  1779 is 1.467872201572092\n",
            "loss at  1780 is 1.467689369745647\n",
            "loss at  1781 is 1.467506667186967\n",
            "loss at  1782 is 1.4673240937374477\n",
            "loss at  1783 is 1.4671416492387797\n",
            "loss at  1784 is 1.466959333532945\n",
            "loss at  1785 is 1.4667771464622197\n",
            "loss at  1786 is 1.466595087869169\n",
            "loss at  1787 is 1.4664131575966504\n",
            "loss at  1788 is 1.466231355487811\n",
            "loss at  1789 is 1.466049681386088\n",
            "loss at  1790 is 1.4658681351352056\n",
            "loss at  1791 is 1.4656867165791783\n",
            "loss at  1792 is 1.465505425562305\n",
            "loss at  1793 is 1.4653242619291733\n",
            "loss at  1794 is 1.465143225524656\n",
            "loss at  1795 is 1.464962316193911\n",
            "loss at  1796 is 1.4647815337823797\n",
            "loss at  1797 is 1.46460087813579\n",
            "loss at  1798 is 1.4644203491001497\n",
            "loss at  1799 is 1.4642399465217506\n",
            "loss at  1800 is 1.4640596702471669\n",
            "loss at  1801 is 1.463879520123251\n",
            "loss at  1802 is 1.4636994959971392\n",
            "loss at  1803 is 1.4635195977162443\n",
            "loss at  1804 is 1.4633398251282608\n",
            "loss at  1805 is 1.4631601780811594\n",
            "loss at  1806 is 1.4629806564231895\n",
            "loss at  1807 is 1.4628012600028772\n",
            "loss at  1808 is 1.4626219886690244\n",
            "loss at  1809 is 1.4624428422707096\n",
            "loss at  1810 is 1.4622638206572849\n",
            "loss at  1811 is 1.4620849236783775\n",
            "loss at  1812 is 1.4619061511838887\n",
            "loss at  1813 is 1.461727503023992\n",
            "loss at  1814 is 1.461548979049132\n",
            "loss at  1815 is 1.4613705791100284\n",
            "loss at  1816 is 1.4611923030576686\n",
            "loss at  1817 is 1.4610141507433105\n",
            "loss at  1818 is 1.460836122018484\n",
            "loss at  1819 is 1.4606582167349857\n",
            "loss at  1820 is 1.4604804347448823\n",
            "loss at  1821 is 1.4603027759005056\n",
            "loss at  1822 is 1.4601252400544573\n",
            "loss at  1823 is 1.4599478270596045\n",
            "loss at  1824 is 1.4597705367690796\n",
            "loss at  1825 is 1.4595933690362801\n",
            "loss at  1826 is 1.4594163237148683\n",
            "loss at  1827 is 1.459239400658771\n",
            "loss at  1828 is 1.4590625997221773\n",
            "loss at  1829 is 1.4588859207595388\n",
            "loss at  1830 is 1.4587093636255695\n",
            "loss at  1831 is 1.4585329281752442\n",
            "loss at  1832 is 1.4583566142638\n",
            "loss at  1833 is 1.4581804217467313\n",
            "loss at  1834 is 1.4580043504797944\n",
            "loss at  1835 is 1.4578284003190027\n",
            "loss at  1836 is 1.4576525711206292\n",
            "loss at  1837 is 1.4574768627412034\n",
            "loss at  1838 is 1.4573012750375114\n",
            "loss at  1839 is 1.4571258078665974\n",
            "loss at  1840 is 1.45695046108576\n",
            "loss at  1841 is 1.4567752345525522\n",
            "loss at  1842 is 1.456600128124783\n",
            "loss at  1843 is 1.4564251416605143\n",
            "loss at  1844 is 1.4562502750180615\n",
            "loss at  1845 is 1.4560755280559932\n",
            "loss at  1846 is 1.4559009006331287\n",
            "loss at  1847 is 1.4557263926085404\n",
            "loss at  1848 is 1.4555520038415493\n",
            "loss at  1849 is 1.4553777341917296\n",
            "loss at  1850 is 1.4552035835189023\n",
            "loss at  1851 is 1.4550295516831389\n",
            "loss at  1852 is 1.4548556385447584\n",
            "loss at  1853 is 1.454681843964329\n",
            "loss at  1854 is 1.4545081678026652\n",
            "loss at  1855 is 1.4543346099208285\n",
            "loss at  1856 is 1.4541611701801256\n",
            "loss at  1857 is 1.4539878484421092\n",
            "loss at  1858 is 1.4538146445685776\n",
            "loss at  1859 is 1.4536415584215725\n",
            "loss at  1860 is 1.453468589863379\n",
            "loss at  1861 is 1.453295738756526\n",
            "loss at  1862 is 1.4531230049637853\n",
            "loss at  1863 is 1.4529503883481696\n",
            "loss at  1864 is 1.4527778887729337\n",
            "loss at  1865 is 1.4526055061015715\n",
            "loss at  1866 is 1.4524332401978204\n",
            "loss at  1867 is 1.4522610909256548\n",
            "loss at  1868 is 1.4520890581492891\n",
            "loss at  1869 is 1.4519171417331758\n",
            "loss at  1870 is 1.451745341542006\n",
            "loss at  1871 is 1.4515736574407068\n",
            "loss at  1872 is 1.4514020892944435\n",
            "loss at  1873 is 1.4512306369686172\n",
            "loss at  1874 is 1.4510593003288634\n",
            "loss at  1875 is 1.4508880792410552\n",
            "loss at  1876 is 1.4507169735712981\n",
            "loss at  1877 is 1.4505459831859326\n",
            "loss at  1878 is 1.450375107951532\n",
            "loss at  1879 is 1.4502043477349027\n",
            "loss at  1880 is 1.4500337024030836\n",
            "loss at  1881 is 1.4498631718233452\n",
            "loss at  1882 is 1.4496927558631885\n",
            "loss at  1883 is 1.4495224543903464\n",
            "loss at  1884 is 1.449352267272781\n",
            "loss at  1885 is 1.4491821943786845\n",
            "loss at  1886 is 1.4490122355764772\n",
            "loss at  1887 is 1.4488423907348087\n",
            "loss at  1888 is 1.4486726597225552\n",
            "loss at  1889 is 1.4485030424088228\n",
            "loss at  1890 is 1.4483335386629415\n",
            "loss at  1891 is 1.4481641483544694\n",
            "loss at  1892 is 1.4479948713531898\n",
            "loss at  1893 is 1.4478257075291114\n",
            "loss at  1894 is 1.447656656752467\n",
            "loss at  1895 is 1.4474877188937125\n",
            "loss at  1896 is 1.4473188938235308\n",
            "loss at  1897 is 1.4471501814128245\n",
            "loss at  1898 is 1.4469815815327207\n",
            "loss at  1899 is 1.4468130940545672\n",
            "loss at  1900 is 1.446644718849933\n",
            "loss at  1901 is 1.4464764557906105\n",
            "loss at  1902 is 1.4463083047486094\n",
            "loss at  1903 is 1.4461402655961606\n",
            "loss at  1904 is 1.4459723382057152\n",
            "loss at  1905 is 1.4458045224499416\n",
            "loss at  1906 is 1.4456368182017276\n",
            "loss at  1907 is 1.4454692253341783\n",
            "loss at  1908 is 1.4453017437206168\n",
            "loss at  1909 is 1.4451343732345812\n",
            "loss at  1910 is 1.444967113749827\n",
            "loss at  1911 is 1.444799965140326\n",
            "loss at  1912 is 1.4446329272802643\n",
            "loss at  1913 is 1.4444660000440432\n",
            "loss at  1914 is 1.444299183306277\n",
            "loss at  1915 is 1.4441324769417951\n",
            "loss at  1916 is 1.44396588082564\n",
            "loss at  1917 is 1.4437993948330654\n",
            "loss at  1918 is 1.4436330188395394\n",
            "loss at  1919 is 1.4434667527207397\n",
            "loss at  1920 is 1.4433005963525556\n",
            "loss at  1921 is 1.4431345496110881\n",
            "loss at  1922 is 1.4429686123726473\n",
            "loss at  1923 is 1.4428027845137525\n",
            "loss at  1924 is 1.4426370659111345\n",
            "loss at  1925 is 1.4424714564417291\n",
            "loss at  1926 is 1.442305955982684\n",
            "loss at  1927 is 1.4421405644113519\n",
            "loss at  1928 is 1.4419752816052933\n",
            "loss at  1929 is 1.4418101074422764\n",
            "loss at  1930 is 1.4416450418002744\n",
            "loss at  1931 is 1.4414800845574662\n",
            "loss at  1932 is 1.441315235592237\n",
            "loss at  1933 is 1.4411504947831748\n",
            "loss at  1934 is 1.4409858620090743\n",
            "loss at  1935 is 1.4408213371489322\n",
            "loss at  1936 is 1.4406569200819483\n",
            "loss at  1937 is 1.4404926106875262\n",
            "loss at  1938 is 1.4403284088452708\n",
            "loss at  1939 is 1.4401643144349903\n",
            "loss at  1940 is 1.440000327336692\n",
            "loss at  1941 is 1.4398364474305865\n",
            "loss at  1942 is 1.4396726745970823\n",
            "loss at  1943 is 1.43950900871679\n",
            "loss at  1944 is 1.439345449670519\n",
            "loss at  1945 is 1.4391819973392754\n",
            "loss at  1946 is 1.4390186516042678\n",
            "loss at  1947 is 1.4388554123468993\n",
            "loss at  1948 is 1.4386922794487722\n",
            "loss at  1949 is 1.4385292527916869\n",
            "loss at  1950 is 1.4383663322576365\n",
            "loss at  1951 is 1.4382035177288153\n",
            "loss at  1952 is 1.438040809087609\n",
            "loss at  1953 is 1.4378782062166007\n",
            "loss at  1954 is 1.4377157089985677\n",
            "loss at  1955 is 1.437553317316483\n",
            "loss at  1956 is 1.4373910310535103\n",
            "loss at  1957 is 1.437228850093009\n",
            "loss at  1958 is 1.437066774318531\n",
            "loss at  1959 is 1.4369048036138206\n",
            "loss at  1960 is 1.436742937862813\n",
            "loss at  1961 is 1.436581176949637\n",
            "loss at  1962 is 1.43641952075861\n",
            "loss at  1963 is 1.4362579691742428\n",
            "loss at  1964 is 1.4360965220812336\n",
            "loss at  1965 is 1.4359351793644721\n",
            "loss at  1966 is 1.4357739409090367\n",
            "loss at  1967 is 1.4356128066001954\n",
            "loss at  1968 is 1.4354517763234023\n",
            "loss at  1969 is 1.4352908499643018\n",
            "loss at  1970 is 1.435130027408725\n",
            "loss at  1971 is 1.4349693085426904\n",
            "loss at  1972 is 1.434808693252401\n",
            "loss at  1973 is 1.434648181424249\n",
            "loss at  1974 is 1.4344877729448102\n",
            "loss at  1975 is 1.4343274677008462\n",
            "loss at  1976 is 1.434167265579304\n",
            "loss at  1977 is 1.4340071664673142\n",
            "loss at  1978 is 1.433847170252192\n",
            "loss at  1979 is 1.433687276821435\n",
            "loss at  1980 is 1.4335274860627252\n",
            "loss at  1981 is 1.4333677978639265\n",
            "loss at  1982 is 1.4332082121130851\n",
            "loss at  1983 is 1.433048728698429\n",
            "loss at  1984 is 1.432889347508368\n",
            "loss at  1985 is 1.4327300684314916\n",
            "loss at  1986 is 1.4325708913565718\n",
            "loss at  1987 is 1.4324118161725576\n",
            "loss at  1988 is 1.4322528427685812\n",
            "loss at  1989 is 1.4320939710339513\n",
            "loss at  1990 is 1.4319352008581558\n",
            "loss at  1991 is 1.4317765321308626\n",
            "loss at  1992 is 1.4316179647419156\n",
            "loss at  1993 is 1.4314594985813374\n",
            "loss at  1994 is 1.4313011335393278\n",
            "loss at  1995 is 1.4311428695062611\n",
            "loss at  1996 is 1.430984706372691\n",
            "loss at  1997 is 1.4308266440293453\n",
            "loss at  1998 is 1.430668682367127\n",
            "loss at  1999 is 1.4305108212771138\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [51], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     weights\u001b[38;5;241m=\u001b[39m(weights \u001b[38;5;241m-\u001b[39m (dw\u001b[38;5;241m*\u001b[39mlr))\n\u001b[1;32m     29\u001b[0m     bias\u001b[38;5;241m=\u001b[39m(bias \u001b[38;5;241m-\u001b[39m (db\u001b[38;5;241m*\u001b[39mlr))\n\u001b[0;32m---> 31\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,n_iters),logistic_loss[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     32\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "for i in range(n_iters):\n",
        "\n",
        "    # z = x.dot(weights) expects x to be (num_samples, num_features) and weights to be (num_features, num_classes)\n",
        "    z= x_train_data.dot(weights) + bias\n",
        "    \n",
        "    # Softmax \n",
        "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z_shifted)\n",
        "    q = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "    # exp_z = np.exp(z)\n",
        "    # q = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "    \n",
        "    \n",
        "    # Calculate loss using y_train_targets\n",
        "    # --- Stable loss ---\n",
        "    eps = 1e-12\n",
        "    # probability assigned to the correct class for each sample\n",
        "    p_true = np.sum(y_train_targets * q, axis=1)         # (N,)\n",
        "    loss = np.mean(-np.log2(np.clip(p_true, eps, 1.0)))  # avoid log(0)\n",
        "\n",
        "    print(\"loss at \", i, \"is\", loss)\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    # dw = x.T.dot((q-y)) expects x.T to be (num_features, num_samples) and (q-y) to be (num_samples, num_classes)\n",
        "    # Here x_train_data.T is (num_features, num_samples)\n",
        "    db = np.sum((q-y_train_targets), axis=0)/num_samples\n",
        "    dw=x_train_data.T.dot((q-y_train_targets))/num_samples\n",
        "    weights=(weights - (dw*lr))\n",
        "    bias=(bias - (db*lr))\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 56.61%\n",
            "Test Loss: 1.4584\n",
            "\n",
            "Prediction distribution:\n",
            "Counter({0: 4667, 1: 1999, 2: 644})\n",
            "\n",
            "True label distribution:\n",
            "Counter({0: 4469, 1: 2217, 2: 624})\n"
          ]
        }
      ],
      "source": [
        "# Forward pass on test data\n",
        "z_test = M_test.dot(weights) + bias\n",
        "\n",
        "# Softmax\n",
        "\n",
        "exp_z_test = np.exp(z_test)\n",
        "q_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "\n",
        "# Get predictions (class with highest probability)\n",
        "y_pred_test = np.argmax(q_test, axis=1)  # Shape: (7310,)\n",
        "\n",
        "# Get true labels\n",
        "# Assuming y_test has shape (3, 7310), transpose it to (7310, 3)\n",
        "y_test_targets = y_test.T  # Shape: (7310, 3)\n",
        "y_true_test = np.argmax(y_test_targets, axis=1)  # Shape: (7310,)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_test = np.mean(y_pred_test == y_true_test)\n",
        "print(f\"Test Accuracy: {accuracy_test * 100:.2f}%\")\n",
        "\n",
        "# Calculate test loss\n",
        "eps = 1e-12\n",
        "p_true_test = np.sum(y_test_targets * q_test, axis=1)\n",
        "loss_test = np.mean(-np.log2(np.clip(p_true_test, eps, 1.0)))\n",
        "print(f\"Test Loss: {loss_test:.4f}\")\n",
        "\n",
        "# Show confusion matrix (optional)\n",
        "from collections import Counter\n",
        "print(\"\\nPrediction distribution:\")\n",
        "print(Counter(y_pred_test))\n",
        "print(\"\\nTrue label distribution:\")\n",
        "print(Counter(y_true_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: Logistic Loss = 1.9635192220864157\n",
            "Iteration 2: Logistic Loss = 12.806044604611966\n",
            "Iteration 3: Logistic Loss = 5.9999226729651065\n",
            "Iteration 4: Logistic Loss = 5.322811730528054\n",
            "Iteration 5: Logistic Loss = 2.712035180287424\n",
            "Iteration 6: Logistic Loss = 1.5460210649255726\n",
            "Iteration 7: Logistic Loss = 1.5197275698372847\n",
            "Iteration 8: Logistic Loss = 1.5001031123429363\n",
            "Iteration 9: Logistic Loss = 1.4843013488430816\n",
            "Iteration 10: Logistic Loss = 1.4712396000731616\n",
            "Iteration 11: Logistic Loss = 1.4601645101514862\n",
            "Iteration 12: Logistic Loss = 1.450602720092678\n",
            "Iteration 13: Logistic Loss = 1.4422899754441392\n",
            "Iteration 14: Logistic Loss = 1.4349658230206512\n",
            "Iteration 15: Logistic Loss = 1.428315407718188\n",
            "Iteration 16: Logistic Loss = 1.4221452776666246\n",
            "Iteration 17: Logistic Loss = 1.416373811319904\n",
            "Iteration 18: Logistic Loss = 1.4110237459989476\n",
            "Iteration 19: Logistic Loss = 1.405957135986158\n",
            "Iteration 20: Logistic Loss = 1.4011600577191015\n",
            "Iteration 21: Logistic Loss = 1.3968716843211058\n",
            "Iteration 22: Logistic Loss = 1.393317722943678\n",
            "Iteration 23: Logistic Loss = 1.3903283796775647\n",
            "Iteration 24: Logistic Loss = 1.3877505563800352\n",
            "Iteration 25: Logistic Loss = 1.385516390713943\n",
            "Iteration 26: Logistic Loss = 1.3835928793629901\n",
            "Iteration 27: Logistic Loss = 1.3818869628937158\n",
            "Iteration 28: Logistic Loss = 1.3803387004245558\n",
            "Iteration 29: Logistic Loss = 1.3789188468371114\n",
            "Iteration 30: Logistic Loss = 1.3776259307122554\n",
            "Iteration 31: Logistic Loss = 1.3764296571898929\n",
            "Iteration 32: Logistic Loss = 1.3752929432837755\n",
            "Iteration 33: Logistic Loss = 1.3742084581584078\n",
            "Iteration 34: Logistic Loss = 1.3731692812953138\n",
            "Iteration 35: Logistic Loss = 1.3721655396905048\n",
            "Iteration 36: Logistic Loss = 1.3711967260821887\n",
            "Iteration 37: Logistic Loss = 1.3702468291594387\n",
            "Iteration 38: Logistic Loss = 1.3693113899986944\n",
            "Iteration 39: Logistic Loss = 1.3683916669797211\n",
            "Iteration 40: Logistic Loss = 1.3674637452688962\n",
            "Iteration 41: Logistic Loss = 1.3665499746234833\n",
            "Iteration 42: Logistic Loss = 1.3656403376369537\n",
            "Iteration 43: Logistic Loss = 1.3647367608394199\n",
            "Iteration 44: Logistic Loss = 1.3638480285044223\n",
            "Iteration 45: Logistic Loss = 1.362966066293638\n",
            "Iteration 46: Logistic Loss = 1.3620783346762297\n",
            "Iteration 47: Logistic Loss = 1.3611854741476526\n",
            "Iteration 48: Logistic Loss = 1.3602871748155043\n",
            "Iteration 49: Logistic Loss = 1.3593926595599617\n",
            "Iteration 50: Logistic Loss = 1.3585043936796366\n",
            "Iteration 51: Logistic Loss = 1.3576240070242\n",
            "Iteration 52: Logistic Loss = 1.3567459434248996\n",
            "Iteration 53: Logistic Loss = 1.355874006970767\n",
            "Iteration 54: Logistic Loss = 1.3550086134120367\n",
            "Iteration 55: Logistic Loss = 1.354147038863175\n",
            "Iteration 56: Logistic Loss = 1.353288237092204\n",
            "Iteration 57: Logistic Loss = 1.3524301362645414\n",
            "Iteration 58: Logistic Loss = 1.3515740677089587\n",
            "Iteration 59: Logistic Loss = 1.3507234003254442\n",
            "Iteration 60: Logistic Loss = 1.349877027551575\n",
            "Iteration 61: Logistic Loss = 1.349030203803611\n",
            "Iteration 62: Logistic Loss = 1.3481807360567826\n",
            "Iteration 63: Logistic Loss = 1.3473316348900906\n",
            "Iteration 64: Logistic Loss = 1.3464852314198599\n",
            "Iteration 65: Logistic Loss = 1.3456414973238064\n",
            "Iteration 66: Logistic Loss = 1.3448037609383203\n",
            "Iteration 67: Logistic Loss = 1.3439706513832461\n",
            "Iteration 68: Logistic Loss = 1.343140737331506\n",
            "Iteration 69: Logistic Loss = 1.342313026231109\n",
            "Iteration 70: Logistic Loss = 1.3414886513548872\n",
            "Iteration 71: Logistic Loss = 1.3406672269736166\n",
            "Iteration 72: Logistic Loss = 1.3398474159491844\n",
            "Iteration 73: Logistic Loss = 1.339029565745429\n",
            "Iteration 74: Logistic Loss = 1.338215284571697\n",
            "Iteration 75: Logistic Loss = 1.3374038527113457\n",
            "Iteration 76: Logistic Loss = 1.3365977597396044\n",
            "Iteration 77: Logistic Loss = 1.33579555489908\n",
            "Iteration 78: Logistic Loss = 1.3349985522947276\n",
            "Iteration 79: Logistic Loss = 1.33420632780367\n",
            "Iteration 80: Logistic Loss = 1.333418041026415\n",
            "Iteration 81: Logistic Loss = 1.3326315846169212\n",
            "Iteration 82: Logistic Loss = 1.3318471770667375\n",
            "Iteration 83: Logistic Loss = 1.3310677978625094\n",
            "Iteration 84: Logistic Loss = 1.3302937784174493\n",
            "Iteration 85: Logistic Loss = 1.3295237130465023\n",
            "Iteration 86: Logistic Loss = 1.3287571210829765\n",
            "Iteration 87: Logistic Loss = 1.327994829071937\n",
            "Iteration 88: Logistic Loss = 1.3272347487248353\n",
            "Iteration 89: Logistic Loss = 1.3264779863432317\n",
            "Iteration 90: Logistic Loss = 1.3257260146097898\n",
            "Iteration 91: Logistic Loss = 1.3249804836548276\n",
            "Iteration 92: Logistic Loss = 1.324236001630353\n",
            "Iteration 93: Logistic Loss = 1.323498639504576\n",
            "Iteration 94: Logistic Loss = 1.322762629237972\n",
            "Iteration 95: Logistic Loss = 1.3220356210112512\n",
            "Iteration 96: Logistic Loss = 1.3213051168452823\n",
            "Iteration 97: Logistic Loss = 1.3205802618069877\n",
            "Iteration 98: Logistic Loss = 1.3198649703617555\n",
            "Iteration 99: Logistic Loss = 1.3191601804062374\n",
            "Iteration 100: Logistic Loss = 1.3184706666571198\n",
            "Iteration 101: Logistic Loss = 1.3178011007758743\n",
            "Iteration 102: Logistic Loss = 1.3171743564615137\n",
            "Iteration 103: Logistic Loss = 1.3165986236560252\n",
            "Iteration 104: Logistic Loss = 1.3161483191497918\n",
            "Iteration 105: Logistic Loss = 1.3158514101382022\n",
            "Iteration 106: Logistic Loss = 1.3160400518410735\n",
            "Iteration 107: Logistic Loss = 1.3167323461671145\n",
            "Iteration 108: Logistic Loss = 1.319399126251537\n",
            "Iteration 109: Logistic Loss = 1.3230849312879454\n",
            "Iteration 110: Logistic Loss = 1.3349472834543794\n",
            "Iteration 111: Logistic Loss = 1.3421705083629956\n",
            "Iteration 112: Logistic Loss = 1.3764278543453086\n",
            "Iteration 113: Logistic Loss = 1.3582451397868556\n",
            "Iteration 114: Logistic Loss = 1.395375362708187\n",
            "Iteration 115: Logistic Loss = 1.3421559434604766\n",
            "Iteration 116: Logistic Loss = 1.3539306959862494\n",
            "Iteration 117: Logistic Loss = 1.3321267927413118\n",
            "Iteration 118: Logistic Loss = 1.340434815498499\n",
            "Iteration 119: Logistic Loss = 1.328050214477135\n",
            "Iteration 120: Logistic Loss = 1.3357138683870167\n",
            "Iteration 121: Logistic Loss = 1.3259618940259288\n",
            "Iteration 122: Logistic Loss = 1.3334512468147874\n",
            "Iteration 123: Logistic Loss = 1.3245758123259872\n",
            "Iteration 124: Logistic Loss = 1.3320568730638238\n",
            "Iteration 125: Logistic Loss = 1.32347536616142\n",
            "Iteration 126: Logistic Loss = 1.3309990036765815\n",
            "Iteration 127: Logistic Loss = 1.3224833324095122\n",
            "Iteration 128: Logistic Loss = 1.3300958014343267\n",
            "Iteration 129: Logistic Loss = 1.321615473671531\n",
            "Iteration 130: Logistic Loss = 1.3293474891162194\n",
            "Iteration 131: Logistic Loss = 1.3207658828728943\n",
            "Iteration 132: Logistic Loss = 1.328619127572089\n",
            "Iteration 133: Logistic Loss = 1.3199204697517235\n",
            "Iteration 134: Logistic Loss = 1.3278339993456496\n",
            "Iteration 135: Logistic Loss = 1.3190619674865667\n",
            "Iteration 136: Logistic Loss = 1.3270468589873412\n",
            "Iteration 137: Logistic Loss = 1.3181859940874687\n",
            "Iteration 138: Logistic Loss = 1.3261966842124846\n",
            "Iteration 139: Logistic Loss = 1.317324308665857\n",
            "Iteration 140: Logistic Loss = 1.3254023024591137\n",
            "Iteration 141: Logistic Loss = 1.3164849456033128\n",
            "Iteration 142: Logistic Loss = 1.3246217269246543\n",
            "Iteration 143: Logistic Loss = 1.3156336616191486\n",
            "Iteration 144: Logistic Loss = 1.3238308259030624\n",
            "Iteration 145: Logistic Loss = 1.3147899678173713\n",
            "Iteration 146: Logistic Loss = 1.3230305565000768\n",
            "Iteration 147: Logistic Loss = 1.3139842959921022\n",
            "Iteration 148: Logistic Loss = 1.3222899730653161\n",
            "Iteration 149: Logistic Loss = 1.3132105925406854\n",
            "Iteration 150: Logistic Loss = 1.3216022671390852\n",
            "Iteration 151: Logistic Loss = 1.3124080020907565\n",
            "Iteration 152: Logistic Loss = 1.3208226397720133\n",
            "Iteration 153: Logistic Loss = 1.3115994698036246\n",
            "Iteration 154: Logistic Loss = 1.3200546142748022\n",
            "Iteration 155: Logistic Loss = 1.3108178639897825\n",
            "Iteration 156: Logistic Loss = 1.3193034673695418\n",
            "Iteration 157: Logistic Loss = 1.310084801584536\n",
            "Iteration 158: Logistic Loss = 1.318672261067267\n",
            "Iteration 159: Logistic Loss = 1.309381288281654\n",
            "Iteration 160: Logistic Loss = 1.3180164208272782\n",
            "Iteration 161: Logistic Loss = 1.308642689350903\n",
            "Iteration 162: Logistic Loss = 1.3173330269790573\n",
            "Iteration 163: Logistic Loss = 1.307918953846093\n",
            "Iteration 164: Logistic Loss = 1.3166952610587424\n",
            "Iteration 165: Logistic Loss = 1.3072174415973736\n",
            "Iteration 166: Logistic Loss = 1.3160333254815224\n",
            "Iteration 167: Logistic Loss = 1.3064935988777922\n",
            "Iteration 168: Logistic Loss = 1.3153510231377181\n",
            "Iteration 169: Logistic Loss = 1.3057812183666475\n",
            "Iteration 170: Logistic Loss = 1.3146867094649253\n",
            "Iteration 171: Logistic Loss = 1.3050828910416776\n",
            "Iteration 172: Logistic Loss = 1.314008386063645\n",
            "Iteration 173: Logistic Loss = 1.3043573194866975\n",
            "Iteration 174: Logistic Loss = 1.3132835972894876\n",
            "Iteration 175: Logistic Loss = 1.3036434460868427\n",
            "Iteration 176: Logistic Loss = 1.312589545757719\n",
            "Iteration 177: Logistic Loss = 1.3029406275777458\n",
            "Iteration 178: Logistic Loss = 1.3119245940111726\n",
            "Iteration 179: Logistic Loss = 1.3022469018019056\n",
            "Iteration 180: Logistic Loss = 1.3112418028661368\n",
            "Iteration 181: Logistic Loss = 1.3015614583765485\n",
            "Iteration 182: Logistic Loss = 1.3105927552063896\n",
            "Iteration 183: Logistic Loss = 1.3009245320649379\n",
            "Iteration 184: Logistic Loss = 1.310030643242474\n",
            "Iteration 185: Logistic Loss = 1.30032337901197\n",
            "Iteration 186: Logistic Loss = 1.309497527135154\n",
            "Iteration 187: Logistic Loss = 1.2997064053468823\n",
            "Iteration 188: Logistic Loss = 1.308950436740102\n",
            "Iteration 189: Logistic Loss = 1.2991057503461836\n",
            "Iteration 190: Logistic Loss = 1.3084147049735448\n",
            "Iteration 191: Logistic Loss = 1.2985055928976954\n",
            "Iteration 192: Logistic Loss = 1.3078795041736266\n",
            "Iteration 193: Logistic Loss = 1.2978455852544717\n",
            "Iteration 194: Logistic Loss = 1.3071842109849163\n",
            "Iteration 195: Logistic Loss = 1.2972525291833172\n",
            "Iteration 196: Logistic Loss = 1.306605347179854\n",
            "Iteration 197: Logistic Loss = 1.2966803979382806\n",
            "Iteration 198: Logistic Loss = 1.3061066686159042\n",
            "Iteration 199: Logistic Loss = 1.296145432258493\n",
            "Iteration 200: Logistic Loss = 1.3056537435435829\n",
            "Iteration 201: Logistic Loss = 1.29558782460773\n",
            "Iteration 202: Logistic Loss = 1.3051501282007232\n",
            "Iteration 203: Logistic Loss = 1.2949580577008477\n",
            "Iteration 204: Logistic Loss = 1.3044927037114402\n",
            "Iteration 205: Logistic Loss = 1.294334171975152\n",
            "Iteration 206: Logistic Loss = 1.3038621542340145\n",
            "Iteration 207: Logistic Loss = 1.2936976466209258\n",
            "Iteration 208: Logistic Loss = 1.3031942873085822\n",
            "Iteration 209: Logistic Loss = 1.2930785313271262\n",
            "Iteration 210: Logistic Loss = 1.3025943134203803\n",
            "Iteration 211: Logistic Loss = 1.292521700147997\n",
            "Iteration 212: Logistic Loss = 1.3021256354646082\n",
            "Iteration 213: Logistic Loss = 1.2919721519851681\n",
            "Iteration 214: Logistic Loss = 1.3016253224862115\n",
            "Iteration 215: Logistic Loss = 1.2914040221440275\n",
            "Iteration 216: Logistic Loss = 1.3010443243348155\n",
            "Iteration 217: Logistic Loss = 1.290806829021986\n",
            "Iteration 218: Logistic Loss = 1.3004894380583312\n",
            "Iteration 219: Logistic Loss = 1.2902414842922985\n",
            "Iteration 220: Logistic Loss = 1.2999179650702737\n",
            "Iteration 221: Logistic Loss = 1.2896498478040863\n",
            "Iteration 222: Logistic Loss = 1.299305134037041\n",
            "Iteration 223: Logistic Loss = 1.2891099878473669\n",
            "Iteration 224: Logistic Loss = 1.2988160233479347\n",
            "Iteration 225: Logistic Loss = 1.2885336329105193\n",
            "Iteration 226: Logistic Loss = 1.2982288035387235\n",
            "Iteration 227: Logistic Loss = 1.2879194759397812\n",
            "Iteration 228: Logistic Loss = 1.2975751128571962\n",
            "Iteration 229: Logistic Loss = 1.2873145310351186\n",
            "Iteration 230: Logistic Loss = 1.2969231335228275\n",
            "Iteration 231: Logistic Loss = 1.286707904151373\n",
            "Iteration 232: Logistic Loss = 1.2962849216983192\n",
            "Iteration 233: Logistic Loss = 1.2861639668922886\n",
            "Iteration 234: Logistic Loss = 1.295793692367465\n",
            "Iteration 235: Logistic Loss = 1.2856567578638327\n",
            "Iteration 236: Logistic Loss = 1.2953382108765585\n",
            "Iteration 237: Logistic Loss = 1.285163093118521\n",
            "Iteration 238: Logistic Loss = 1.294886020394293\n",
            "Iteration 239: Logistic Loss = 1.2846651807078442\n",
            "Iteration 240: Logistic Loss = 1.2944274215875222\n",
            "Iteration 241: Logistic Loss = 1.28410123032336\n",
            "Iteration 242: Logistic Loss = 1.2938396125271787\n",
            "Iteration 243: Logistic Loss = 1.2835141682522175\n",
            "Iteration 244: Logistic Loss = 1.2932202712808682\n",
            "Iteration 245: Logistic Loss = 1.2829463122619904\n",
            "Iteration 246: Logistic Loss = 1.292611935061729\n",
            "Iteration 247: Logistic Loss = 1.2824187677166274\n",
            "Iteration 248: Logistic Loss = 1.2921038446818547\n",
            "Iteration 249: Logistic Loss = 1.2818889507286944\n",
            "Iteration 250: Logistic Loss = 1.2916116414908123\n",
            "Iteration 251: Logistic Loss = 1.2813635244205102\n",
            "Iteration 252: Logistic Loss = 1.2910754400083693\n",
            "Iteration 253: Logistic Loss = 1.2808916760367275\n",
            "Iteration 254: Logistic Loss = 1.2906949307863635\n",
            "Iteration 255: Logistic Loss = 1.280436133968666\n",
            "Iteration 256: Logistic Loss = 1.2902474320028117\n",
            "Iteration 257: Logistic Loss = 1.2799365861161833\n",
            "Iteration 258: Logistic Loss = 1.2897691960669975\n",
            "Iteration 259: Logistic Loss = 1.279415878571872\n",
            "Iteration 260: Logistic Loss = 1.2892554926669049\n",
            "Iteration 261: Logistic Loss = 1.2788992338517997\n",
            "Iteration 262: Logistic Loss = 1.2887082314465612\n",
            "Iteration 263: Logistic Loss = 1.2783754312984914\n",
            "Iteration 264: Logistic Loss = 1.2881761030990577\n",
            "Iteration 265: Logistic Loss = 1.2778730562176828\n",
            "Iteration 266: Logistic Loss = 1.2877128961580753\n",
            "Iteration 267: Logistic Loss = 1.2774030944773644\n",
            "Iteration 268: Logistic Loss = 1.2872912488685875\n",
            "Iteration 269: Logistic Loss = 1.2769292552183258\n",
            "Iteration 270: Logistic Loss = 1.286857372319826\n",
            "Iteration 271: Logistic Loss = 1.2764910203448296\n",
            "Iteration 272: Logistic Loss = 1.2864755734286941\n",
            "Iteration 273: Logistic Loss = 1.2760338031101977\n",
            "Iteration 274: Logistic Loss = 1.2860437511571767\n",
            "Iteration 275: Logistic Loss = 1.2755473421914099\n",
            "Iteration 276: Logistic Loss = 1.2855509675623813\n",
            "Iteration 277: Logistic Loss = 1.2750351594431724\n",
            "Iteration 278: Logistic Loss = 1.285028529888444\n",
            "Iteration 279: Logistic Loss = 1.2745129152261576\n",
            "Iteration 280: Logistic Loss = 1.284475733705188\n",
            "Iteration 281: Logistic Loss = 1.273973042880119\n",
            "Iteration 282: Logistic Loss = 1.283903888321439\n",
            "Iteration 283: Logistic Loss = 1.2734023335497826\n",
            "Iteration 284: Logistic Loss = 1.2832696887210653\n",
            "Iteration 285: Logistic Loss = 1.2728112795363233\n",
            "Iteration 286: Logistic Loss = 1.2825725672732518\n",
            "Iteration 287: Logistic Loss = 1.2721859185743019\n",
            "Iteration 288: Logistic Loss = 1.2818455712186352\n",
            "Iteration 289: Logistic Loss = 1.2716052984926887\n",
            "Iteration 290: Logistic Loss = 1.2812409566134384\n",
            "Iteration 291: Logistic Loss = 1.2710648497820225\n",
            "Iteration 292: Logistic Loss = 1.2806612976436258\n",
            "Iteration 293: Logistic Loss = 1.2705609146797698\n",
            "Iteration 294: Logistic Loss = 1.280198507310588\n",
            "Iteration 295: Logistic Loss = 1.270101113705817\n",
            "Iteration 296: Logistic Loss = 1.2797580233032229\n",
            "Iteration 297: Logistic Loss = 1.2696221058728199\n",
            "Iteration 298: Logistic Loss = 1.2792402739166846\n",
            "Iteration 299: Logistic Loss = 1.2691388243377995\n",
            "Iteration 300: Logistic Loss = 1.2787811082448461\n",
            "Iteration 301: Logistic Loss = 1.2686331951361347\n",
            "Iteration 302: Logistic Loss = 1.2782575786013937\n",
            "Iteration 303: Logistic Loss = 1.2681269523990588\n",
            "Iteration 304: Logistic Loss = 1.2776836453221667\n",
            "Iteration 305: Logistic Loss = 1.2676433913741814\n",
            "Iteration 306: Logistic Loss = 1.2771994029434917\n",
            "Iteration 307: Logistic Loss = 1.2672322574479642\n",
            "Iteration 308: Logistic Loss = 1.276796868670474\n",
            "Iteration 309: Logistic Loss = 1.2668473739290695\n",
            "Iteration 310: Logistic Loss = 1.276459452639301\n",
            "Iteration 311: Logistic Loss = 1.266480141410239\n",
            "Iteration 312: Logistic Loss = 1.2761626256396466\n",
            "Iteration 313: Logistic Loss = 1.2660802568912441\n",
            "Iteration 314: Logistic Loss = 1.2757191214718377\n",
            "Iteration 315: Logistic Loss = 1.265667732219065\n",
            "Iteration 316: Logistic Loss = 1.2753615737729314\n",
            "Iteration 317: Logistic Loss = 1.265261346180215\n",
            "Iteration 318: Logistic Loss = 1.2749172845507466\n",
            "Iteration 319: Logistic Loss = 1.264803737282585\n",
            "Iteration 320: Logistic Loss = 1.2744479245314007\n",
            "Iteration 321: Logistic Loss = 1.2643224786746445\n",
            "Iteration 322: Logistic Loss = 1.2738906155166843\n",
            "Iteration 323: Logistic Loss = 1.2638470489091855\n",
            "Iteration 324: Logistic Loss = 1.2733639127626948\n",
            "Iteration 325: Logistic Loss = 1.2633934433514835\n",
            "Iteration 326: Logistic Loss = 1.2729169653771266\n",
            "Iteration 327: Logistic Loss = 1.2629442411400194\n",
            "Iteration 328: Logistic Loss = 1.2724002311121507\n",
            "Iteration 329: Logistic Loss = 1.2624830666337599\n",
            "Iteration 330: Logistic Loss = 1.2718886272965955\n",
            "Iteration 331: Logistic Loss = 1.262042459627559\n",
            "Iteration 332: Logistic Loss = 1.2714159150174287\n",
            "Iteration 333: Logistic Loss = 1.2616068806768237\n",
            "Iteration 334: Logistic Loss = 1.2709226570322139\n",
            "Iteration 335: Logistic Loss = 1.2611332430157531\n",
            "Iteration 336: Logistic Loss = 1.2703615324361965\n",
            "Iteration 337: Logistic Loss = 1.2606821670962522\n",
            "Iteration 338: Logistic Loss = 1.2699220604006183\n",
            "Iteration 339: Logistic Loss = 1.2602696037604326\n",
            "Iteration 340: Logistic Loss = 1.269511683532692\n",
            "Iteration 341: Logistic Loss = 1.2598431252068212\n",
            "Iteration 342: Logistic Loss = 1.2689961514138173\n",
            "Iteration 343: Logistic Loss = 1.2593871878937977\n",
            "Iteration 344: Logistic Loss = 1.2684914952998865\n",
            "Iteration 345: Logistic Loss = 1.2589661410045632\n",
            "Iteration 346: Logistic Loss = 1.2680050191608605\n",
            "Iteration 347: Logistic Loss = 1.2585109770613037\n",
            "Iteration 348: Logistic Loss = 1.267496333967872\n",
            "Iteration 349: Logistic Loss = 1.2580934344684438\n",
            "Iteration 350: Logistic Loss = 1.267027404667233\n",
            "Iteration 351: Logistic Loss = 1.2576780757434034\n",
            "Iteration 352: Logistic Loss = 1.2665710010356384\n",
            "Iteration 353: Logistic Loss = 1.257225369453268\n",
            "Iteration 354: Logistic Loss = 1.2659744545429177\n",
            "Iteration 355: Logistic Loss = 1.2567502244632225\n",
            "Iteration 356: Logistic Loss = 1.2654033417239605\n",
            "Iteration 357: Logistic Loss = 1.256288197239165\n",
            "Iteration 358: Logistic Loss = 1.26486692447102\n",
            "Iteration 359: Logistic Loss = 1.2557896654229463\n",
            "Iteration 360: Logistic Loss = 1.264228372659138\n",
            "Iteration 361: Logistic Loss = 1.255332782795277\n",
            "Iteration 362: Logistic Loss = 1.2637170195398861\n",
            "Iteration 363: Logistic Loss = 1.2548412722484226\n",
            "Iteration 364: Logistic Loss = 1.2630697109297433\n",
            "Iteration 365: Logistic Loss = 1.2544059663195304\n",
            "Iteration 366: Logistic Loss = 1.2625610638708429\n",
            "Iteration 367: Logistic Loss = 1.2539621707671296\n",
            "Iteration 368: Logistic Loss = 1.2620680868987537\n",
            "Iteration 369: Logistic Loss = 1.2535197737507073\n",
            "Iteration 370: Logistic Loss = 1.2614978904211838\n",
            "Iteration 371: Logistic Loss = 1.2530964692233877\n",
            "Iteration 372: Logistic Loss = 1.261030150223408\n",
            "Iteration 373: Logistic Loss = 1.2527130477536783\n",
            "Iteration 374: Logistic Loss = 1.2605693214513127\n",
            "Iteration 375: Logistic Loss = 1.2523173355726265\n",
            "Iteration 376: Logistic Loss = 1.260070064268554\n",
            "Iteration 377: Logistic Loss = 1.2518560882598748\n",
            "Iteration 378: Logistic Loss = 1.2594303093225472\n",
            "Iteration 379: Logistic Loss = 1.2513782376548888\n",
            "Iteration 380: Logistic Loss = 1.2588660756166157\n",
            "Iteration 381: Logistic Loss = 1.250910920321423\n",
            "Iteration 382: Logistic Loss = 1.2583314030154777\n",
            "Iteration 383: Logistic Loss = 1.2505263991586038\n",
            "Iteration 384: Logistic Loss = 1.2579129224937309\n",
            "Iteration 385: Logistic Loss = 1.250185942426738\n",
            "Iteration 386: Logistic Loss = 1.257472610560347\n",
            "Iteration 387: Logistic Loss = 1.2498947911026568\n",
            "Iteration 388: Logistic Loss = 1.2570849847592933\n",
            "Iteration 389: Logistic Loss = 1.2495657509488294\n",
            "Iteration 390: Logistic Loss = 1.2565999314076128\n",
            "Iteration 391: Logistic Loss = 1.2491901816132045\n",
            "Iteration 392: Logistic Loss = 1.2561041851164352\n",
            "Iteration 393: Logistic Loss = 1.2487532605169516\n",
            "Iteration 394: Logistic Loss = 1.2555387680470855\n",
            "Iteration 395: Logistic Loss = 1.2482854971566895\n",
            "Iteration 396: Logistic Loss = 1.254978983223446\n",
            "Iteration 397: Logistic Loss = 1.2478349251076062\n",
            "Iteration 398: Logistic Loss = 1.2544323492131775\n",
            "Iteration 399: Logistic Loss = 1.2474021239576638\n",
            "Iteration 400: Logistic Loss = 1.2539724819825309\n",
            "Iteration 401: Logistic Loss = 1.2469448512254786\n",
            "Iteration 402: Logistic Loss = 1.253379333708009\n",
            "Iteration 403: Logistic Loss = 1.2464239473083334\n",
            "Iteration 404: Logistic Loss = 1.2527569384410484\n",
            "Iteration 405: Logistic Loss = 1.2459191320445924\n",
            "Iteration 406: Logistic Loss = 1.2521737063565606\n",
            "Iteration 407: Logistic Loss = 1.2454207718975783\n",
            "Iteration 408: Logistic Loss = 1.251636436104479\n",
            "Iteration 409: Logistic Loss = 1.2448904579215996\n",
            "Iteration 410: Logistic Loss = 1.2509830028147455\n",
            "Iteration 411: Logistic Loss = 1.2443348364560713\n",
            "Iteration 412: Logistic Loss = 1.2503573983777747\n",
            "Iteration 413: Logistic Loss = 1.243779480704241\n",
            "Iteration 414: Logistic Loss = 1.2497857538154142\n",
            "Iteration 415: Logistic Loss = 1.243273939559957\n",
            "Iteration 416: Logistic Loss = 1.2492970019457827\n",
            "Iteration 417: Logistic Loss = 1.2428339927391243\n",
            "Iteration 418: Logistic Loss = 1.2488384019905547\n",
            "Iteration 419: Logistic Loss = 1.2424558359985975\n",
            "Iteration 420: Logistic Loss = 1.2484908634042393\n",
            "Iteration 421: Logistic Loss = 1.2421124305238647\n",
            "Iteration 422: Logistic Loss = 1.248084240983716\n",
            "Iteration 423: Logistic Loss = 1.2417920330977046\n",
            "Iteration 424: Logistic Loss = 1.247675604032408\n",
            "Iteration 425: Logistic Loss = 1.2414865245771343\n",
            "Iteration 426: Logistic Loss = 1.2472839812956944\n",
            "Iteration 427: Logistic Loss = 1.2412616857581797\n",
            "Iteration 428: Logistic Loss = 1.2468261166154002\n",
            "Iteration 429: Logistic Loss = 1.241050043909397\n",
            "Iteration 430: Logistic Loss = 1.2463404219498468\n",
            "Iteration 431: Logistic Loss = 1.2409467932905187\n",
            "Iteration 432: Logistic Loss = 1.2458796507106757\n",
            "Iteration 433: Logistic Loss = 1.2408713409997143\n",
            "Iteration 434: Logistic Loss = 1.2453328526637493\n",
            "Iteration 435: Logistic Loss = 1.2410251354039268\n",
            "Iteration 436: Logistic Loss = 1.2449066964725215\n",
            "Iteration 437: Logistic Loss = 1.241281765662529\n",
            "Iteration 438: Logistic Loss = 1.2446440603896658\n",
            "Iteration 439: Logistic Loss = 1.241704205815514\n",
            "Iteration 440: Logistic Loss = 1.244576789516528\n",
            "Iteration 441: Logistic Loss = 1.2422133793711534\n",
            "Iteration 442: Logistic Loss = 1.2450591070361843\n",
            "Iteration 443: Logistic Loss = 1.2426598837216487\n",
            "Iteration 444: Logistic Loss = 1.2466739121702672\n",
            "Iteration 445: Logistic Loss = 1.2432557467704273\n",
            "Iteration 446: Logistic Loss = 1.2512696223494297\n",
            "Iteration 447: Logistic Loss = 1.2459302631103384\n",
            "Iteration 448: Logistic Loss = 1.2634886283980375\n",
            "Iteration 449: Logistic Loss = 1.2577677803134524\n",
            "Iteration 450: Logistic Loss = 1.2875435824279993\n",
            "Iteration 451: Logistic Loss = 1.2905207007983763\n",
            "Iteration 452: Logistic Loss = 1.2936372378198677\n",
            "Iteration 453: Logistic Loss = 1.305161022566851\n",
            "Iteration 454: Logistic Loss = 1.2584362065679877\n",
            "Iteration 455: Logistic Loss = 1.2668873283331663\n",
            "Iteration 456: Logistic Loss = 1.2416512479072697\n",
            "Iteration 457: Logistic Loss = 1.2542508725780508\n",
            "Iteration 458: Logistic Loss = 1.2347454576213532\n",
            "Iteration 459: Logistic Loss = 1.2500638324203603\n",
            "Iteration 460: Logistic Loss = 1.2326148314234675\n",
            "Iteration 461: Logistic Loss = 1.249707830090437\n",
            "Iteration 462: Logistic Loss = 1.2331162096160633\n",
            "Iteration 463: Logistic Loss = 1.2517003074747362\n",
            "Iteration 464: Logistic Loss = 1.2347693301456788\n",
            "Iteration 465: Logistic Loss = 1.2536198058728476\n",
            "Iteration 466: Logistic Loss = 1.2371642801302343\n",
            "Iteration 467: Logistic Loss = 1.255780855483106\n",
            "Iteration 468: Logistic Loss = 1.2390769537434774\n",
            "Iteration 469: Logistic Loss = 1.2559874264755773\n",
            "Iteration 470: Logistic Loss = 1.2409635460923585\n",
            "Iteration 471: Logistic Loss = 1.255677192300862\n",
            "Iteration 472: Logistic Loss = 1.2419934335428837\n",
            "Iteration 473: Logistic Loss = 1.2538932093081145\n",
            "Iteration 474: Logistic Loss = 1.2418992755184737\n",
            "Iteration 475: Logistic Loss = 1.2510505670653493\n",
            "Iteration 476: Logistic Loss = 1.2410206568474877\n",
            "Iteration 477: Logistic Loss = 1.2481341879552277\n",
            "Iteration 478: Logistic Loss = 1.239424787274129\n",
            "Iteration 479: Logistic Loss = 1.2451219108200238\n",
            "Iteration 480: Logistic Loss = 1.237214741676629\n",
            "Iteration 481: Logistic Loss = 1.2419248604199238\n",
            "Iteration 482: Logistic Loss = 1.234995868993799\n",
            "Iteration 483: Logistic Loss = 1.2392681970021335\n",
            "Iteration 484: Logistic Loss = 1.233195971781939\n",
            "Iteration 485: Logistic Loss = 1.2376784009228456\n",
            "Iteration 486: Logistic Loss = 1.2318762452581986\n",
            "Iteration 487: Logistic Loss = 1.236567343224397\n",
            "Iteration 488: Logistic Loss = 1.2308045943003776\n",
            "Iteration 489: Logistic Loss = 1.236255746516165\n",
            "Iteration 490: Logistic Loss = 1.2299772096109147\n",
            "Iteration 491: Logistic Loss = 1.236505539765203\n",
            "Iteration 492: Logistic Loss = 1.2291736214030515\n",
            "Iteration 493: Logistic Loss = 1.2373499805289692\n",
            "Iteration 494: Logistic Loss = 1.228513403379021\n",
            "Iteration 495: Logistic Loss = 1.2398177995438981\n",
            "Iteration 496: Logistic Loss = 1.2283902390834684\n",
            "Iteration 497: Logistic Loss = 1.2451912112220112\n",
            "Iteration 498: Logistic Loss = 1.2308059099765027\n",
            "Iteration 499: Logistic Loss = 1.257331369284255\n",
            "Iteration 500: Logistic Loss = 1.2432302547202627\n",
            "Iteration 501: Logistic Loss = 1.2819880297291786\n",
            "Iteration 502: Logistic Loss = 1.2829500074778089\n",
            "Iteration 503: Logistic Loss = 1.3049969865744786\n",
            "Iteration 504: Logistic Loss = 1.3263424396206858\n",
            "Iteration 505: Logistic Loss = 1.2817918677020321\n",
            "Iteration 506: Logistic Loss = 1.2903071354987197\n",
            "Iteration 507: Logistic Loss = 1.2490335467326377\n",
            "Iteration 508: Logistic Loss = 1.2468305981174095\n",
            "Iteration 509: Logistic Loss = 1.228002980043316\n",
            "Iteration 510: Logistic Loss = 1.2261373360837544\n",
            "Iteration 511: Logistic Loss = 1.2177013371530887\n",
            "Iteration 512: Logistic Loss = 1.216888247591126\n",
            "Iteration 513: Logistic Loss = 1.2124668039735769\n",
            "Iteration 514: Logistic Loss = 1.2126054540440812\n",
            "Iteration 515: Logistic Loss = 1.209976916120237\n",
            "Iteration 516: Logistic Loss = 1.2109149548303497\n",
            "Iteration 517: Logistic Loss = 1.2092400348444394\n",
            "Iteration 518: Logistic Loss = 1.21129899929197\n",
            "Iteration 519: Logistic Loss = 1.210422139068224\n",
            "Iteration 520: Logistic Loss = 1.2143249022116902\n",
            "Iteration 521: Logistic Loss = 1.215272607927349\n",
            "Iteration 522: Logistic Loss = 1.2233562705912737\n",
            "Iteration 523: Logistic Loss = 1.2298834647023658\n",
            "Iteration 524: Logistic Loss = 1.248002021569734\n",
            "Iteration 525: Logistic Loss = 1.2610382035829573\n",
            "Iteration 526: Logistic Loss = 1.2914499651129503\n",
            "Iteration 527: Logistic Loss = 1.2734827824746615\n",
            "Iteration 528: Logistic Loss = 1.2913025446707607\n",
            "Iteration 529: Logistic Loss = 1.2494847986348439\n",
            "Iteration 530: Logistic Loss = 1.2616447101897907\n",
            "Iteration 531: Logistic Loss = 1.2345519626042627\n",
            "Iteration 532: Logistic Loss = 1.24989836933594\n",
            "Iteration 533: Logistic Loss = 1.2280323035852974\n",
            "Iteration 534: Logistic Loss = 1.246858792756735\n",
            "Iteration 535: Logistic Loss = 1.2257322129759571\n",
            "Iteration 536: Logistic Loss = 1.2475740636706276\n",
            "Iteration 537: Logistic Loss = 1.2250008896394982\n",
            "Iteration 538: Logistic Loss = 1.2481053331469645\n",
            "Iteration 539: Logistic Loss = 1.2251849431831647\n",
            "Iteration 540: Logistic Loss = 1.248500332745195\n",
            "Iteration 541: Logistic Loss = 1.225433354389978\n",
            "Iteration 542: Logistic Loss = 1.2469945048166877\n",
            "Iteration 543: Logistic Loss = 1.2264726879004988\n",
            "Iteration 544: Logistic Loss = 1.2449948296832893\n",
            "Iteration 545: Logistic Loss = 1.2276784169930555\n",
            "Iteration 546: Logistic Loss = 1.2419126627029056\n",
            "Iteration 547: Logistic Loss = 1.2288587363956969\n",
            "Iteration 548: Logistic Loss = 1.238311075622276\n",
            "Iteration 549: Logistic Loss = 1.2296133615262583\n",
            "Iteration 550: Logistic Loss = 1.2344425618451924\n",
            "Iteration 551: Logistic Loss = 1.229638237321475\n",
            "Iteration 552: Logistic Loss = 1.2300377684285733\n",
            "Iteration 553: Logistic Loss = 1.2287229732477625\n",
            "Iteration 554: Logistic Loss = 1.2255900549939973\n",
            "Iteration 555: Logistic Loss = 1.22800639336931\n",
            "Iteration 556: Logistic Loss = 1.222164186922582\n",
            "Iteration 557: Logistic Loss = 1.2281730639790769\n",
            "Iteration 558: Logistic Loss = 1.2210713218141729\n",
            "Iteration 559: Logistic Loss = 1.2314249039749123\n",
            "Iteration 560: Logistic Loss = 1.2265578704185578\n",
            "Iteration 561: Logistic Loss = 1.2420831483863934\n",
            "Iteration 562: Logistic Loss = 1.2478222759601154\n",
            "Iteration 563: Logistic Loss = 1.27069543203456\n",
            "Iteration 564: Logistic Loss = 1.2766616392778112\n",
            "Iteration 565: Logistic Loss = 1.2942267186979266\n",
            "Iteration 566: Logistic Loss = 1.2583273036509408\n",
            "Iteration 567: Logistic Loss = 1.2629959082993756\n",
            "Iteration 568: Logistic Loss = 1.234931872134431\n",
            "Iteration 569: Logistic Loss = 1.2446725865754926\n",
            "Iteration 570: Logistic Loss = 1.2249821509925776\n",
            "Iteration 571: Logistic Loss = 1.237997735834861\n",
            "Iteration 572: Logistic Loss = 1.2213823400721036\n",
            "Iteration 573: Logistic Loss = 1.2366241225184236\n",
            "Iteration 574: Logistic Loss = 1.220155480435409\n",
            "Iteration 575: Logistic Loss = 1.236664555533626\n",
            "Iteration 576: Logistic Loss = 1.2196938802663917\n",
            "Iteration 577: Logistic Loss = 1.2365977756862092\n",
            "Iteration 578: Logistic Loss = 1.2197753454799665\n",
            "Iteration 579: Logistic Loss = 1.2368074674280394\n",
            "Iteration 580: Logistic Loss = 1.219923503761051\n",
            "Iteration 581: Logistic Loss = 1.2360350630039847\n",
            "Iteration 582: Logistic Loss = 1.220347798282832\n",
            "Iteration 583: Logistic Loss = 1.2350891958749382\n",
            "Iteration 584: Logistic Loss = 1.2217256087645387\n",
            "Iteration 585: Logistic Loss = 1.2350731484659276\n",
            "Iteration 586: Logistic Loss = 1.223604731717247\n",
            "Iteration 587: Logistic Loss = 1.2348776917224933\n",
            "Iteration 588: Logistic Loss = 1.225628464297509\n",
            "Iteration 589: Logistic Loss = 1.234572673502557\n",
            "Iteration 590: Logistic Loss = 1.2272986196343145\n",
            "Iteration 591: Logistic Loss = 1.233655787364965\n",
            "Iteration 592: Logistic Loss = 1.2281468099558517\n",
            "Iteration 593: Logistic Loss = 1.23199647808413\n",
            "Iteration 594: Logistic Loss = 1.227010737948944\n",
            "Iteration 595: Logistic Loss = 1.2291414458358858\n",
            "Iteration 596: Logistic Loss = 1.2244485577336552\n",
            "Iteration 597: Logistic Loss = 1.2263418666467634\n",
            "Iteration 598: Logistic Loss = 1.2212925241635093\n",
            "Iteration 599: Logistic Loss = 1.2250464904221072\n",
            "Iteration 600: Logistic Loss = 1.2192347128874297\n",
            "Iteration 601: Logistic Loss = 1.2280916355931981\n",
            "Iteration 602: Logistic Loss = 1.2213733919976504\n",
            "Iteration 603: Logistic Loss = 1.2388153945054068\n",
            "Iteration 604: Logistic Loss = 1.2345056673888912\n",
            "Iteration 605: Logistic Loss = 1.2569352834560186\n",
            "Iteration 606: Logistic Loss = 1.2647304479742925\n",
            "Iteration 607: Logistic Loss = 1.2529444222804997\n",
            "Iteration 608: Logistic Loss = 1.270377763527048\n",
            "Iteration 609: Logistic Loss = 1.2234392062696307\n",
            "Iteration 610: Logistic Loss = 1.2365703101678582\n",
            "Iteration 611: Logistic Loss = 1.2125201459972055\n",
            "Iteration 612: Logistic Loss = 1.2297667403263928\n",
            "Iteration 613: Logistic Loss = 1.2093384637217364\n",
            "Iteration 614: Logistic Loss = 1.230105540478468\n",
            "Iteration 615: Logistic Loss = 1.2103151143178699\n",
            "Iteration 616: Logistic Loss = 1.2336552527948765\n",
            "Iteration 617: Logistic Loss = 1.212923880117645\n",
            "Iteration 618: Logistic Loss = 1.2363555865188105\n",
            "Iteration 619: Logistic Loss = 1.2167054644443125\n",
            "Iteration 620: Logistic Loss = 1.2386387942542212\n",
            "Iteration 621: Logistic Loss = 1.2210953374197278\n",
            "Iteration 622: Logistic Loss = 1.2395881008684544\n",
            "Iteration 623: Logistic Loss = 1.2245826599339817\n",
            "Iteration 624: Logistic Loss = 1.2381223963942323\n",
            "Iteration 625: Logistic Loss = 1.2263337805513836\n",
            "Iteration 626: Logistic Loss = 1.2352760590105594\n",
            "Iteration 627: Logistic Loss = 1.224709949370905\n",
            "Iteration 628: Logistic Loss = 1.2299756102499133\n",
            "Iteration 629: Logistic Loss = 1.221105232206849\n",
            "Iteration 630: Logistic Loss = 1.2244483666190202\n",
            "Iteration 631: Logistic Loss = 1.2170882245595913\n",
            "Iteration 632: Logistic Loss = 1.2198348629716085\n",
            "Iteration 633: Logistic Loss = 1.213201156124127\n",
            "Iteration 634: Logistic Loss = 1.2160242564687427\n",
            "Iteration 635: Logistic Loss = 1.2103780903213632\n",
            "Iteration 636: Logistic Loss = 1.2144245882246196\n",
            "Iteration 637: Logistic Loss = 1.2089969402903098\n",
            "Iteration 638: Logistic Loss = 1.214622787982612\n",
            "Iteration 639: Logistic Loss = 1.2082941100168734\n",
            "Iteration 640: Logistic Loss = 1.2161928435464369\n",
            "Iteration 641: Logistic Loss = 1.2080287856875578\n",
            "Iteration 642: Logistic Loss = 1.2197542178048402\n",
            "Iteration 643: Logistic Loss = 1.2080667558750018\n",
            "Iteration 644: Logistic Loss = 1.2267686709131738\n",
            "Iteration 645: Logistic Loss = 1.2094902229557538\n",
            "Iteration 646: Logistic Loss = 1.2396655786020996\n",
            "Iteration 647: Logistic Loss = 1.2185119392136066\n",
            "Iteration 648: Logistic Loss = 1.2631337218382923\n",
            "Iteration 649: Logistic Loss = 1.2518839312805639\n",
            "Iteration 650: Logistic Loss = 1.2873147406654373\n",
            "Iteration 651: Logistic Loss = 1.2973144495001483\n",
            "Iteration 652: Logistic Loss = 1.2636464634033453\n",
            "Iteration 653: Logistic Loss = 1.2712809713371451\n",
            "Iteration 654: Logistic Loss = 1.2284923072342873\n",
            "Iteration 655: Logistic Loss = 1.226927947109084\n",
            "Iteration 656: Logistic Loss = 1.2067351741503847\n",
            "Iteration 657: Logistic Loss = 1.2053047460417177\n",
            "Iteration 658: Logistic Loss = 1.195907528802681\n",
            "Iteration 659: Logistic Loss = 1.1955105123601335\n",
            "Iteration 660: Logistic Loss = 1.1907343241334385\n",
            "Iteration 661: Logistic Loss = 1.1914485092657658\n",
            "Iteration 662: Logistic Loss = 1.1888840595637493\n",
            "Iteration 663: Logistic Loss = 1.191071011024327\n",
            "Iteration 664: Logistic Loss = 1.1901492622571572\n",
            "Iteration 665: Logistic Loss = 1.1948575383282163\n",
            "Iteration 666: Logistic Loss = 1.1967329097305985\n",
            "Iteration 667: Logistic Loss = 1.2074490140925627\n",
            "Iteration 668: Logistic Loss = 1.2150995691839046\n",
            "Iteration 669: Logistic Loss = 1.2363362039535104\n",
            "Iteration 670: Logistic Loss = 1.2395779799093367\n",
            "Iteration 671: Logistic Loss = 1.2666409490756452\n",
            "Iteration 672: Logistic Loss = 1.2355097059926787\n",
            "Iteration 673: Logistic Loss = 1.2552549317090391\n",
            "Iteration 674: Logistic Loss = 1.2183743508996006\n",
            "Iteration 675: Logistic Loss = 1.239556280054362\n",
            "Iteration 676: Logistic Loss = 1.2098628357223473\n",
            "Iteration 677: Logistic Loss = 1.2350214360844933\n",
            "Iteration 678: Logistic Loss = 1.2060982818253683\n",
            "Iteration 679: Logistic Loss = 1.2330264804641125\n",
            "Iteration 680: Logistic Loss = 1.204392352647213\n",
            "Iteration 681: Logistic Loss = 1.2322346454865785\n",
            "Iteration 682: Logistic Loss = 1.2038789705006576\n",
            "Iteration 683: Logistic Loss = 1.2310493995087688\n",
            "Iteration 684: Logistic Loss = 1.204666174721447\n",
            "Iteration 685: Logistic Loss = 1.2291250739766753\n",
            "Iteration 686: Logistic Loss = 1.2071867890335772\n",
            "Iteration 687: Logistic Loss = 1.2276166714633407\n",
            "Iteration 688: Logistic Loss = 1.2119874778158999\n",
            "Iteration 689: Logistic Loss = 1.2262076228380108\n",
            "Iteration 690: Logistic Loss = 1.2182400718940862\n",
            "Iteration 691: Logistic Loss = 1.223114283844965\n",
            "Iteration 692: Logistic Loss = 1.2247701588878823\n",
            "Iteration 693: Logistic Loss = 1.2191871046880451\n",
            "Iteration 694: Logistic Loss = 1.2274810883555276\n",
            "Iteration 695: Logistic Loss = 1.2139379013306861\n",
            "Iteration 696: Logistic Loss = 1.2246159533770984\n",
            "Iteration 697: Logistic Loss = 1.2099381576308113\n",
            "Iteration 698: Logistic Loss = 1.2206715540261712\n",
            "Iteration 699: Logistic Loss = 1.2126754331827654\n",
            "Iteration 700: Logistic Loss = 1.2233105893182716\n",
            "Iteration 701: Logistic Loss = 1.2246989203465597\n",
            "Iteration 702: Logistic Loss = 1.2410248739975382\n",
            "Iteration 703: Logistic Loss = 1.2320573321827122\n",
            "Iteration 704: Logistic Loss = 1.2521383628793075\n",
            "Iteration 705: Logistic Loss = 1.2204854985979052\n",
            "Iteration 706: Logistic Loss = 1.2380694076870895\n",
            "Iteration 707: Logistic Loss = 1.210232694480043\n",
            "Iteration 708: Logistic Loss = 1.2291734059303634\n",
            "Iteration 709: Logistic Loss = 1.2052593161639389\n",
            "Iteration 710: Logistic Loss = 1.2247461562135482\n",
            "Iteration 711: Logistic Loss = 1.2030862917657943\n",
            "Iteration 712: Logistic Loss = 1.2228060672754086\n",
            "Iteration 713: Logistic Loss = 1.2022793837287968\n",
            "Iteration 714: Logistic Loss = 1.2217309165631713\n",
            "Iteration 715: Logistic Loss = 1.202627399332105\n",
            "Iteration 716: Logistic Loss = 1.221260487871393\n",
            "Iteration 717: Logistic Loss = 1.2038380340425565\n",
            "Iteration 718: Logistic Loss = 1.2210758173282779\n",
            "Iteration 719: Logistic Loss = 1.2055785577945608\n",
            "Iteration 720: Logistic Loss = 1.2199651850048583\n",
            "Iteration 721: Logistic Loss = 1.2076457093654789\n",
            "Iteration 722: Logistic Loss = 1.2190100092804512\n",
            "Iteration 723: Logistic Loss = 1.2099443990361474\n",
            "Iteration 724: Logistic Loss = 1.2181822357615382\n",
            "Iteration 725: Logistic Loss = 1.2112966873598516\n",
            "Iteration 726: Logistic Loss = 1.2162004492442267\n",
            "Iteration 727: Logistic Loss = 1.2104531669394605\n",
            "Iteration 728: Logistic Loss = 1.2130159482848106\n",
            "Iteration 729: Logistic Loss = 1.207842529087907\n",
            "Iteration 730: Logistic Loss = 1.210201338634146\n",
            "Iteration 731: Logistic Loss = 1.2046282278389646\n",
            "Iteration 732: Logistic Loss = 1.2099916730667635\n",
            "Iteration 733: Logistic Loss = 1.2031811989920256\n",
            "Iteration 734: Logistic Loss = 1.2152256828392847\n",
            "Iteration 735: Logistic Loss = 1.2085119843293826\n",
            "Iteration 736: Logistic Loss = 1.2312709743236105\n",
            "Iteration 737: Logistic Loss = 1.232353324710067\n",
            "Iteration 738: Logistic Loss = 1.2456408793193297\n",
            "Iteration 739: Logistic Loss = 1.261692876747444\n",
            "Iteration 740: Logistic Loss = 1.2154423007358879\n",
            "Iteration 741: Logistic Loss = 1.2296658616881475\n",
            "Iteration 742: Logistic Loss = 1.1959654651228464\n",
            "Iteration 743: Logistic Loss = 1.2132110925692923\n",
            "Iteration 744: Logistic Loss = 1.1906565180078463\n",
            "Iteration 745: Logistic Loss = 1.212301946219552\n",
            "Iteration 746: Logistic Loss = 1.190966665314385\n",
            "Iteration 747: Logistic Loss = 1.2149921135871313\n",
            "Iteration 748: Logistic Loss = 1.1937605900467971\n",
            "Iteration 749: Logistic Loss = 1.218821439425023\n",
            "Iteration 750: Logistic Loss = 1.1982108037919281\n",
            "Iteration 751: Logistic Loss = 1.2219468062267533\n",
            "Iteration 752: Logistic Loss = 1.2037114710869634\n",
            "Iteration 753: Logistic Loss = 1.2237172924203845\n",
            "Iteration 754: Logistic Loss = 1.2082012701567777\n",
            "Iteration 755: Logistic Loss = 1.222988098629515\n",
            "Iteration 756: Logistic Loss = 1.2104410602296325\n",
            "Iteration 757: Logistic Loss = 1.219610322456017\n",
            "Iteration 758: Logistic Loss = 1.2090552226973945\n",
            "Iteration 759: Logistic Loss = 1.2136205997859728\n",
            "Iteration 760: Logistic Loss = 1.20484557168404\n",
            "Iteration 761: Logistic Loss = 1.2072221758775776\n",
            "Iteration 762: Logistic Loss = 1.199897621905202\n",
            "Iteration 763: Logistic Loss = 1.2017574058073877\n",
            "Iteration 764: Logistic Loss = 1.1958005470207125\n",
            "Iteration 765: Logistic Loss = 1.1978020632521829\n",
            "Iteration 766: Logistic Loss = 1.1928526575558158\n",
            "Iteration 767: Logistic Loss = 1.1959014111391062\n",
            "Iteration 768: Logistic Loss = 1.1915599470236165\n",
            "Iteration 769: Logistic Loss = 1.1958287123574645\n",
            "Iteration 770: Logistic Loss = 1.1912556563182963\n",
            "Iteration 771: Logistic Loss = 1.1971184050312662\n",
            "Iteration 772: Logistic Loss = 1.1913902021569138\n",
            "Iteration 773: Logistic Loss = 1.1998646822399468\n",
            "Iteration 774: Logistic Loss = 1.1916685748534428\n",
            "Iteration 775: Logistic Loss = 1.2062277421369298\n",
            "Iteration 776: Logistic Loss = 1.1923126867982117\n",
            "Iteration 777: Logistic Loss = 1.2198278933588316\n",
            "Iteration 778: Logistic Loss = 1.197730166367044\n",
            "Iteration 779: Logistic Loss = 1.2485772006657292\n",
            "Iteration 780: Logistic Loss = 1.223960738216592\n",
            "Iteration 781: Logistic Loss = 1.2841886857159543\n",
            "Iteration 782: Logistic Loss = 1.279982432502831\n",
            "Iteration 783: Logistic Loss = 1.2553769077914259\n",
            "Iteration 784: Logistic Loss = 1.2654457178586882\n",
            "Iteration 785: Logistic Loss = 1.2113567744906422\n",
            "Iteration 786: Logistic Loss = 1.2108038770437715\n",
            "Iteration 787: Logistic Loss = 1.1875757385993422\n",
            "Iteration 788: Logistic Loss = 1.1868762012706193\n",
            "Iteration 789: Logistic Loss = 1.1769985493427588\n",
            "Iteration 790: Logistic Loss = 1.177499066277442\n",
            "Iteration 791: Logistic Loss = 1.1728887530597985\n",
            "Iteration 792: Logistic Loss = 1.1749855345195184\n",
            "Iteration 793: Logistic Loss = 1.1728218199007647\n",
            "Iteration 794: Logistic Loss = 1.177037776365754\n",
            "Iteration 795: Logistic Loss = 1.1764549232068116\n",
            "Iteration 796: Logistic Loss = 1.1837193789067193\n",
            "Iteration 797: Logistic Loss = 1.1846683179589486\n",
            "Iteration 798: Logistic Loss = 1.1946152811214035\n",
            "Iteration 799: Logistic Loss = 1.2006560035380476\n",
            "Iteration 800: Logistic Loss = 1.206446140479455\n",
            "Iteration 801: Logistic Loss = 1.2284016853925503\n",
            "Iteration 802: Logistic Loss = 1.2015129334486772\n",
            "Iteration 803: Logistic Loss = 1.2368182084951234\n",
            "Iteration 804: Logistic Loss = 1.1860927600273203\n",
            "Iteration 805: Logistic Loss = 1.2182119397232603\n",
            "Iteration 806: Logistic Loss = 1.1907577054211957\n",
            "Iteration 807: Logistic Loss = 1.2278523736628189\n",
            "Iteration 808: Logistic Loss = 1.2086142625283665\n",
            "Iteration 809: Logistic Loss = 1.2425027004898643\n",
            "Iteration 810: Logistic Loss = 1.227379842958904\n",
            "Iteration 811: Logistic Loss = 1.2410813533842886\n",
            "Iteration 812: Logistic Loss = 1.2235477665996175\n",
            "Iteration 813: Logistic Loss = 1.2226802882128573\n",
            "Iteration 814: Logistic Loss = 1.2070489105756337\n",
            "Iteration 815: Logistic Loss = 1.2092114000876077\n",
            "Iteration 816: Logistic Loss = 1.1961335169628693\n",
            "Iteration 817: Logistic Loss = 1.2050149808466442\n",
            "Iteration 818: Logistic Loss = 1.1915456941779956\n",
            "Iteration 819: Logistic Loss = 1.206541102384388\n",
            "Iteration 820: Logistic Loss = 1.189642587419253\n",
            "Iteration 821: Logistic Loss = 1.2102134891933514\n",
            "Iteration 822: Logistic Loss = 1.187811688564231\n",
            "Iteration 823: Logistic Loss = 1.211850763201545\n",
            "Iteration 824: Logistic Loss = 1.1849995762513839\n",
            "Iteration 825: Logistic Loss = 1.2107232243160373\n",
            "Iteration 826: Logistic Loss = 1.182388690436493\n",
            "Iteration 827: Logistic Loss = 1.2092168572074609\n",
            "Iteration 828: Logistic Loss = 1.1829127440874514\n",
            "Iteration 829: Logistic Loss = 1.2124778674349546\n",
            "Iteration 830: Logistic Loss = 1.18837317002472\n",
            "Iteration 831: Logistic Loss = 1.2170016798656842\n",
            "Iteration 832: Logistic Loss = 1.2063834605326458\n",
            "Iteration 833: Logistic Loss = 1.2216376938923805\n",
            "Iteration 834: Logistic Loss = 1.2371000421619451\n",
            "Iteration 835: Logistic Loss = 1.2195732913756316\n",
            "Iteration 836: Logistic Loss = 1.2468667071803812\n",
            "Iteration 837: Logistic Loss = 1.2110893426690952\n",
            "Iteration 838: Logistic Loss = 1.225400698084066\n",
            "Iteration 839: Logistic Loss = 1.1974435340617102\n",
            "Iteration 840: Logistic Loss = 1.1989278991711472\n",
            "Iteration 841: Logistic Loss = 1.186940770173775\n",
            "Iteration 842: Logistic Loss = 1.1905829034625346\n",
            "Iteration 843: Logistic Loss = 1.1873230355631685\n",
            "Iteration 844: Logistic Loss = 1.203295482928056\n",
            "Iteration 845: Logistic Loss = 1.1946720510971225\n",
            "Iteration 846: Logistic Loss = 1.2222487931432318\n",
            "Iteration 847: Logistic Loss = 1.1936238055753452\n",
            "Iteration 848: Logistic Loss = 1.22144891177915\n",
            "Iteration 849: Logistic Loss = 1.187571028780873\n",
            "Iteration 850: Logistic Loss = 1.21238304072926\n",
            "Iteration 851: Logistic Loss = 1.1835399135341316\n",
            "Iteration 852: Logistic Loss = 1.2077017963534065\n",
            "Iteration 853: Logistic Loss = 1.181485208849951\n",
            "Iteration 854: Logistic Loss = 1.2050623444846467\n",
            "Iteration 855: Logistic Loss = 1.1804650581793312\n",
            "Iteration 856: Logistic Loss = 1.2024931989701981\n",
            "Iteration 857: Logistic Loss = 1.1804972143836547\n",
            "Iteration 858: Logistic Loss = 1.2011841446608937\n",
            "Iteration 859: Logistic Loss = 1.1819808171966426\n",
            "Iteration 860: Logistic Loss = 1.2010363849683638\n",
            "Iteration 861: Logistic Loss = 1.186664315551536\n",
            "Iteration 862: Logistic Loss = 1.204774064603222\n",
            "Iteration 863: Logistic Loss = 1.195011975507276\n",
            "Iteration 864: Logistic Loss = 1.2099633641632566\n",
            "Iteration 865: Logistic Loss = 1.205322353401946\n",
            "Iteration 866: Logistic Loss = 1.2150334652748533\n",
            "Iteration 867: Logistic Loss = 1.209102902687348\n",
            "Iteration 868: Logistic Loss = 1.215200676758217\n",
            "Iteration 869: Logistic Loss = 1.2031499324081798\n",
            "Iteration 870: Logistic Loss = 1.2180130218047396\n",
            "Iteration 871: Logistic Loss = 1.2033551655779209\n",
            "Iteration 872: Logistic Loss = 1.230762286267375\n",
            "Iteration 873: Logistic Loss = 1.2204961444041358\n",
            "Iteration 874: Logistic Loss = 1.2203447489006545\n",
            "Iteration 875: Logistic Loss = 1.2192714535950675\n",
            "Iteration 876: Logistic Loss = 1.1841090872494888\n",
            "Iteration 877: Logistic Loss = 1.1864145753701214\n",
            "Iteration 878: Logistic Loss = 1.1708292457837992\n",
            "Iteration 879: Logistic Loss = 1.1823948324989038\n",
            "Iteration 880: Logistic Loss = 1.1695368530090222\n",
            "Iteration 881: Logistic Loss = 1.1885862743924316\n",
            "Iteration 882: Logistic Loss = 1.1717522716040774\n",
            "Iteration 883: Logistic Loss = 1.1945635694857235\n",
            "Iteration 884: Logistic Loss = 1.1748331540053218\n",
            "Iteration 885: Logistic Loss = 1.1972003102814188\n",
            "Iteration 886: Logistic Loss = 1.1808969093533943\n",
            "Iteration 887: Logistic Loss = 1.2026914926767707\n",
            "Iteration 888: Logistic Loss = 1.1908662114521105\n",
            "Iteration 889: Logistic Loss = 1.2096529348919045\n",
            "Iteration 890: Logistic Loss = 1.2003820413354096\n",
            "Iteration 891: Logistic Loss = 1.2104141620470013\n",
            "Iteration 892: Logistic Loss = 1.200240978903391\n",
            "Iteration 893: Logistic Loss = 1.2003532446884642\n",
            "Iteration 894: Logistic Loss = 1.1904420727402325\n",
            "Iteration 895: Logistic Loss = 1.1878259573688255\n",
            "Iteration 896: Logistic Loss = 1.1796979509682173\n",
            "Iteration 897: Logistic Loss = 1.178654307550198\n",
            "Iteration 898: Logistic Loss = 1.1727666515672583\n",
            "Iteration 899: Logistic Loss = 1.174272642591393\n",
            "Iteration 900: Logistic Loss = 1.1703710049881006\n",
            "Iteration 901: Logistic Loss = 1.1748613934556016\n",
            "Iteration 902: Logistic Loss = 1.1713720171109956\n",
            "Iteration 903: Logistic Loss = 1.1780234674576204\n",
            "Iteration 904: Logistic Loss = 1.1735782233551668\n",
            "Iteration 905: Logistic Loss = 1.1845320425323704\n",
            "Iteration 906: Logistic Loss = 1.1796123043475686\n",
            "Iteration 907: Logistic Loss = 1.2023018266891796\n",
            "Iteration 908: Logistic Loss = 1.2038989633775594\n",
            "Iteration 909: Logistic Loss = 1.2345519323311607\n",
            "Iteration 910: Logistic Loss = 1.2496673590489484\n",
            "Iteration 911: Logistic Loss = 1.1964206461434865\n",
            "Iteration 912: Logistic Loss = 1.198090254493934\n",
            "Iteration 913: Logistic Loss = 1.1687858839603564\n",
            "Iteration 914: Logistic Loss = 1.1771496818095633\n",
            "Iteration 915: Logistic Loss = 1.163910060765508\n",
            "Iteration 916: Logistic Loss = 1.182937413182823\n",
            "Iteration 917: Logistic Loss = 1.1711267789548656\n",
            "Iteration 918: Logistic Loss = 1.200158234852913\n",
            "Iteration 919: Logistic Loss = 1.1797534505801308\n",
            "Iteration 920: Logistic Loss = 1.2056692109162928\n",
            "Iteration 921: Logistic Loss = 1.1850306588243134\n",
            "Iteration 922: Logistic Loss = 1.2050568769926662\n",
            "Iteration 923: Logistic Loss = 1.189644203425813\n",
            "Iteration 924: Logistic Loss = 1.2038109525562455\n",
            "Iteration 925: Logistic Loss = 1.1902308217929467\n",
            "Iteration 926: Logistic Loss = 1.199125791455444\n",
            "Iteration 927: Logistic Loss = 1.1853330686397143\n",
            "Iteration 928: Logistic Loss = 1.1929087501297304\n",
            "Iteration 929: Logistic Loss = 1.179574342464663\n",
            "Iteration 930: Logistic Loss = 1.1909754095644625\n",
            "Iteration 931: Logistic Loss = 1.1758353839553315\n",
            "Iteration 932: Logistic Loss = 1.1948253467145082\n",
            "Iteration 933: Logistic Loss = 1.1741039668339648\n",
            "Iteration 934: Logistic Loss = 1.2017717240443182\n",
            "Iteration 935: Logistic Loss = 1.1713433353245801\n",
            "Iteration 936: Logistic Loss = 1.2015581803746522\n",
            "Iteration 937: Logistic Loss = 1.1738224284548504\n",
            "Iteration 938: Logistic Loss = 1.201420482649971\n",
            "Iteration 939: Logistic Loss = 1.1886335506078483\n",
            "Iteration 940: Logistic Loss = 1.195908634756656\n",
            "Iteration 941: Logistic Loss = 1.2082770270147436\n",
            "Iteration 942: Logistic Loss = 1.1885902982281693\n",
            "Iteration 943: Logistic Loss = 1.2117923144218274\n",
            "Iteration 944: Logistic Loss = 1.191916201388772\n",
            "Iteration 945: Logistic Loss = 1.212915028994612\n",
            "Iteration 946: Logistic Loss = 1.1873148903930149\n",
            "Iteration 947: Logistic Loss = 1.1901887730340304\n",
            "Iteration 948: Logistic Loss = 1.1714861302123534\n",
            "Iteration 949: Logistic Loss = 1.16933440319623\n",
            "Iteration 950: Logistic Loss = 1.1617700176602483\n",
            "Iteration 951: Logistic Loss = 1.1659646025386166\n",
            "Iteration 952: Logistic Loss = 1.165027171663558\n",
            "Iteration 953: Logistic Loss = 1.1828505691710922\n",
            "Iteration 954: Logistic Loss = 1.1802520322492966\n",
            "Iteration 955: Logistic Loss = 1.2222025510893764\n",
            "Iteration 956: Logistic Loss = 1.1829097759439926\n",
            "Iteration 957: Logistic Loss = 1.2243528069318854\n",
            "Iteration 958: Logistic Loss = 1.1688647077450947\n",
            "Iteration 959: Logistic Loss = 1.1938356963686878\n",
            "Iteration 960: Logistic Loss = 1.1630329952790355\n",
            "Iteration 961: Logistic Loss = 1.1802353177239684\n",
            "Iteration 962: Logistic Loss = 1.1615547262325956\n",
            "Iteration 963: Logistic Loss = 1.1741706924982878\n",
            "Iteration 964: Logistic Loss = 1.1641770685891835\n",
            "Iteration 965: Logistic Loss = 1.1726191933121435\n",
            "Iteration 966: Logistic Loss = 1.172293319114952\n",
            "Iteration 967: Logistic Loss = 1.1743019620351332\n",
            "Iteration 968: Logistic Loss = 1.1844077591972044\n",
            "Iteration 969: Logistic Loss = 1.17746267057865\n",
            "Iteration 970: Logistic Loss = 1.2007823294509357\n",
            "Iteration 971: Logistic Loss = 1.1889758703990547\n",
            "Iteration 972: Logistic Loss = 1.218152071610419\n",
            "Iteration 973: Logistic Loss = 1.1945945776050184\n",
            "Iteration 974: Logistic Loss = 1.2025448308829658\n",
            "Iteration 975: Logistic Loss = 1.180461489399861\n",
            "Iteration 976: Logistic Loss = 1.178117037824163\n",
            "Iteration 977: Logistic Loss = 1.1710937173793565\n",
            "Iteration 978: Logistic Loss = 1.1888756880618463\n",
            "Iteration 979: Logistic Loss = 1.1813713558461054\n",
            "Iteration 980: Logistic Loss = 1.2248489033531675\n",
            "Iteration 981: Logistic Loss = 1.1763720061428615\n",
            "Iteration 982: Logistic Loss = 1.2076398121140994\n",
            "Iteration 983: Logistic Loss = 1.1624447080024107\n",
            "Iteration 984: Logistic Loss = 1.1812961128246753\n",
            "Iteration 985: Logistic Loss = 1.155932704567982\n",
            "Iteration 986: Logistic Loss = 1.1736460883034856\n",
            "Iteration 987: Logistic Loss = 1.1569153551972629\n",
            "Iteration 988: Logistic Loss = 1.1792412972588109\n",
            "Iteration 989: Logistic Loss = 1.163650521339752\n",
            "Iteration 990: Logistic Loss = 1.187870698096299\n",
            "Iteration 991: Logistic Loss = 1.1772629114409594\n",
            "Iteration 992: Logistic Loss = 1.201222282891538\n",
            "Iteration 993: Logistic Loss = 1.1953470418292107\n",
            "Iteration 994: Logistic Loss = 1.2073935158175801\n",
            "Iteration 995: Logistic Loss = 1.1972346029508545\n",
            "Iteration 996: Logistic Loss = 1.1936118759162613\n",
            "Iteration 997: Logistic Loss = 1.1776145580393982\n",
            "Iteration 998: Logistic Loss = 1.177179063371067\n",
            "Iteration 999: Logistic Loss = 1.1647305086973536\n",
            "Iteration 1000: Logistic Loss = 1.1806819748499084\n"
          ]
        }
      ],
      "source": [
        "# Add more layers for hte multilayer perceptron\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "learning_rate = 0.1\n",
        "\n",
        "num_features= M_train.shape[1]\n",
        "hidden_size = 64\n",
        "num_classes = 3\n",
        "\n",
        "helpfulness_labels = y_train # 3,2198\n",
        "np.random.seed(1)\n",
        "# Initialize weights_0_1 with positive values to encourage ReLU activation\n",
        "weights_0_1 = np.random.rand(num_features,hidden_size) * np.sqrt(2 / num_features)\n",
        "weights_1_2 = np.random.randn(hidden_size,num_classes) * np.sqrt(2 / hidden_size)\n",
        "y_train_target = y_train.T\n",
        "\n",
        "loss_history = []\n",
        "n_iters=1000\n",
        "num_samples = M_train.shape[0] # Number of training samples\n",
        "\n",
        "for iteration in range(n_iters):\n",
        "\n",
        "    layer_2_error = 0\n",
        "    layer_0 = M_train\n",
        "\n",
        "    ## Add forward pass\n",
        "    layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "    layer_2 = np.dot(layer_1,weights_1_2)\n",
        "   \n",
        "\n",
        "\n",
        "    # Then apply sigmoid\n",
        "    # layer_2_s = 1/(1+np.exp(-layer_2))\n",
        "    exp_z = np.exp(layer_2)\n",
        "    layer_2_s = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "    q = layer_2_s\n",
        "\n",
        "\n",
        "    # # Helped by AI chatgpt\n",
        "    eps = 1e-8\n",
        "    # # ----- BCE loss report -----\n",
        "    # eps = 1e-12\n",
        "    # probability assigned to the correct class for each sample\n",
        "    p_true = np.sum(y_train_target * q, axis=1)         # (N,)\n",
        "    loss = np.mean(-np.log2(np.clip(p_true, eps, 1.0)))  # avoid log(0)\n",
        "\n",
        "    loss_history.append(loss)\n",
        "    print(f\"Iteration {iteration+1}: Logistic Loss = {loss}\")\n",
        "\n",
        "    ## Add backward pass and update weights\n",
        "    layer_2_diff = (layer_2_s - y_train_target)\n",
        "\n",
        "    z1 = np.dot(layer_0, weights_0_1)\n",
        "    relu_grad = (z1 > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "    hidden_delta = np.dot(layer_2_diff, weights_1_2.T) * relu_grad\n",
        "\n",
        "    # Normalize weight updates by N\n",
        "    weights_1_2 -= learning_rate * (np.dot(layer_1.T, layer_2_diff) /num_samples)\n",
        "    weights_0_1 -= learning_rate * (np.dot(layer_0.T, hidden_delta) / num_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.626812585499316\n"
          ]
        }
      ],
      "source": [
        "# Forward pass on test dataset\n",
        "layer_0_test = M_test\n",
        "layer_1_test = np.maximum(np.dot(layer_0_test, weights_0_1), 0)  # ReLU activation\n",
        "layer_2_test = np.dot(layer_1_test, weights_1_2)\n",
        "\n",
        "# Apply softmax\n",
        "exp_z_test = np.exp(layer_2_test)\n",
        "layer_2_s_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "\n",
        "# Get predictions (class with highest probability)\n",
        "y_test_pred = np.argmax(layer_2_s_test, axis=1)\n",
        "\n",
        "# Get true labels\n",
        "y_test_true = np.argmax(y_test.T, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_test_pred == y_test_true)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
