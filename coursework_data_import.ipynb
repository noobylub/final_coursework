{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobylub/final_coursework/blob/main/coursework_data_import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqwi4b5geG6"
      },
      "source": [
        "**Experiment**\n",
        "<br>\n",
        "This research seeks to compare One-Hot-Encoding against Multi-Layer Perceptron, which has be de-facto for many ML problems. \n",
        "<br>\n",
        "The following experiement will be performed and evaluated: \n",
        "\n",
        "*   One hot encoding (OHE), sigmoid\n",
        "*   Multi Layer Perceptron (MLP), sigmoid\n",
        "*   OHE, softmax  \n",
        "*   MLP, softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the Data and Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhNWdx7l84yP",
        "outputId": "8563f28d-5f96-41bb-c5b6-4a9adba9bb8e"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 21.2M  100 21.2M    0     0  21.5M      0 --:--:-- --:--:-- --:--:-- 21.5M\n"
          ]
        }
      ],
      "source": [
        "# Run this when editing in code editor \n",
        "!curl -O https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YHzNVVHQPd9",
        "outputId": "bf37b091-809e-4b00-f8c3-83c688764170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REVIEW\tRATING\tPRODUCT_TYPE\tHELPFUL\n",
            "\"This is a wonderful album, that evokes memories of the 60's folk boom, yet contains original songs. I was amazed at the fantastic harmonies and musical arrangements.Anyone who loves the movie \"\"A Mighty Wind\"\" and who loves folk music will fall in love with this album. I know I did\"\tpositive\tmusic\tneutral\n",
            "\"On one hand, this CD is a straight ahead instrumental rocker, but Johnny A really shows how great he is with ballads, such as his covers of \"\"Wichita Lineman,\"\" and \"\"Yes it Is.\"\"  In fact, those two ballads alone are worth the price of the CD by themselves.But Johnny A can flat kick your ass, too.  He's a biker and his tunes like Oh Yeah, In the Wind and Two Wheel Horse are named for his other hobby.  And they rock, but there's nothing cliched or tired in his style.  He always seems to be looking for new ways to say something.I saw him in person at the Triple Door in Seattle sometime in February 2005 in a power trio format and he played most of the tunes on this album.  The guy is one amazing guitar player.  He played his signature Gibson hollow body, fitted with a vibrato tailpiece (Bigsby? It was like the old Chet Atkins \"\"country gentleman\"\" model) and he utilized a battery of foot pedal effects, coming eventually through a pair of Marshall combo amps.  The guy had some of the best clean tones I've ever heard from anyone, anywhere.Basically, Johnny A is a guitarist who has complete command of the instrument.  And he's got a rocking soul that cuts loose on originals and covers alike in a style that's all his own, and that's saying something these days.  If you love great rocking/rockabilly guitar, combined with really cool ballad playing in the power trio format, this CD is just what you're looking for.  In fact, I guarantee you'll be knocked out.Five stars.\"\tpositive\tmusic\thelpful\n",
            "\"this band reminds me of the thrill i first got when i listened to an Atreyu Album. It dies today rip off the former bands style, but they are still a very good band.  In the over-crowded metalcore market of today, that is a rarity.  My only complaint, is that the vocalist has a beautiful singing voice (as heard on \"\"The Radiance), but more often then not goes for the screams and growls that are associated with this type of music. still 5/5 material though.Favorite songs: \"\"The Radiance,\"\" \"\"Freak Gasoline Fight,\"\" \"\"Our Disintigration,\"\" and :THe Caitliff Choir:Defeatism\"\tpositive\tmusic\tunhelpful\n",
            "\"Like I said I would, I finally got around to purchasing this CD. I especially like tracks 9-12. Cheap Trick is solid as always. I have been a long time fan and enjoy all their releases. If I do have any criticism at all, it is the fact that some songs sound like recycled Trick riffs. I guess it's hard to create new stuff after 30+ years. Anyway, there are some great tunes on this. Buy it like I did!\"\tpositive\tmusic\tunhelpful\n"
          ]
        }
      ],
      "source": [
        "# Example of what the data looks like\n",
        "!head -n5 Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_eYghCK5OX"
      },
      "source": [
        "**Data loading and pre-processing**\n",
        "<br>\n",
        "Below we preprocess the data from the raw file \"Compiled_reviews.txt\"\n",
        "<br>\n",
        "We remove any unwanted characters, to improve the integrity of the text corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Li-IcrXi9O8G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reviews=[]\n",
        "sentiment_ratings=[]\n",
        "product_types=[]\n",
        "helpfulness_ratings=[]\n",
        "\n",
        "with open(\"Compiled_Reviews.txt\") as f:\n",
        "   for line in f.readlines()[1:]:\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # remove punctuation/numbers and replace it with a space\n",
        "        fields[0] = re.sub(r'[.,!?;:()\\[\\]{}\\-â€”\\'\\/\\\"\\\"\\d+]', \" \",fields[0])\n",
        "        reviews.append(fields[0])\n",
        "        sentiment_ratings.append(fields[1])\n",
        "        \n",
        "        helpfulness_ratings.append(fields[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qV1XhmNXI4"
      },
      "source": [
        "**Data Analysis**\n",
        "<br/>\n",
        "Below we see what the data looks like after pre-processing\n",
        "<br/>\n",
        "Data analysis can also be shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMAE3UTkK-jO",
        "outputId": "6c0af6b2-c7fe-43d7-e8a7-87965c784bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is   This is a wonderful album  that evokes memories of the    s folk boom  yet contains original songs  I was amazed at the fantastic harmonies and musical arrangements Anyone who loves the movie   A Mighty Wind   and who loves folk music will fall in love with this album  I know I did \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   On one hand  this CD is a straight ahead instrumental rocker  but Johnny A really shows how great he is with ballads  such as his covers of   Wichita Lineman    and   Yes it Is     In fact  those two ballads alone are worth the price of the CD by themselves But Johnny A can flat kick your ass  too   He s a biker and his tunes like Oh Yeah  In the Wind and Two Wheel Horse are named for his other hobby   And they rock  but there s nothing cliched or tired in his style   He always seems to be looking for new ways to say something I saw him in person at the Triple Door in Seattle sometime in February      in a power trio format and he played most of the tunes on this album   The guy is one amazing guitar player   He played his signature Gibson hollow body  fitted with a vibrato tailpiece  Bigsby  It was like the old Chet Atkins   country gentleman   model  and he utilized a battery of foot pedal effects  coming eventually through a pair of Marshall combo amps   The guy had some of the best clean tones I ve ever heard from anyone  anywhere Basically  Johnny A is a guitarist who has complete command of the instrument   And he s got a rocking soul that cuts loose on originals and covers alike in a style that s all his own  and that s saying something these days   If you love great rocking rockabilly guitar  combined with really cool ballad playing in the power trio format  this CD is just what you re looking for   In fact  I guarantee you ll be knocked out Five stars  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is   this band reminds me of the thrill i first got when i listened to an Atreyu Album  It dies today rip off the former bands style  but they are still a very good band   In the over crowded metalcore market of today  that is a rarity   My only complaint  is that the vocalist has a beautiful singing voice  as heard on   The Radiance   but more often then not goes for the screams and growls that are associated with this type of music  still     material though Favorite songs    The Radiance      Freak Gasoline Fight      Our Disintigration    and  THe Caitliff Choir Defeatism \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Like I said I would  I finally got around to purchasing this CD  I especially like tracks       Cheap Trick is solid as always  I have been a long time fan and enjoy all their releases  If I do have any criticism at all  it is the fact that some songs sound like recycled Trick riffs  I guess it s hard to create new stuff after     years  Anyway  there are some great tunes on this  Buy it like I did  \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Ok good CD  im not suprised  Ok jaheim may not have the b best voice but his music is good and it goes will with the voice that he has   Yes yall this album does use profanity but the songs actually have meanings  I like that daddy thing song  and like a dj  This album looks at issues from a mans  point of view  Ya know we ve heard the angry woman and how the man did t his and that now jaheim looks at it from a males view  Ok yall i know he is not the first one to do this but he did it well  Also this is a break from that       that is played on the radio  \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   Review by Mike Watson With the recent slew of Victory Records releases  not to mention the wide variety  one can be to say the least skeptical of any of their assortment of new releases   However  picking through all of the less notable releases from the label  the up and coming band  The Forecast  have stepped to the plate and scored a home run on Late Night Conversations   The Forecast are a perfect blend of male and female vocals over a very modern rock soundtrack  executed flawlessly over a ten song full length Late Night Conversations is an album confronting different topics ranging from relationships to the teen trend of mutilation and loathing   The harmonies arranged by both vocalists are undeniably catching and well written and the music follows closely in suit leaving your head bobbing back and forth to the beat and your feet tapping almost involuntarily  The Forecast is not only a great recorded band  but bring the same toe tapping energy to the stage  with both great stage presence and cohesion as a band   The Forecast will defiantly be one of the new big bands in poppier rock music  and checking their Victory Records debut out is a must for fans of the genre  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is  great anniversary present  fav song    I love the way you love me    my husband loves to listen to it in the car\n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "index = 0\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Positive review\n",
        "for index in range(len(reviews)):\n",
        "  print(\"Review is \",reviews[index])\n",
        "  print(\"Sentiment \", sentiment_ratings[index])\n",
        "  print(\"Helpfullness is \", helpfulness_ratings[index])\n",
        "  print(\"-----------\")\n",
        "  if(index >5):\n",
        "    break;\n",
        "# helpfulness_ratings[0:5]\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is   I ve always held the philosophy you are what you re entertained by  So  those who like Ludacris are definitely telling on themselves about their views of life and women  And speaking of women   Ludacris never met one who wasn t a h o Ludacris is the epitome of the sexual hypocrisy that plagues men  you demean and look down on women for being h oes  yet you hope every woman is a h o so you ll have no problems getting laid  But then  you re so disgusted with the fact the woman behaves too much like you  so you go and find a   nice girl   because you can respect her and marry her and procreate with her  But then you cheat on her with a   bad girl   because h oes need h oes  but then you can t respect the   bad girl   because she s a slut  so you go back to the   good girl    But she s not nasty enough for you  so then   around and around we go  Women are basically being punished for men not being able to make up their minds about what the heck they want from us He s OBSESSED with sex and h oes  It makes you wonder if he has hang ups about it  Maybe Ludapis saw his mother selling herself in a backalley for a dollar and it scarred his outlook on women forever  I guess if she had actually RAISED him and shown him how to respect women   ALL women  then he probably could have grown up to be a respectable guy instead of the prurient dummy he is What s the music like  Well I ll give him   stars for mentioning TLC in What s Your Fantasy  That s it Addendum          Some may wonder why I m   wasting  my time giving Ludacris negative reviews  Well  first of all  if only people who liked the product were the only ones to review it  everything on Amazon would have   stars  which would be an inaccurate sign of its quality Secondly  I don t like Ludacris  I would give Ludacris more respect if I felt he gave my gender more respect  but he doesn t  My diatribe above shows what my problem with him is  and I feel that argument is valid  There s no way to critique Ludacris  music without critiquing his hypocrisy regarding women  Why should I like him when he seems to have nothing but contempt for us  I m biased against him because he s biased against me  So he gets a bad rating out of s p i t e \n",
            "Sentiment  negative\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   someone get this band a producer and put them in a decent studio  please   the recording and mixing on this disc is flat out atrocious    download a few tracks and hear for yourself  feburary  and     with an anchor  show promise otherwise it s taking back sunday meets hawthorn heights meets fallout boy meets      you get the picture \n",
            "Sentiment  negative\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is  Tihs Album is not all that good when it came out in still not good till this day  I like all his other albums better I don t Know why he decided to release this crap But the only song I really Like is hip hip qurtables\n",
            "Sentiment  negative\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   this industry is   goin down  u call this an album     LO \n",
            "Sentiment  negative\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is  I m sorry but the guy below me doesn t know music if he was slapped with a Gang starr cd  This mess is ridiculous  I wouldn t even want to hear this garbage in the klub let alone banging in my car  They are the modern day version of Young mc    here today gone tomorrow\n",
            "Sentiment  negative\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is   This is not what I expected from such a gifted artist as Jaheim  did he change producers  The music and the man with the voice didn t go together well for me   This is one I will not be purchasing  it is a serious down grade from his previous two albums  Better luck next time  Jaheim I was truly disappointed  \n",
            "Sentiment  negative\n",
            "Helpfullness is  neutral\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "total_printed = 0\n",
        "for index in range(len(reviews)):\n",
        "  if(sentiment_ratings[index] == 'negative'):\n",
        "    print(\"Review is \",reviews[index])\n",
        "    print(\"Sentiment \", sentiment_ratings[index])\n",
        "    print(\"Helpfullness is \", helpfulness_ratings[index])\n",
        "    print(\"-----------\")\n",
        "    total_printed += 1\n",
        "  if(total_printed > 5):\n",
        "    break;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment distribution:\n",
            "positive: 20972 (57.38%)\n",
            "negative: 15576 (42.62%)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentiment_distribution = Counter(sentiment_ratings)\n",
        "total_data = len(sentiment_ratings)\n",
        "\n",
        "print(\"Sentiment distribution:\")\n",
        "for sentiment, count in sentiment_distribution.items():\n",
        "    percentage = (count / total_data) * 100\n",
        "    print(f\"{sentiment}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(sentiment_distribution.keys(), sentiment_distribution.values())\n",
        "plt.xlabel('Sentiment Classificaiton')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution in Full Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Fc8KrpQhYV"
      },
      "source": [
        "**Tokenising Corpus Dataset**\n",
        "<br/>\n",
        "Tokenising the words all the words in the reviews database. This will then be used to create the list of features, which would be the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TjHzxsgNNLH4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenising it by spaces\n",
        "tokenised_set = []\n",
        "for review in reviews:\n",
        "  # Basically, re.split(' ') results in an array of words split by spaces\n",
        "  # Then iterate through that array of words and append it individually to tokenised_set\n",
        "  [tokenised_set.append(tokens) for tokens in re.split(' ', review)]\n",
        "\n",
        "counts = Counter(tokenised_set)\n",
        "so_with_content=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "so_with_content=list(zip(*so_with_content))[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We want to focus on content words\n",
        "function_words = []\n",
        "with open(\"function_words.txt\") as f:\n",
        "    function_words = f.read().splitlines()\n",
        "so: list[str] = []\n",
        "for word in so_with_content:\n",
        "    if word not in function_words and len(word) > 2:\n",
        "        so.append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Creating the sparse embedding**\n",
        "<br/>\n",
        "This is a simple sparse embedding of filtered content words of corpus. This will be used as the main features for all classification model. \n",
        "\n",
        "**Ensuring Reproducibility**\n",
        "<br/>\n",
        "To ensure reproducibility, we will set the random seed to 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random \n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# 8000 Features\n",
        "word_list = so[0:8000]\n",
        "M = np.zeros((len(reviews), len(word_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "  # Ensure we are looking at word\n",
        "  rev = rev.split(' ')\n",
        "  for(j,word) in enumerate(word_list):\n",
        "    if word in rev:\n",
        "      M[i,j]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36548, 8000)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 36548 examples, with 8000 features (words occurence)\n",
        "M.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ints = np.random.choice(len(reviews), int(len(reviews)*0.6), replace=False)\n",
        "test_train_ints = list(set(range(0, len(reviews))) - set(train_ints))\n",
        "test_ints = np.random.choice(test_train_ints, int(len(test_train_ints)*0.5), replace=False)\n",
        "final_test_ints = list(set(test_train_ints) - set(test_ints))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training test 21928\n",
            "Validation test 7310\n",
            "Final test 7310\n",
            "Total 36548\n"
          ]
        }
      ],
      "source": [
        "print(\"Training test\", len(train_ints))\n",
        "print(\"Validation test\", len(test_ints))\n",
        "print(\"Final test\", len(final_test_ints))\n",
        "print(\"Total\", len(train_ints)+len(test_ints)+len(final_test_ints))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-EDhKyxxtMl"
      },
      "source": [
        "**Classifiers for Sentiment Analysis**\n",
        "<br>\n",
        "Multiple classifiers will be run, and tested againts each other for sentiment analysis classifier.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yW_OPROzSFl"
      },
      "source": [
        "Experiment for 8000 features, meaning 8000 words\n",
        "<br>\n",
        "Below we divide the data into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D5zci15pCO7t"
      },
      "outputs": [],
      "source": [
        "# Divide the features by the training indices\n",
        "# Select all rows that are in the indices of the respective lists and select all the rows\n",
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "M_final_test = M[final_test_ints,]\n",
        "sentiment_labels = [sentiment_ratings[i] for i in train_ints]\n",
        "sentiment_labels_test = [sentiment_ratings[i] for i in test_ints]\n",
        "sentiment_labels_final_test = [sentiment_ratings[i] for i in final_test_ints]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'positive': 20972, 'negative': 15576})\n",
            "Counter({'positive': 4190, 'negative': 3120})\n",
            "Counter({'positive': 4228, 'negative': 3082})\n",
            "Counter({'positive': 12554, 'negative': 9374})\n"
          ]
        }
      ],
      "source": [
        "# Class Distribution Test to check\n",
        "class_distribution = Counter(sentiment_ratings)\n",
        "class_distribution_test = Counter(sentiment_labels_test)\n",
        "class_distribution_final_test = Counter(sentiment_labels_final_test)\n",
        "class_distribution_train = Counter(sentiment_labels)\n",
        "\n",
        "print(class_distribution)\n",
        "print(class_distribution_test)\n",
        "print(class_distribution_final_test)\n",
        "print(class_distribution_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUfBV2eCHOeK",
        "outputId": "dad11401-36e1-4801-85f1-7b57602092fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21928, 8000)\n",
            "(7310, 8000)\n",
            "(7310, 8000)\n",
            "21928\n",
            "7310\n",
            "7310\n"
          ]
        }
      ],
      "source": [
        "print(M_train.shape)\n",
        "print(M_test.shape)\n",
        "print(M_final_test.shape)\n",
        "# Sentiment Labels are ordered list\n",
        "print(len(sentiment_labels))\n",
        "print(len(sentiment_labels_test))\n",
        "print(len(sentiment_labels_final_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Function for Classifiers**\n",
        "<br/>\n",
        "This function will be called multiple times to test out different hyperparameters and their effects. \n",
        "<br/>\n",
        "In each training, the function will perform a test on the sentiment_labels_test after finishing training on sentiment_labels_train.\n",
        "<br/>\n",
        "The best performing models will be tested further with the last 20 percent to test whether they generalise well or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run Logistic Regression with Sigmoid\n",
        "def logistic_regresssion_sigmoid(x_dataset,y_dataset,x_test,y_test,num_features,n_iters=1000,lr=0.4,random_seed=42):\n",
        "    np.random.seed(random_seed)\n",
        "    weights = np.random.rand(num_features)\n",
        "    bias=np.random.rand(1)\n",
        "    logistic_loss=[]\n",
        "    num_samples=len(y_dataset)\n",
        "    for i in range(n_iters):\n",
        "        # Basically you are multiplying all the values of M_train with the weights\n",
        "        # It would be similar to this: z= bias + (x[0]*weights[0] + x[1]*weights[1])\n",
        "        # The values here would be 21928, 8000 and 8000, 1, leading to a matrix of 21928, 1\n",
        "        z= x_dataset.dot(weights) + bias\n",
        "        # print(z)\n",
        "        # (1 / (1+np.exp(-z))) we use sigmoid because we only need to know whether it is positive or negative, two possible values\n",
        "        q = (1 / (1+np.exp(-z)))\n",
        "        # print(q)\n",
        "        eps=0.00001\n",
        "        # Binary Cross Entropy Loss\n",
        "        loss = -np.sum((y_dataset*np.log2(q+eps)+(np.ones(len(y_dataset))-y_dataset)*np.log2(np.ones(len(y_dataset))-q+eps)))/num_samples\n",
        "        \n",
        "        \n",
        "        logistic_loss.append(loss)\n",
        "        # print(logistic_loss)\n",
        "        # We then make the prediction, if it is below a certain number, 0.5 it is negative and vice versa\n",
        "        y_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "        # For logistic regression one shot encoder= dw1 = np.dot(x[0],q-y)/num_samples\n",
        "        # dw1 = np.do(x[0], q-y)/num_samples\n",
        "\n",
        "        dw = (q-y_dataset).dot(x_dataset)/num_samples\n",
        "        db = np.sum(q-y_dataset)/num_samples\n",
        "        weights = weights - dw*lr\n",
        "        bias = bias - db*lr\n",
        "\n",
        "    \n",
        "    # Model test on validation dataset \n",
        "    result = LogReg_Sigmoid_Test(weights,bias,x_test,y_test)\n",
        "    return weights,bias, result, logistic_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to invoke the function and test results \n",
        "# For this specific function, we expect the labels to be already in string format\n",
        "def LogReg_Sigmoid_Test(weights,bias,test_dataset,y_test:list):\n",
        "    # Perform Forward Propagation\n",
        "    z= test_dataset.dot(weights) + bias\n",
        "    q = (1 / (1+np.exp(-z)))\n",
        "    x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "    \n",
        "    \n",
        "\n",
        "    # # Accuracy\n",
        "    # y_final_test = [int(l=='positive') for l in sentiment_labels_final_test]\n",
        "    # acc_test = [int(yp == y_final_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "    # print(\"Final Test Accuracy: \", sum(acc_test)/len(acc_test))\n",
        "    # y_test_compare = [\"positive\" if s == 1 else \"negative\" for s in y_test ]\n",
        "    x_labels=[\"positive\" if s == 1 else \"negative\" for s in x_test_pred]\n",
        "    \n",
        "    # TP\n",
        "    true_positives=sum([int(yt == \"positive\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "\n",
        "    true_negatives=sum([int(yt == \"negative\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    false_positives = sum([int(yt == \"negative\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "    # FN: actual is POSITIVE, predicted is NEGATIVE  \n",
        "    false_negatives = sum([int(yt == \"positive\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    \n",
        "    # print(\"True Positives: \", true_positives)\n",
        "    # print(\"False Positives: \", false_positives)\n",
        "    # print(\"False Negatives: \", false_negatives)\n",
        "    # print(\"True Negatives: \", true_negatives)\n",
        "    # Precision\n",
        "    accuracy = (true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
        "    precision = true_positives/(true_positives+false_positives)\n",
        "    recall = true_positives/(true_positives+false_negatives)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(\"--------------\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1: \", f1)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "    return precision, recall, f1, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Different Features forLogistic Regression with Logistic Regression and MLP**\n",
        "<br/>\n",
        "- 8000 features \n",
        "- Learning Rate \n",
        "    - 0.4\n",
        "    - 0.2\n",
        "    - 0.1\n",
        "    - 0.05\n",
        "    <br/>\n",
        "- This will all be done by running multiple classifiers and storing the results in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of hyperparameters to be used for training the model\n",
        "learning_rates = [0.4,0.2,0.1,0.05]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.4\n",
            "--------------\n",
            "Precision:  0.7965981573352232\n",
            "Recall:  0.8047732696897375\n",
            "F1:  0.8006648462543037\n",
            "accuracy:  0.7703146374829002\n",
            "-----\n",
            "Learning rate:  0.2\n",
            "--------------\n",
            "Precision:  0.7530545112781954\n",
            "Recall:  0.7649164677804295\n",
            "F1:  0.7589391427894863\n",
            "accuracy:  0.7214774281805746\n",
            "-----\n",
            "Learning rate:  0.1\n",
            "--------------\n",
            "Precision:  0.7079775555013418\n",
            "Recall:  0.692601431980907\n",
            "F1:  0.70020509108457\n",
            "accuracy:  0.6600547195622435\n",
            "-----\n",
            "Learning rate:  0.05\n",
            "--------------\n",
            "Precision:  0.6487350199733688\n",
            "Recall:  0.5813842482100239\n",
            "F1:  0.6132158590308371\n",
            "accuracy:  0.5796169630642954\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# invoking the function\n",
        "y=[int(l == \"positive\") for l in sentiment_labels]\n",
        "LogReg_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = logistic_regresssion_sigmoid(M_train,y,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    LogReg_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MLP_sigmoid(x_dataset,y_dataset,x_test,y_test,num_features,n_iters=1000,lr=0.4,random_seed=42):\n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    num_features= x_dataset.shape[1]\n",
        "    hidden_size = 32\n",
        "\n",
        "    \n",
        "    \n",
        "    # Weight initialization with Xavier weight initialization technique to encourage ReLU activation\n",
        "    # https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf#page=4.98\n",
        "    # Np.random will not work\n",
        "    limit_1 = np.sqrt(6 / (num_features ))\n",
        "    weights_0_1 = np.random.uniform(-limit_1, limit_1, (num_features, hidden_size))\n",
        "    limit_2 = np.sqrt(6 / (hidden_size ))\n",
        "    weights_1_2 = np.random.uniform(-limit_2, limit_2, (hidden_size,1))\n",
        "\n",
        "\n",
        "    loss_history = []\n",
        "    \n",
        "    N = x_dataset.shape[0] # Number of training samples\n",
        "\n",
        "    for iteration in range(n_iters):\n",
        "\n",
        "        layer_2_error = 0\n",
        "        layer_0 = x_dataset\n",
        "\n",
        "        ## Add forward pass\n",
        "        layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "    \n",
        "\n",
        "\n",
        "        # Then apply sigmoid\n",
        "        layer_2_s = 1/(1+np.exp(-layer_2))\n",
        "\n",
        "    \n",
        "        eps = 1e-8\n",
        "        q = np.clip(layer_2_s, eps, 1 - eps)\n",
        "        # BCE Cross Entropy Loss with clipping\n",
        "        loss = (-np.sum(y_dataset * np.log2(q) + (1 - y_dataset) * np.log2(1 - q)))/N\n",
        "        \n",
        "        loss_history.append(loss)\n",
        "        \n",
        "\n",
        "        ## Add backward pass and update weights\n",
        "        layer_2_diff = (layer_2_s - y_dataset)\n",
        "\n",
        "        z1 = np.dot(layer_0, weights_0_1)\n",
        "        relu_grad = (z1 > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "        hidden_delta = np.dot(layer_2_diff, weights_1_2.T) * relu_grad\n",
        "\n",
        "        # Normalize weight updates by N\n",
        "        weights_1_2 -= lr * (np.dot(layer_1.T, layer_2_diff) / N)\n",
        "        weights_0_1 -= lr * (np.dot(layer_0.T, hidden_delta) / N)\n",
        "\n",
        "    result = test_models_MLP_Sigmoid(weights_0_1,weights_1_2,x_test,y_test)\n",
        "    return weights_0_1,weights_1_2,result, loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to invoke the function and test results \n",
        "# For this specific function, we expect the labels to be already in string format\n",
        "def test_models_MLP_Sigmoid(weights_MLP_1,weights_MLP_2,x_dataset,y_test:list):\n",
        "    # Forward propagation\n",
        "    layer_0_final_test = x_dataset\n",
        "    layer_1_final_test = np.maximum(np.dot(layer_0_final_test, weights_MLP_1), 0)\n",
        "    layer_2_final_test = np.dot(layer_1_final_test, weights_MLP_2)\n",
        "    layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "    #Converting probabilities\n",
        "    x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "\n",
        "    \n",
        "\n",
        "    # # Accuracy\n",
        "    x_labels=[\"positive\" if s == 1 else \"negative\" for s in x_pred]\n",
        "    \n",
        "    # TP\n",
        "    true_positives=np.sum([int(yt == \"positive\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "\n",
        "    true_negatives=np.sum([int(yt == \"negative\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    false_positives = np.sum([int(yt == \"negative\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    # FN: actual is POSITIVE, predicted is NEGATIVE  \n",
        "    false_negatives = np.sum([int(yt == \"positive\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Precision\n",
        "    accuracy = (true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
        "    precision = true_positives/(true_positives+false_positives)\n",
        "    recall = true_positives/(true_positives+false_negatives)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(\"----------\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1: \", f1)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "    return precision, recall, f1, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*******\n",
            "Learning rate:  0.4\n",
            "----------\n",
            "Precision:  0.8727534148094895\n",
            "Recall:  0.8692124105011934\n",
            "F1:  0.8709793136434294\n",
            "accuracy:  0.8523939808481532\n",
            "-----\n",
            "*******\n",
            "Learning rate:  0.2\n",
            "----------\n",
            "Precision:  0.8673902784332232\n",
            "Recall:  0.8773269689737471\n",
            "F1:  0.8723303274798292\n",
            "accuracy:  0.8528043775649795\n",
            "-----\n",
            "*******\n",
            "Learning rate:  0.1\n",
            "----------\n",
            "Precision:  0.860475968268782\n",
            "Recall:  0.8801909307875895\n",
            "F1:  0.8702218027371401\n",
            "accuracy:  0.8495212038303693\n",
            "-----\n",
            "*******\n",
            "Learning rate:  0.05\n",
            "----------\n",
            "Precision:  0.8490433971068595\n",
            "Recall:  0.8684964200477326\n",
            "F1:  0.8586597451628125\n",
            "accuracy:  0.8361149110807113\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).reshape(-1,1)\n",
        "# invoking the function\n",
        "MLP_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"*******\")\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,weights_1_2,result,logistic_loss = MLP_sigmoid(M_train,true_labels,M_test,sentiment_labels_test,lr=lr,num_features=8000,n_iters=1000)\n",
        "    print(\"-----\")\n",
        "    MLP_models_sigmoid[lr] = (weights,weights_1_2,result,logistic_loss)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Further Analysis on Result**\n",
        "<br/>\n",
        "Below we gather the best perfoming models and perform further analysis on the result, comparing the two models, MLP and Logistic Regression. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "import builtins\n",
        "sum = builtins.sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and gathering the best performing models\n",
        "LogReg_models_sigmoid = {}\n",
        "LogReg_models_softmax = {}\n",
        "MLP_models_sigmoid = {}\n",
        "MLP_models_softmax = {}\n",
        "\n",
        "# 0.4 as a learning rate seems to result in best perfomance for MLP and Logistic Regression binary classification\n",
        "learning_rates = [0.4]\n",
        "y=[int(l == \"positive\") for l in sentiment_labels]\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = logistic_regresssion_sigmoid(M_train,y,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    LogReg_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "\n",
        "learning_rates = [0.2]\n",
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).reshape(-1,1)\n",
        "# invoking the function\n",
        "MLP_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"*******\")\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = MLP_sigmoid(M_train,true_labels,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    MLP_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "\n",
        "# learning_rates = [0.4]\n",
        "# LogReg_models_softmax = {}\n",
        "# for lr in learning_rates:\n",
        "#     print(\"Learning rate: \",lr)\n",
        "#     weights,bias,result,loss = LogisticRegression_Softmax(M_train,y_train,M_test,y_test,lr=lr) \n",
        "#     print(\"-----\")\n",
        "#     LogReg_models_softmax[lr] = (weights,bias,result,loss)\n",
        "\n",
        "# learning_rates = [0.05]\n",
        "# MLP_models_softmax = {}\n",
        "# for lr in learning_rates:\n",
        "#     print(\"Learning rate: \",lr)\n",
        "#     weights,bias,result,loss = MLP_softmax(M_train,y_train,M_test,y_test,lr=lr) \n",
        "#     print(\"-----\")\n",
        "#     MLP_models_softmax[lr] = (weights,bias,result,loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Methods to perform forward propagation to conduct further error slice analysis \n",
        "def LogReg_ForwardPass(weights, bias, x_dataset, Sigmoid:bool):\n",
        "    if(Sigmoid):\n",
        "        z= x_dataset.dot(weights) + bias\n",
        "        q = (1 / (1+np.exp(-z)))\n",
        "        x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "        return x_test_pred, q\n",
        "    else:\n",
        "       z_test = x_dataset.dot(weights) + bias\n",
        "       # Perform theSoftmax\n",
        "       exp_z_test = np.exp(z_test)\n",
        "       q_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "       # Get predictions, basically get the class with highest probability\n",
        "       y_test_pred = np.argmax(q_test, axis=1)  # Shape: (7310,)\n",
        "       return y_test_pred,q_test\n",
        "\n",
        "def MLP_ForwardPass(weights_0_1,weights_1_2,x_dataset, Sigmoid:bool):\n",
        "    if(Sigmoid):\n",
        "        layer_0_final_test = x_dataset\n",
        "        layer_1_final_test = np.maximum(np.dot(layer_0_final_test, weights_0_1), 0)\n",
        "        layer_2_final_test = np.dot(layer_1_final_test, weights_1_2)\n",
        "        layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "        #Converting probabilities\n",
        "        x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "        return x_pred, layer_2_s_final_test\n",
        "    else:\n",
        "        layer_0_test = x_dataset    \n",
        "        layer_1_test = np.maximum(np.dot(layer_0_test, weights_0_1), 0)  # ReLU activation\n",
        "        layer_2_test = np.dot(layer_1_test, weights_1_2)\n",
        "\n",
        "        # Apply softmax\n",
        "        exp_z_test = np.exp(layer_2_test)\n",
        "        layer_2_s_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "\n",
        "        # Get predictions (class with highest probability)\n",
        "        y_pred_test = np.argmax(layer_2_s_test, axis=1)\n",
        "        return y_pred_test, layer_2_s_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conducting Error Analysis**\n",
        "<br/>\n",
        "- What information do these models encode to help classify a text as negative or positive ? \n",
        "<br/>\n",
        "- In the end what makes one better than the other ?  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP wins : 1049 (14.35%)\n",
            "LogReg wins : 449 (6.14%)\n",
            "Both correct: 5175 (70.79%)\n",
            "Both wrong: 637 (8.71%)\n",
            "Total examples: 7310\n",
            "0\n",
            "32799\n",
            "Review:  I bought this sippy cup seeing how informative the review was   and I was happy when i saw it and my baby liked it  But whn i went to wash it  i cleaned the valve  by removin a obvious piece that had to be removed to clean    and it never was able to attach corrctly it will fall in her drink and completely get my babys face all wet so after ONE use i had to discard it    maybe it was defected but i was not satisfied with it \n",
            "True label: negative\n",
            "MLP prediction: positive\n",
            "LogReg prediction: negative\n",
            "MLP confidence: 0.885451310832483\n",
            "LogReg confidence: 0.35000750281019327\n",
            "-------\n",
            "1\n",
            "32839\n",
            "Review:  I am     and this jump rope was too short for me  it says that its adujustable  but it can only be made shorter and not longer   For a kid this is a good jump rope  but its definitely not for a tall person \n",
            "True label: negative\n",
            "MLP prediction: positive\n",
            "LogReg prediction: negative\n",
            "MLP confidence: 0.513771040198224\n",
            "LogReg confidence: 0.23894995599631352\n",
            "-------\n",
            "2\n",
            "73\n",
            "Review: I was waiting in anticipation for this cd to arrive and was not disapointed   Etta is the bes\n",
            "True label: positive\n",
            "MLP prediction: negative\n",
            "LogReg prediction: positive\n",
            "MLP confidence: 0.11549661587946393\n",
            "LogReg confidence: 0.5169243934342359\n",
            "-------\n",
            "3\n",
            "105\n",
            "Review: I have been a Barry Manilow fan since I was a teenager   I have NEVER been disappointed with any of his work and this CD is no exception   He is such a brilliant arranger and the remix of these classics is fabulous   His voice is as good as ever   Even my   year old begs to listen to the CD everytime we get in the car and she can sing all the words \n",
            "True label: positive\n",
            "MLP prediction: negative\n",
            "LogReg prediction: positive\n",
            "MLP confidence: 0.22383219405705326\n",
            "LogReg confidence: 0.5197442601526958\n",
            "-------\n",
            "4\n",
            "32885\n",
            "Review: I am a HUGE ski person and I got the jump it is so small for me because I am do like    feet drops all the time ___________________MY SUGGESTION___________________This is good for you starting your kid out on there first scooter they got for the holidays or skate board what ever you get him and its good for those things  Not Ideal what so ever for skiing  I got it to practice low ground tricks like a     at least so go for it if you are doing it for your son or daughter with there first scooter or bike or skateboard NOT skiing\n",
            "True label: negative\n",
            "MLP prediction: positive\n",
            "LogReg prediction: negative\n",
            "MLP confidence: 0.7630353204608136\n",
            "LogReg confidence: 0.4142762325931471\n",
            "-------\n"
          ]
        }
      ],
      "source": [
        "# Error slice analysis\n",
        "# What can MLP capture that Logistic Regression cannot and vice versa ? \n",
        "lr = 0.4\n",
        "# Get predictions from both models\n",
        "# Sigmoids\n",
        "logregSigmoid_preds, logregSigmoid_probs = LogReg_ForwardPass(LogReg_models_sigmoid[lr][0], LogReg_models_sigmoid[lr][1], M_final_test, Sigmoid=True)\n",
        "lr = 0.2\n",
        "mlpSigmoid_preds, mlpSigmoid_probs = MLP_ForwardPass(MLP_models_sigmoid[lr][0], MLP_models_sigmoid[lr][1], M_final_test, Sigmoid=True)\n",
        "\n",
        "\n",
        "# Get true labels for sentiment analysis\n",
        "true_labels = np.array([int(l == \"positive\") for l in sentiment_labels_final_test])\n",
        "\n",
        "# # Create error slices\n",
        "mlpSigmoid_correct = (mlpSigmoid_preds.flatten() == true_labels)\n",
        "logregSigmoid_correct = (np.array(logregSigmoid_preds).flatten() == true_labels)\n",
        "\n",
        "# print(mlpWins)\n",
        "# Create error slices \n",
        "mlp_wins_sigmoid = [mlpSigmoid_correct[i] and not logregSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "logreg_wins_sigmoid = [logregSigmoid_correct[i] and not mlpSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "both_correct_sigmoid = [mlpSigmoid_correct[i] and logregSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "both_wrong_sigmoid = [not mlpSigmoid_correct[i] and not logregSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "\n",
        "# Get indices where MLP wins\n",
        "mlp_wins_indices = []\n",
        "for index,value in enumerate(mlp_wins_sigmoid):\n",
        "    if (value == True):\n",
        "        mlp_wins_indices.append(index)\n",
        "# mlp_wins_indices = [i for i in range(len(mlp_wins_sigmoid)) if mlp_wins_sigmoid[i]]\n",
        "\n",
        "# Get indices where LogReg wins\n",
        "logreg_wins_indices = []\n",
        "for index,value in enumerate(logreg_wins_sigmoid):\n",
        "    if (value == True):\n",
        "        logreg_wins_indices.append(index)\n",
        "\n",
        "# Get indices where both correct\n",
        "both_correct_indices = []\n",
        "for index,value in enumerate(both_correct_sigmoid):\n",
        "    if (value == True):\n",
        "        both_correct_indices.append(index)\n",
        "\n",
        "# Get indices where both wrong\n",
        "both_wrong_indices = []\n",
        "for index,value in enumerate(both_wrong_sigmoid):\n",
        "    if (value == True):\n",
        "        both_wrong_indices.append(index)\n",
        "\n",
        "# print(sum([x == 'True' for x in mlp_wins_indices]))\n",
        "\n",
        "# Statistics of comparison between the. two models \n",
        "print(f\"MLP wins : {np.sum(mlp_wins_sigmoid)} ({np.sum(mlp_wins_sigmoid)/len(mlp_wins_sigmoid)*100:.2f}%)\")\n",
        "print(f\"LogReg wins : {np.sum(logreg_wins_sigmoid)} ({np.sum(logreg_wins_sigmoid)/len(logreg_wins_sigmoid)*100:.2f}%)\")\n",
        "print(f\"Both correct: {np.sum(both_correct_sigmoid)} ({np.sum(both_correct_sigmoid)/len(both_correct_sigmoid)*100:.2f}%)\")\n",
        "print(f\"Both wrong: {np.sum(both_wrong_sigmoid)} ({np.sum(both_wrong_sigmoid)/len(both_wrong_sigmoid)*100:.2f}%)\")\n",
        "print(f\"Total examples: {len(mlp_wins_sigmoid)}\")\n",
        "\n",
        "\n",
        "# This is where \n",
        "for index,idx in enumerate(logreg_wins_indices[:5]):\n",
        "    print(index)\n",
        "    # print(test_ints[idx])\n",
        "    test_example_idx = final_test_ints[idx]  # Map to original review index\n",
        "    print(test_example_idx)\n",
        "    print(\"Review:\", reviews[test_example_idx])\n",
        "    print(\"True label:\", sentiment_labels_final_test[idx])\n",
        "    print(\"MLP prediction:\", 'positive' if mlpSigmoid_preds.flatten()[idx] == 1 else 'negative')\n",
        "    print(\"LogReg prediction:\", 'positive' if logregSigmoid_preds[idx] == 1 else 'negative')\n",
        "    print(\"MLP confidence:\", mlpSigmoid_probs.flatten()[idx])\n",
        "    print(\"LogReg confidence:\", logregSigmoid_probs[idx])\n",
        "    print(\"-------\")\n",
        "# Print statistics for error slices\n",
        "\n",
        "\n",
        "# Print statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before conducting forward propagation analysis, it is worthwile to look at how MLP and Logistic Regression encode different, or assign weights differently. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LogReg Information Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top words contributing to negative sentiment\n",
            "Word: disappointed, Weight: -1.73\n",
            "Word: bad, Weight: -1.71\n",
            "Word: NOT, Weight: -1.43\n",
            "Word: return, Weight: -1.42\n",
            "Word: poor, Weight: -1.29\n",
            "Word: AND, Weight: -1.26\n",
            "Word: waste, Weight: -1.26\n",
            "Word: money, Weight: -1.21\n",
            "Word: When, Weight: -1.20\n",
            "Word: Unfortunately, Weight: -1.16\n",
            "Word: point, Weight: -1.13\n",
            "Word: After, Weight: -1.13\n",
            "Word: were, Weight: -1.10\n",
            "Word: Not, Weight: -1.07\n",
            "Word: same, Weight: -1.03\n",
            "Word: support, Weight: -1.03\n",
            "Word: worse, Weight: -1.01\n",
            "Word: There, Weight: -1.00\n",
            "Word: trying, Weight: -1.00\n",
            "Word: However, Weight: -0.99\n",
            "--------\n",
            "Top words contributing to positive sentiment\n",
            "Word: regrets, Weight: 1.05\n",
            "Word: price, Weight: 1.06\n",
            "Word: rocks, Weight: 1.07\n",
            "Word: snack, Weight: 1.11\n",
            "Word: awesome, Weight: 1.15\n",
            "Word: Highly, Weight: 1.15\n",
            "Word: highly, Weight: 1.16\n",
            "Word: sturdy, Weight: 1.22\n",
            "Word: excellent, Weight: 1.22\n",
            "Word: Great, Weight: 1.24\n",
            "Word: Excellent, Weight: 1.25\n",
            "Word: amazing, Weight: 1.29\n",
            "Word: happy, Weight: 1.30\n",
            "Word: loves, Weight: 1.30\n",
            "Word: perfect, Weight: 1.39\n",
            "Word: great, Weight: 1.43\n",
            "Word: best, Weight: 1.51\n",
            "Word: pleased, Weight: 1.60\n",
            "Word: easy, Weight: 1.60\n",
            "Word: love, Weight: 1.67\n"
          ]
        }
      ],
      "source": [
        "# Get the highest weights amont LogReg Models Sigmoid \n",
        "# Simply assign weights to each word, and those words can contribute or not it depends. \n",
        "indices = [i for i in (LogReg_models_sigmoid[0.4][0])]\n",
        "word_weight = []\n",
        "for i,value in enumerate(LogReg_models_sigmoid[0.4][0]):\n",
        "    word_weight.append({\n",
        "            \"word\": word_list[i], \n",
        "            \"weight\": value\n",
        "        })\n",
        "print(\"Top words contributing to negative sentiment\")\n",
        "word_weight = sorted(word_weight, key=lambda x: x['weight'], reverse=False)\n",
        "for i in word_weight[:20]:\n",
        "    print(f\"Word: {i['word']}, Weight: {i['weight']:.2f}\")\n",
        "print(\"--------\")\n",
        "print(\"Top words contributing to positive sentiment\")\n",
        "for i in word_weight[-20:]:\n",
        "    print(f\"Word: {i['word']}, Weight: {i['weight']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MLP Neuron ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the highest weights amont MLP SoftMax Models Sigmoid \n",
        "# Different neurons focus on different thigns, and their importance is different accordingly. \n",
        "info_neurons_unsorted = {}\n",
        "second_layer_reverse = MLP_models_sigmoid[0.4][1].T\n",
        "for i in range(32):\n",
        "    info_neurons_unsorted[i+1] = {\n",
        "        \"words\": [word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, i])[::-1][0:15]], \"weights\": second_layer_reverse[:, i],\n",
        "        \"second_layer\": second_layer_reverse[:, i]\n",
        "    }\n",
        "    # print(\"layer\", i+1)\n",
        "    # print([word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, i])[::-1][0:15]])\n",
        "    # print(second_layer_reverse[:, i])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 7\n",
            "Words ['waste', 'boring', 'terrible', 'worst', 'horrible', 'disappointed', 'return']\n",
            "Second Layer Weights -1.88\n",
            "Layer 9\n",
            "Words ['waste', 'boring', 'terrible', 'returning', 'disappointment', 'horrible', 'worst']\n",
            "Second Layer Weights -1.45\n",
            "Layer 4\n",
            "Words ['boring', 'worst', 'terrible', 'useless', 'horrible', 'poor', 'awful']\n",
            "Second Layer Weights -1.40\n",
            "Layer 15\n",
            "Words ['boring', 'terrible', 'worst', 'waste', 'returning', 'return', 'horrible']\n",
            "Second Layer Weights -1.34\n",
            "Layer 26\n",
            "Words ['boring', 'terrible', 'waste', 'worst', 'awful', 'disappointment', 'trash']\n",
            "Second Layer Weights -1.18\n"
          ]
        }
      ],
      "source": [
        "# In order from negative to positive \n",
        "info_neurons = sorted(info_neurons_unsorted.items(), key=lambda x: x[1][\"second_layer\"], reverse=False)\n",
        "\n",
        "for i in range(5):\n",
        "    print('Layer', info_neurons[i][0])\n",
        "    print('Words',info_neurons[i][1]['words'][0:7])\n",
        "    print(f\"Second Layer Weights {info_neurons[i][1]['second_layer'][0]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 31\n",
            "Words ['Excellent', 'Highly', 'awesome', 'delicious', 'Great', 'pleased', 'amazing']\n",
            "Second Layer Weights 1.85\n",
            "Layer 10\n",
            "Words ['Excellent', 'Great', 'Highly', 'awesome', 'delicious', 'pleased', 'excellent']\n",
            "Second Layer Weights 1.78\n",
            "Layer 13\n",
            "Words ['Excellent', 'Great', 'pleased', 'Highly', 'amazing', 'love', 'awesome']\n",
            "Second Layer Weights 1.66\n",
            "Layer 17\n",
            "Words ['Excellent', 'Highly', 'excellent', 'Great', 'GREAT', 'delicious', 'great']\n",
            "Second Layer Weights 1.65\n",
            "Layer 19\n",
            "Words ['Excellent', 'Highly', 'awesome', 'Great', 'love', 'job', 'delicious']\n",
            "Second Layer Weights 1.57\n"
          ]
        }
      ],
      "source": [
        "# In order from negative to positive \n",
        "info_neurons = sorted(info_neurons_unsorted.items(), key=lambda x: x[1][\"second_layer\"], reverse=True)\n",
        "\n",
        "for i in range(5):\n",
        "    print('Layer', info_neurons[i][0])\n",
        "    print('Words',info_neurons[i][1]['words'][0:7])\n",
        "    print(f\"Second Layer Weights {info_neurons[i][1]['second_layer'][0]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 32\n",
            "Words ['boring', 'returning', 'horrible', 'ridiculous', 'waste']\n",
            "Second Layer Weights -0.89\n",
            "Layer 31\n",
            "Words ['Excellent', 'Highly', 'awesome', 'delicious', 'Great']\n",
            "Second Layer Weights 1.85\n",
            "Layer 30\n",
            "Words ['love', 'good', 'long', 'perfect', 'easy']\n",
            "Second Layer Weights 0.66\n",
            "Layer 29\n",
            "Words ['waste', 'worst', 'boring', 'hopes', 'returning']\n",
            "Second Layer Weights -0.91\n",
            "Layer 28\n",
            "Words ['pay', 'get', 'waste', 'junk', 'games']\n",
            "Second Layer Weights -0.14\n",
            "Layer 27\n",
            "Words ['Excellent', 'love', 'Great', 'pleased', 'perfect']\n",
            "Second Layer Weights 1.16\n",
            "Layer 26\n",
            "Words ['boring', 'terrible', 'waste', 'worst', 'awful']\n",
            "Second Layer Weights -1.18\n",
            "Layer 25\n",
            "Words ['years', 'This', 'was', 'amazing', 'very']\n",
            "Second Layer Weights 0.14\n",
            "Layer 24\n",
            "Words ['returning', 'waste', 'useless', 'hopes', 'return']\n",
            "Second Layer Weights -1.05\n",
            "Layer 23\n",
            "Words ['terrible', 'The', 'useless', 'waste', 'mediocre']\n",
            "Second Layer Weights -0.53\n",
            "Layer 22\n",
            "Words ['got', 'money', 'twist', 'Mine', 'sequel']\n",
            "Second Layer Weights -0.02\n",
            "Layer 21\n",
            "Words ['excellent', 'delicious', 'hooked', 'Highly', 'Great']\n",
            "Second Layer Weights 1.05\n",
            "Layer 20\n",
            "Words ['going', 'purchased', 'always', 'whole', 'wanted']\n",
            "Second Layer Weights 0.29\n",
            "Layer 19\n",
            "Words ['Excellent', 'Highly', 'awesome', 'Great', 'love']\n",
            "Second Layer Weights 1.57\n",
            "Layer 18\n",
            "Words ['This', 'Excellent', 'favorite', 'very', 'nice']\n",
            "Second Layer Weights 0.79\n",
            "Layer 17\n",
            "Words ['Excellent', 'Highly', 'excellent', 'Great', 'GREAT']\n",
            "Second Layer Weights 1.65\n",
            "Layer 16\n",
            "Words ['great', 'best', 'very', 'satisfied', 'for']\n",
            "Second Layer Weights 0.76\n",
            "Layer 15\n",
            "Words ['boring', 'terrible', 'worst', 'waste', 'returning']\n",
            "Second Layer Weights -1.34\n",
            "Layer 14\n",
            "Words ['Great', 'Excellent', 'pleased', 'delicious', 'Highly']\n",
            "Second Layer Weights 1.04\n",
            "Layer 13\n",
            "Words ['Excellent', 'Great', 'pleased', 'Highly', 'amazing']\n",
            "Second Layer Weights 1.66\n",
            "Layer 12\n",
            "Words ['perfect', 'holds', 'come', 'having', 'party']\n",
            "Second Layer Weights 0.07\n",
            "Layer 11\n",
            "Words ['bad', 'got', 'sell', 'worse', 'read']\n",
            "Second Layer Weights -0.26\n",
            "Layer 10\n",
            "Words ['Excellent', 'Great', 'Highly', 'awesome', 'delicious']\n",
            "Second Layer Weights 1.78\n",
            "Layer 9\n",
            "Words ['waste', 'boring', 'terrible', 'returning', 'disappointment']\n",
            "Second Layer Weights -1.45\n",
            "Layer 8\n",
            "Words ['found', 'didn', 'call', 'annoying', 'wear']\n",
            "Second Layer Weights -0.11\n",
            "Layer 7\n",
            "Words ['waste', 'boring', 'terrible', 'worst', 'horrible']\n",
            "Second Layer Weights -1.88\n",
            "Layer 6\n",
            "Words ['great', 'Great', 'job', 'perfect', 'are']\n",
            "Second Layer Weights 0.74\n",
            "Layer 5\n",
            "Words ['terrible', 'boring', 'waste', 'useless', 'worst']\n",
            "Second Layer Weights -1.13\n",
            "Layer 4\n",
            "Words ['boring', 'worst', 'terrible', 'useless', 'horrible']\n",
            "Second Layer Weights -1.40\n",
            "Layer 3\n",
            "Words ['with', 'This', 'Easy', 'easy', 'instructions']\n",
            "Second Layer Weights 0.15\n",
            "Layer 2\n",
            "Words ['Excellent', 'perfect', 'amazing', 'love', 'delicious']\n",
            "Second Layer Weights 1.39\n",
            "Layer 1\n",
            "Words ['returning', 'useless', 'for', 'very', 'worse']\n",
            "Second Layer Weights -0.69\n"
          ]
        }
      ],
      "source": [
        "# Complete list of neurons and the words they are associated with\n",
        "info_neurons = sorted(info_neurons_unsorted.items(), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "for i in range(32):\n",
        "    print('Layer', info_neurons[i][0])\n",
        "    print('Words',info_neurons[i][1]['words'][0:5])\n",
        "    print(f\"Second Layer Weights {info_neurons[i][1]['second_layer'][0]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MLP Analysis Wrong Logistic Regression Correct**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Forward Pass ----------------\n",
            "125\n",
            "Text,   Please do yourself a favor and buy this CD  Whatever you do  dont listen to it in your car though  No kidding aside  I almost ran my car off the road because I was laughing so hard \n",
            "Actual Label:  positive\n",
            "Prediction:  negative\n",
            "X_pred (Before Sigmoid):  [[-3.34942171]]\n",
            "X_pred (After Sigmoid):  [[0.03391411]]\n",
            "Neuron Analysis--------------------\n",
            "POSITIVE NEURON ANALYSIS--------\n",
            "Text,   Please do yourself a favor and buy this CD  Whatever you do  dont listen to it in your car though  No kidding aside  I almost ran my car off the road because I was laughing so hard \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['love', 'good', 'long', 'perfect', 'easy']\n",
            "Top words in sentence ['use', 'aside', 'Whatever', 'kid', 'car']\n",
            "Neuron:  30\n",
            "Output Weight:0.07\n",
            "Contribution:0.04\n",
            "---------\n",
            "Top words for the neuron ['This', 'Excellent', 'favorite', 'very', 'nice']\n",
            "Top words in sentence ['What', 'listen', 'car', 'ran', 'ours']\n",
            "Neuron:  18\n",
            "Output Weight:0.04\n",
            "Contribution:0.03\n",
            "---------\n",
            "Top words for the neuron ['years', 'This', 'was', 'amazing', 'very']\n",
            "Top words in sentence ['was', 'kidding', 'aside', 'laugh', 'buy']\n",
            "Neuron:  25\n",
            "Output Weight:0.14\n",
            "Contribution:0.02\n",
            "---------\n",
            "Top words for the neuron ['perfect', 'holds', 'come', 'having', 'party']\n",
            "Top words in sentence ['kid', 'ate', 'hat', 'kidding', 'aside']\n",
            "Neuron:  12\n",
            "Output Weight:0.05\n",
            "Contribution:0.00\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'perfect', 'amazing', 'love', 'delicious']\n",
            "Top words in sentence ['use', 'ever', 'listen', 'almost', 'Whatever']\n",
            "Neuron:  2\n",
            "Output Weight:0.00\n",
            "Contribution:0.00\n",
            "---------\n",
            "Top words for the neuron ['with', 'This', 'Easy', 'easy', 'instructions']\n",
            "Top words in sentence ['What', 'ran', 'kidding', 'dont', 'kid']\n",
            "Neuron:  3\n",
            "Output Weight:0.00\n",
            "Contribution:0.00\n",
            "---------\n",
            "NEGATIVE NEURON ANALYSIS -------------\n",
            "Text,   Please do yourself a favor and buy this CD  Whatever you do  dont listen to it in your car though  No kidding aside  I almost ran my car off the road because I was laughing so hard \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'worst', 'horrible']\n",
            "Top words in sentence ['favor', 'buy', 'cause', 'use', 'was']\n",
            "Neuron:  7\n",
            "Output Weight:0.44\n",
            "Contribution:-0.82\n",
            "---------\n",
            "Top words for the neuron ['boring', 'worst', 'terrible', 'useless', 'horrible']\n",
            "Top words in sentence ['buy', 'favor', 'Please', 'was', 'self']\n",
            "Neuron:  4\n",
            "Output Weight:0.44\n",
            "Contribution:-0.61\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'worst', 'waste', 'returning']\n",
            "Top words in sentence ['buy', 'Please', 'favor', 'was', 'ease']\n",
            "Neuron:  15\n",
            "Output Weight:0.34\n",
            "Contribution:-0.46\n",
            "---------\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'returning', 'disappointment']\n",
            "Top words in sentence ['favor', 'buy', 'hate', 'dont', 'Please']\n",
            "Neuron:  9\n",
            "Output Weight:0.31\n",
            "Contribution:-0.44\n",
            "---------\n",
            "Top words for the neuron ['terrible', 'boring', 'waste', 'useless', 'worst']\n",
            "Top words in sentence ['favor', 'Please', 'cause', 'dont', 'use']\n",
            "Neuron:  5\n",
            "Output Weight:0.25\n",
            "Contribution:-0.28\n",
            "---------\n",
            "Top words for the neuron ['returning', 'waste', 'useless', 'hopes', 'return']\n",
            "Top words in sentence ['almost', 'cause', 'was', 'favor', 'list']\n",
            "Neuron:  24\n",
            "Output Weight:0.19\n",
            "Contribution:-0.20\n",
            "---------\n",
            "-3.349421709464153\n",
            "0.03391410612931308\n"
          ]
        }
      ],
      "source": [
        "# We define several functions taht would help us decompose the classificaiton result \n",
        "# Performing MLP \n",
        "def perform_MLP_analysis(idx):\n",
        "    # Activation for certain examples \n",
        "    # Perform forward pass\n",
        "    print(\"Performing Forward Pass ----------------\")\n",
        "    # idx = logreg_wins_indices[0]\n",
        "    print(idx)\n",
        "    test_example_idx = final_test_ints[idx] \n",
        "    # Map to original review index and put it into all reviews test, but we stil use M_test, as we have set that previously \n",
        "    print(\"Text, \", reviews[test_example_idx])\n",
        "    print(\"Actual Label: \", sentiment_ratings[test_example_idx])\n",
        "    # This specifically helps it to have 2d array \n",
        "    layer_0_final_test = M_final_test[idx:idx+1]\n",
        "    layer_1_final_test = np.maximum((np.dot(layer_0_final_test, MLP_models_sigmoid[0.4][0])), 0)\n",
        "    layer_2_final_test = np.dot(layer_1_final_test, MLP_models_sigmoid[0.4][1])\n",
        "    layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "    #Converting probabilities\n",
        "    x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "    prediction = \"positive\" if x_pred[0][0] == 1 else \"negative\"\n",
        "    print(\"Prediction: \", prediction)\n",
        "    print('X_pred (Before Sigmoid): ', layer_2_final_test)\n",
        "    print('X_pred (After Sigmoid): ', layer_2_s_final_test)\n",
        "\n",
        "    print(\"Neuron Analysis--------------------\")\n",
        "    layer = []\n",
        "    second_layer_reverse = MLP_models_sigmoid[0.4][1].T.flatten()\n",
        "    for i,weight in enumerate(layer_1_final_test[0]):\n",
        "        layer.append({\n",
        "            'neuron': i+1,\n",
        "            'output_weight': weight,\n",
        "            'contribution': weight * second_layer_reverse[i]\n",
        "        })\n",
        "        \n",
        "    print(\"POSITIVE NEURON ANALYSIS--------\")\n",
        "    sorted_layers = sorted(layer, key=lambda x: x['contribution'], reverse=True)\n",
        "    # With one hot encoding, we have a list of words activated\n",
        "    print(\"Text, \", reviews[test_example_idx])\n",
        "    print(\"Neuron contributions (sorted by impact):\")\n",
        "    for index, info in enumerate(sorted_layers):\n",
        "        if(index>5):\n",
        "            break\n",
        "        # Top word for that specific neuron \n",
        "        words = [word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, info['neuron']-1])[::-1]]\n",
        "        print(\"Top words for the neuron\", words[0:5])\n",
        "        # Also retrieve the weights of the words for that specific neuron\n",
        "        top_words_in_sentence = [word for word in words if word in reviews[test_example_idx]]\n",
        "        print(\"Top words in sentence\",top_words_in_sentence[0:5])\n",
        "        print('Neuron: ',info['neuron'])\n",
        "\n",
        "        print( f'Output Weight:{info[\"output_weight\"]:.2f}')\n",
        "        print(f'Contribution:{info[\"contribution\"]:.2f}')\n",
        "        print(\"---------\")\n",
        "    \n",
        "    print(\"NEGATIVE NEURON ANALYSIS -------------\")\n",
        "    sorted_layers = sorted(layer, key=lambda x: x['contribution'], reverse=False)\n",
        "    # With one hot encoding, we have a list of words activated\n",
        "    print(\"Text, \", reviews[test_example_idx])\n",
        "    print(\"Neuron contributions (sorted by impact):\")\n",
        "    for index, info in enumerate(sorted_layers):\n",
        "        if(index>5):\n",
        "            break\n",
        "        # Top word for that specific neuron \n",
        "        words = [word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, info['neuron']-1])[::-1]]\n",
        "        print(\"Top words for the neuron\", words[0:5])\n",
        "        # Also retrieve the weights of the words for that specific neuron\n",
        "        top_words_in_sentence = [word for word in words if word in reviews[test_example_idx]]\n",
        "        print(\"Top words in sentence\",top_words_in_sentence[0:5])\n",
        "        print('Neuron: ',info['neuron'])\n",
        "\n",
        "        print(f'Output Weight:{info[\"output_weight\"]:.2f}')\n",
        "        print(f'Contribution:{info[\"contribution\"]:.2f}')\n",
        "        print(\"---------\")\n",
        "\n",
        "    # To see whether it has the same score as probability gathered in the previous forward pass\n",
        "    # An it actually is, suggesting our method correctly analyses the different componenet of the output\n",
        "    sum = 0\n",
        "    for items in sorted_layers:\n",
        "        sum += items['contribution']\n",
        "    print(sum)\n",
        "    # Run through sigmoid function \n",
        "    sum = 1 / (1 + np.exp(-sum))\n",
        "    print(sum)\n",
        "    \n",
        "perform_MLP_analysis(logreg_wins_indices[10])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perform forward pass for Logistic Regression --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Review Text  I am     and this jump rope was too short for me  it says that its adujustable  but it can only be made shorter and not longer   For a kid this is a good jump rope  but its definitely not for a tall person \n",
            "Actual Prediction negative\n",
            "negative\n",
            "Prediction Numbers\n",
            "[-1.15844498]\n",
            "Performing weight analysis --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence  I am     and this jump rope was too short for me  it says that its adujustable  but it can only be made shorter and not longer   For a kid this is a good jump rope  but its definitely not for a tall person \n",
            "Word Contributing: too\n",
            "Weight:-0.66\n",
            "Word Contributing: was\n",
            "Weight:-0.60\n",
            "Word Contributing: made\n",
            "Weight:-0.60\n",
            "Word Contributing: but\n",
            "Weight:-0.52\n",
            "Word Contributing: For\n",
            "Weight:-0.51\n",
            "Word Contributing: only\n",
            "Weight:-0.29\n",
            "Word Contributing: for\n",
            "Weight:-0.17\n",
            "Word Contributing: person\n",
            "Weight:-0.10\n",
            "Word Contributing: kid\n",
            "Weight:-0.05\n",
            "Word Contributing: short\n",
            "Weight:-0.04\n",
            "Word Contributing: says\n",
            "Weight:-0.03\n",
            "Word Contributing: shorter\n",
            "Weight:0.01\n",
            "Word Contributing: tall\n",
            "Weight:0.15\n",
            "Word Contributing: longer\n",
            "Weight:0.17\n",
            "Word Contributing: jump\n",
            "Weight:0.41\n",
            "Word Contributing: good\n",
            "Weight:0.48\n",
            "Word Contributing: definitely\n",
            "Weight:0.50\n",
            "Word Contributing: rope\n",
            "Weight:0.67\n",
            "Sum of weights: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sum of weights from decomposition: [-1.15844498]\n"
          ]
        }
      ],
      "source": [
        "def perform_LogReg_analysis(idx):\n",
        "    # LogReg forward pass \n",
        "    print(\"Perform forward pass for Logistic Regression\",'----'*50)\n",
        "    bias = LogReg_models_sigmoid[0.4][1]\n",
        "    test_example_idx = final_test_ints[idx]\n",
        "    z= M_final_test[idx:idx+1].dot(LogReg_models_sigmoid[0.4][0]) + LogReg_models_sigmoid[0.4][1]\n",
        "    q = (1 / (1+np.exp(-z)))\n",
        "    x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "    prediction = \"positive\" if x_test_pred[0] == 1 else \"negative\"\n",
        "    print(\"Review Text\",reviews[test_example_idx])\n",
        "    print(\"Actual Prediction\", sentiment_ratings[test_example_idx])\n",
        "    print(prediction)\n",
        "    print(\"Prediction Numbers\")\n",
        "    print(z)\n",
        "    print(\"Performing weight analysis\",'----'*50)\n",
        "    # Perform weight analysis for the specific example \n",
        "    # Get the feature vector for this example\n",
        "    feature_vector = M_final_test[idx]  # Shape: (8000,) - contains 0s and 1s\n",
        "    # Analyse the activations for this sentence \n",
        "    activated_indices = [index for index,value in enumerate(feature_vector) if value == 1]\n",
        "    # Dictionary to attach words and it's corresponding weight \n",
        "    word_weight = []\n",
        "    for index_value in activated_indices:\n",
        "        word_weight.append({\n",
        "            'word': word_list[index_value],\n",
        "            'weight' : LogReg_models_sigmoid[0.4][0][index_value]\n",
        "        })\n",
        "    word_activation_sorted = sorted(word_weight, key=lambda x: x['weight'], reverse=False)\n",
        "    # # The sentence \n",
        "    print(\"Sentence\", reviews[test_example_idx])\n",
        "\n",
        "\n",
        "    for item in word_activation_sorted:\n",
        "        print(\"Word Contributing:\",item['word'])\n",
        "        print(f\"Weight:{item['weight']:.2f}\")\n",
        "    print(\"Sum of weights:\", \"----\"*40)\n",
        "    sum_total = 0\n",
        "    for word in word_activation_sorted:\n",
        "        sum_total += word['weight']\n",
        "    print(\"Sum of weights from decomposition:\",sum_total+bias)\n",
        "\n",
        "perform_LogReg_analysis(logreg_wins_indices[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MLP Analysis Correct Logistic Regression Wrong**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_analyse = mlp_wins_indices[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Forward Pass ----------------\n",
            "64\n",
            "Text,   I have to admit that  at first listening  I found some aspects of Ms Belle s interpretations of these pieces disorienting  However  as I listened to these covers repeatedly  I can now say that I understand better what the vocalist is after  Not that I always agree with her and with George Duke the arranger producer  but I agree with their choices and their totally new project and enjoy having had my horizons broadened Ms Belle has immense talent  superb technique  and a clear idea of each piece  and she delivers the goods without faltering or compromise  I admire that a lot in a performer  Her choices ask us for a reaction and we are free to accept or reject what she has offered us  When I first heard these renditions  I was quite stand offish  but over time I have come to appreciate them more and more   almost to the point of addiction   and am grateful to Regina and George for their strength of conviction as well as their superior musicianship They do NOT want us to listen in a passive way  they are waiting for our reaction  and God  this is really so so good    That s the true spirit in which an album should be recorded I d encourage you to get this album because it is about strong  powerful and personal interpretations of some great jazz standards by Regina  You may love everything about it  or you may reject aspects of her interpretations  But in any case you will not remain indifferent  you will be enriched by hearing these pieces in a clear and beautifully personal way  That is an important aspect of why I love this imaginative  innovative quality music and why this recording is invaluable And it cannot be said enough that her backing band is incredible  giving this album the necessary push to take it over the edge  It is a success because it skilfully avoids being categorized  Regina has had the courage to break out of the traditional mould and make a successful cross over  which some purists may not approve of   Only time will tell     \n",
            "Actual Label:  positive\n",
            "Prediction:  positive\n",
            "X_pred (Before Sigmoid):  [[4.11487565]]\n",
            "X_pred (After Sigmoid):  [[0.98393435]]\n",
            "Neuron Analysis--------------------\n",
            "POSITIVE NEURON ANALYSIS--------\n",
            "Text,   I have to admit that  at first listening  I found some aspects of Ms Belle s interpretations of these pieces disorienting  However  as I listened to these covers repeatedly  I can now say that I understand better what the vocalist is after  Not that I always agree with her and with George Duke the arranger producer  but I agree with their choices and their totally new project and enjoy having had my horizons broadened Ms Belle has immense talent  superb technique  and a clear idea of each piece  and she delivers the goods without faltering or compromise  I admire that a lot in a performer  Her choices ask us for a reaction and we are free to accept or reject what she has offered us  When I first heard these renditions  I was quite stand offish  but over time I have come to appreciate them more and more   almost to the point of addiction   and am grateful to Regina and George for their strength of conviction as well as their superior musicianship They do NOT want us to listen in a passive way  they are waiting for our reaction  and God  this is really so so good    That s the true spirit in which an album should be recorded I d encourage you to get this album because it is about strong  powerful and personal interpretations of some great jazz standards by Regina  You may love everything about it  or you may reject aspects of her interpretations  But in any case you will not remain indifferent  you will be enriched by hearing these pieces in a clear and beautifully personal way  That is an important aspect of why I love this imaginative  innovative quality music and why this recording is invaluable And it cannot be said enough that her backing band is incredible  giving this album the necessary push to take it over the edge  It is a success because it skilfully avoids being categorized  Regina has had the courage to break out of the traditional mould and make a successful cross over  which some purists may not approve of   Only time will tell     \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['Excellent', 'Great', 'Highly', 'awesome', 'delicious']\n",
            "Top words in sentence ['great', 'beautiful', 'love', 'con', 'beautifully']\n",
            "Neuron:  10\n",
            "Output Weight:0.84\n",
            "Contribution:1.51\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'awesome', 'Great', 'love']\n",
            "Top words in sentence ['love', 'beautiful', 'beautifully', 'super', 'with']\n",
            "Neuron:  19\n",
            "Output Weight:0.96\n",
            "Contribution:1.51\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'awesome', 'delicious', 'Great']\n",
            "Top words in sentence ['great', 'love', 'beautiful', 'con', 'beautifully']\n",
            "Neuron:  31\n",
            "Output Weight:0.81\n",
            "Contribution:1.50\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Great', 'pleased', 'Highly', 'amazing']\n",
            "Top words in sentence ['love', 'great', 'beautiful', 'aspects', 'well']\n",
            "Neuron:  13\n",
            "Output Weight:0.56\n",
            "Contribution:0.93\n",
            "---------\n",
            "Top words for the neuron ['Great', 'Excellent', 'pleased', 'delicious', 'Highly']\n",
            "Top words in sentence ['love', 'con', 'beautiful', 'well', 'aspects']\n",
            "Neuron:  14\n",
            "Output Weight:0.80\n",
            "Contribution:0.84\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'excellent', 'Great', 'GREAT']\n",
            "Top words in sentence ['great', 'beautiful', 'love', 'con', 'aspects']\n",
            "Neuron:  17\n",
            "Output Weight:0.49\n",
            "Contribution:0.81\n",
            "---------\n",
            "NEGATIVE NEURON ANALYSIS -------------\n",
            "Text,   I have to admit that  at first listening  I found some aspects of Ms Belle s interpretations of these pieces disorienting  However  as I listened to these covers repeatedly  I can now say that I understand better what the vocalist is after  Not that I always agree with her and with George Duke the arranger producer  but I agree with their choices and their totally new project and enjoy having had my horizons broadened Ms Belle has immense talent  superb technique  and a clear idea of each piece  and she delivers the goods without faltering or compromise  I admire that a lot in a performer  Her choices ask us for a reaction and we are free to accept or reject what she has offered us  When I first heard these renditions  I was quite stand offish  but over time I have come to appreciate them more and more   almost to the point of addiction   and am grateful to Regina and George for their strength of conviction as well as their superior musicianship They do NOT want us to listen in a passive way  they are waiting for our reaction  and God  this is really so so good    That s the true spirit in which an album should be recorded I d encourage you to get this album because it is about strong  powerful and personal interpretations of some great jazz standards by Regina  You may love everything about it  or you may reject aspects of her interpretations  But in any case you will not remain indifferent  you will be enriched by hearing these pieces in a clear and beautifully personal way  That is an important aspect of why I love this imaginative  innovative quality music and why this recording is invaluable And it cannot be said enough that her backing band is incredible  giving this album the necessary push to take it over the edge  It is a success because it skilfully avoids being categorized  Regina has had the courage to break out of the traditional mould and make a successful cross over  which some purists may not approve of   Only time will tell     \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'worst', 'horrible']\n",
            "Top words in sentence ['NOT', 'Not', 'listened', 'cannot', 'deliver']\n",
            "Neuron:  7\n",
            "Output Weight:0.85\n",
            "Contribution:-1.60\n",
            "---------\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'returning', 'disappointment']\n",
            "Top words in sentence ['for', 'The', 'cannot', 'talent', 'quality']\n",
            "Neuron:  9\n",
            "Output Weight:0.61\n",
            "Contribution:-0.88\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'worst', 'waste', 'returning']\n",
            "Top words in sentence ['for', 'NOT', 'cannot', 'thin', 'listened']\n",
            "Neuron:  15\n",
            "Output Weight:0.54\n",
            "Contribution:-0.72\n",
            "---------\n",
            "Top words for the neuron ['boring', 'worst', 'terrible', 'useless', 'horrible']\n",
            "Top words in sentence ['Not', 'NOT', 'ain', 'cannot', 'for']\n",
            "Neuron:  4\n",
            "Output Weight:0.39\n",
            "Contribution:-0.54\n",
            "---------\n",
            "Top words for the neuron ['terrible', 'boring', 'waste', 'useless', 'worst']\n",
            "Top words in sentence ['NOT', 'repeated', 'for', 'cannot', 'has']\n",
            "Neuron:  5\n",
            "Output Weight:0.46\n",
            "Contribution:-0.52\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'waste', 'worst', 'awful']\n",
            "Top words in sentence ['NOT', 'Not', 'ain', 'very', 'offered']\n",
            "Neuron:  26\n",
            "Output Weight:0.20\n",
            "Contribution:-0.24\n",
            "---------\n",
            "4.1148756458089535\n",
            "0.9839343487486762\n"
          ]
        }
      ],
      "source": [
        "perform_MLP_analysis(index_analyse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perform forward pass for Logistic Regression --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Review Text  I have to admit that  at first listening  I found some aspects of Ms Belle s interpretations of these pieces disorienting  However  as I listened to these covers repeatedly  I can now say that I understand better what the vocalist is after  Not that I always agree with her and with George Duke the arranger producer  but I agree with their choices and their totally new project and enjoy having had my horizons broadened Ms Belle has immense talent  superb technique  and a clear idea of each piece  and she delivers the goods without faltering or compromise  I admire that a lot in a performer  Her choices ask us for a reaction and we are free to accept or reject what she has offered us  When I first heard these renditions  I was quite stand offish  but over time I have come to appreciate them more and more   almost to the point of addiction   and am grateful to Regina and George for their strength of conviction as well as their superior musicianship They do NOT want us to listen in a passive way  they are waiting for our reaction  and God  this is really so so good    That s the true spirit in which an album should be recorded I d encourage you to get this album because it is about strong  powerful and personal interpretations of some great jazz standards by Regina  You may love everything about it  or you may reject aspects of her interpretations  But in any case you will not remain indifferent  you will be enriched by hearing these pieces in a clear and beautifully personal way  That is an important aspect of why I love this imaginative  innovative quality music and why this recording is invaluable And it cannot be said enough that her backing band is incredible  giving this album the necessary push to take it over the edge  It is a success because it skilfully avoids being categorized  Regina has had the courage to break out of the traditional mould and make a successful cross over  which some purists may not approve of   Only time will tell     \n",
            "Actual Prediction positive\n",
            "negative\n",
            "Prediction Numbers\n",
            "[-0.56685422]\n",
            "Performing weight analysis --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence  I have to admit that  at first listening  I found some aspects of Ms Belle s interpretations of these pieces disorienting  However  as I listened to these covers repeatedly  I can now say that I understand better what the vocalist is after  Not that I always agree with her and with George Duke the arranger producer  but I agree with their choices and their totally new project and enjoy having had my horizons broadened Ms Belle has immense talent  superb technique  and a clear idea of each piece  and she delivers the goods without faltering or compromise  I admire that a lot in a performer  Her choices ask us for a reaction and we are free to accept or reject what she has offered us  When I first heard these renditions  I was quite stand offish  but over time I have come to appreciate them more and more   almost to the point of addiction   and am grateful to Regina and George for their strength of conviction as well as their superior musicianship They do NOT want us to listen in a passive way  they are waiting for our reaction  and God  this is really so so good    That s the true spirit in which an album should be recorded I d encourage you to get this album because it is about strong  powerful and personal interpretations of some great jazz standards by Regina  You may love everything about it  or you may reject aspects of her interpretations  But in any case you will not remain indifferent  you will be enriched by hearing these pieces in a clear and beautifully personal way  That is an important aspect of why I love this imaginative  innovative quality music and why this recording is invaluable And it cannot be said enough that her backing band is incredible  giving this album the necessary push to take it over the edge  It is a success because it skilfully avoids being categorized  Regina has had the courage to break out of the traditional mould and make a successful cross over  which some purists may not approve of   Only time will tell     \n",
            "Word Contributing: There\n",
            "Weight:-1.00\n",
            "Word Contributing: was\n",
            "Weight:-0.60\n",
            "Word Contributing: get\n",
            "Weight:-0.32\n",
            "Word Contributing: like\n",
            "Weight:-0.25\n",
            "Word Contributing: case\n",
            "Weight:-0.19\n",
            "Word Contributing: whole\n",
            "Weight:-0.19\n",
            "Word Contributing: standard\n",
            "Weight:-0.19\n",
            "Word Contributing: camera\n",
            "Weight:-0.19\n",
            "Word Contributing: battery\n",
            "Weight:-0.15\n",
            "Word Contributing: alone\n",
            "Weight:-0.07\n",
            "Word Contributing: very\n",
            "Weight:-0.05\n",
            "Word Contributing: cameras\n",
            "Weight:0.01\n",
            "Word Contributing: This\n",
            "Weight:0.08\n",
            "Word Contributing: intended\n",
            "Weight:0.14\n",
            "Word Contributing: printer\n",
            "Weight:0.17\n",
            "Word Contributing: accessories\n",
            "Weight:0.23\n",
            "Word Contributing: Canon\n",
            "Weight:0.27\n",
            "Word Contributing: deal\n",
            "Weight:0.27\n",
            "Word Contributing: needed\n",
            "Weight:0.28\n",
            "Word Contributing: absolutely\n",
            "Weight:0.28\n",
            "Word Contributing: addition\n",
            "Weight:0.30\n",
            "Word Contributing: bag\n",
            "Weight:0.31\n",
            "Word Contributing: extra\n",
            "Weight:0.37\n",
            "Word Contributing: carry\n",
            "Weight:0.47\n",
            "Word Contributing: cheaper\n",
            "Weight:0.61\n",
            "Word Contributing: portable\n",
            "Weight:0.71\n",
            "Word Contributing: shoulder\n",
            "Weight:0.71\n",
            "Word Contributing: kit\n",
            "Weight:0.72\n",
            "Word Contributing: nice\n",
            "Weight:0.80\n",
            "Word Contributing: price\n",
            "Weight:1.06\n",
            "Sum of weights: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sum of weights from decomposition: [4.62319121]\n"
          ]
        }
      ],
      "source": [
        "perform_LogReg_analysis(index_analyse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second example "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_analyse = mlp_wins_indices[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Forward Pass ----------------\n",
            "33\n",
            "Text,   I gave this pedometer one star because it is not for anyone who wants to use it straight off of the shelf   This pedometer can work if you do not mind working on it   It has problems that need modification The top of the clip on this pedometer pushes against the body  making the vertical angle    degrees or more off of plumb   That means the weighted arm inside drags on the circuit board and does not move freely   Remove the factory clip and the tabs that hold it   Add some adhesive Velchro to the back of the case   Make a fabric band for your ankle   Attach the ends to each other with Velchro   Put a strip of Velchro on the outside of the band for attaching the pedometer to the band   I get  accurate readings with this pedometer when it is attached to the inside of my right ankle   I got inconsistent readings with it on my waistline The hair spring that returns the weighted arm to its upright position weakened considerably after about ten miles and the pedometer drastically undercounted   I opened the pedometer and added a strut to brace the spring   The strut is made of thin brass   Aluminum from a soda can would work  too   I made a   V   in one end of the strut to hold the spring   I pushed lightly against the spring from the left side  drilled a hole in the brass  and anchored the strut to a screw on the lower left part of the circuit board   The strut angles up just slightly from the screw that anchors it   I now have quite a few miles on the pedometer    I have a theory the spring weakened because its solder joint to the circuit board may have broken slightly and allowed the spring to twist freely  Several times the pedometer reset itself to       for no apparent reason   I know nothing pushed on the reset button   One incident was while it was going through the scanner at airport security   Once it reset while I was walking   Twice it reset while I was napping on the sofa   I am thinking a charge of static electricity overwhelmed the circuit  but I do not know   Static electricity from too many synthetic fabrics can be a problem with more expensive pedometers  too This pedometer uses a counter circuit   If someone needs an inexpensive electronic counter  connect a wire to the weighted arm and another wire to the coiled spring it contacts   Connect a button switch to the two wires   Freeze the weighted arm in place with hot glue \n",
            "Actual Label:  negative\n",
            "Prediction:  negative\n",
            "X_pred (Before Sigmoid):  [[-9.52650655]]\n",
            "X_pred (After Sigmoid):  [[7.28885172e-05]]\n",
            "Neuron Analysis--------------------\n",
            "POSITIVE NEURON ANALYSIS--------\n",
            "Text,   I gave this pedometer one star because it is not for anyone who wants to use it straight off of the shelf   This pedometer can work if you do not mind working on it   It has problems that need modification The top of the clip on this pedometer pushes against the body  making the vertical angle    degrees or more off of plumb   That means the weighted arm inside drags on the circuit board and does not move freely   Remove the factory clip and the tabs that hold it   Add some adhesive Velchro to the back of the case   Make a fabric band for your ankle   Attach the ends to each other with Velchro   Put a strip of Velchro on the outside of the band for attaching the pedometer to the band   I get  accurate readings with this pedometer when it is attached to the inside of my right ankle   I got inconsistent readings with it on my waistline The hair spring that returns the weighted arm to its upright position weakened considerably after about ten miles and the pedometer drastically undercounted   I opened the pedometer and added a strut to brace the spring   The strut is made of thin brass   Aluminum from a soda can would work  too   I made a   V   in one end of the strut to hold the spring   I pushed lightly against the spring from the left side  drilled a hole in the brass  and anchored the strut to a screw on the lower left part of the circuit board   The strut angles up just slightly from the screw that anchors it   I now have quite a few miles on the pedometer    I have a theory the spring weakened because its solder joint to the circuit board may have broken slightly and allowed the spring to twist freely  Several times the pedometer reset itself to       for no apparent reason   I know nothing pushed on the reset button   One incident was while it was going through the scanner at airport security   Once it reset while I was walking   Twice it reset while I was napping on the sofa   I am thinking a charge of static electricity overwhelmed the circuit  but I do not know   Static electricity from too many synthetic fabrics can be a problem with more expensive pedometers  too This pedometer uses a counter circuit   If someone needs an inexpensive electronic counter  connect a wire to the weighted arm and another wire to the coiled spring it contacts   Connect a button switch to the two wires   Freeze the weighted arm in place with hot glue \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['Excellent', 'Highly', 'awesome', 'Great', 'love']\n",
            "Top words in sentence ['This', 'problems', 'with', 'use', 'mind']\n",
            "Neuron:  19\n",
            "Output Weight 0.30250386276519475\n",
            "Contribution 0.4747868940165278\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Great', 'pleased', 'Highly', 'amazing']\n",
            "Top words in sentence ['for', 'use', 'This', 'One', 'ever']\n",
            "Neuron:  13\n",
            "Output Weight 0.16982265021278808\n",
            "Contribution 0.2814821996046377\n",
            "---------\n",
            "Top words for the neuron ['Great', 'Excellent', 'pleased', 'delicious', 'Highly']\n",
            "Top words in sentence ['con', 'art', 'One', 'pens', 'degree']\n",
            "Neuron:  14\n",
            "Output Weight 0.19743164611333033\n",
            "Contribution 0.2056864414551236\n",
            "---------\n",
            "Top words for the neuron ['love', 'good', 'long', 'perfect', 'easy']\n",
            "Top words in sentence ['led', 'con', 'set', 'band', 'open']\n",
            "Neuron:  30\n",
            "Output Weight 0.0951982473897301\n",
            "Contribution 0.06317921554725914\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'excellent', 'Great', 'GREAT']\n",
            "Top words in sentence ['con', 'This', 'art', 'band', 'One']\n",
            "Neuron:  17\n",
            "Output Weight 0.02058370685892249\n",
            "Contribution 0.03404413006450532\n",
            "---------\n",
            "Top words for the neuron ['years', 'This', 'was', 'amazing', 'very']\n",
            "Top words in sentence ['This', 'was', 'con', 'wants', 'king']\n",
            "Neuron:  25\n",
            "Output Weight 0.04577104860162331\n",
            "Contribution 0.006303235564909752\n",
            "---------\n",
            "NEGATIVE NEURON ANALYSIS -------------\n",
            "Text,   I gave this pedometer one star because it is not for anyone who wants to use it straight off of the shelf   This pedometer can work if you do not mind working on it   It has problems that need modification The top of the clip on this pedometer pushes against the body  making the vertical angle    degrees or more off of plumb   That means the weighted arm inside drags on the circuit board and does not move freely   Remove the factory clip and the tabs that hold it   Add some adhesive Velchro to the back of the case   Make a fabric band for your ankle   Attach the ends to each other with Velchro   Put a strip of Velchro on the outside of the band for attaching the pedometer to the band   I get  accurate readings with this pedometer when it is attached to the inside of my right ankle   I got inconsistent readings with it on my waistline The hair spring that returns the weighted arm to its upright position weakened considerably after about ten miles and the pedometer drastically undercounted   I opened the pedometer and added a strut to brace the spring   The strut is made of thin brass   Aluminum from a soda can would work  too   I made a   V   in one end of the strut to hold the spring   I pushed lightly against the spring from the left side  drilled a hole in the brass  and anchored the strut to a screw on the lower left part of the circuit board   The strut angles up just slightly from the screw that anchors it   I now have quite a few miles on the pedometer    I have a theory the spring weakened because its solder joint to the circuit board may have broken slightly and allowed the spring to twist freely  Several times the pedometer reset itself to       for no apparent reason   I know nothing pushed on the reset button   One incident was while it was going through the scanner at airport security   Once it reset while I was walking   Twice it reset while I was napping on the sofa   I am thinking a charge of static electricity overwhelmed the circuit  but I do not know   Static electricity from too many synthetic fabrics can be a problem with more expensive pedometers  too This pedometer uses a counter circuit   If someone needs an inexpensive electronic counter  connect a wire to the weighted arm and another wire to the coiled spring it contacts   Connect a button switch to the two wires   Freeze the weighted arm in place with hot glue \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'worst', 'horrible']\n",
            "Top words in sentence ['return', 'broke', 'too', 'pedometer', 'for']\n",
            "Neuron:  7\n",
            "Output Weight 1.454825604834556\n",
            "Contribution -2.7305686702763845\n",
            "---------\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'returning', 'disappointment']\n",
            "Top words in sentence ['return', 'for', 'The', 'broke', 'weak']\n",
            "Neuron:  9\n",
            "Output Weight 1.0740545155948402\n",
            "Contribution -1.5541656231626453\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'worst', 'waste', 'returning']\n",
            "Top words in sentence ['return', 'broke', 'for', 'thin', 'ain']\n",
            "Neuron:  15\n",
            "Output Weight 1.108504259395916\n",
            "Contribution -1.4810630794853215\n",
            "---------\n",
            "Top words for the neuron ['boring', 'worst', 'terrible', 'useless', 'horrible']\n",
            "Top words in sentence ['return', 'weak', 'broke', 'ain', 'for']\n",
            "Neuron:  4\n",
            "Output Weight 0.8821734031058194\n",
            "Contribution -1.235058087971799\n",
            "---------\n",
            "Top words for the neuron ['terrible', 'boring', 'waste', 'useless', 'worst']\n",
            "Top words in sentence ['return', 'broke', 'too', 'weak', 'This']\n",
            "Neuron:  5\n",
            "Output Weight 0.9576944338339445\n",
            "Contribution -1.0829981652416851\n",
            "---------\n",
            "Top words for the neuron ['waste', 'worst', 'boring', 'hopes', 'returning']\n",
            "Top words in sentence ['broke', 'return', 'This', 'par', 'pedometer']\n",
            "Neuron:  29\n",
            "Output Weight 0.7809321418493345\n",
            "Contribution -0.70846999158677\n",
            "---------\n",
            "-9.526506554206756\n",
            "7.288851722763707e-05\n"
          ]
        }
      ],
      "source": [
        "perform_MLP_analysis(index_analyse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perform forward pass for Logistic Regression --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Review Text  I have to admit that  at first listening  I found some aspects of Ms Belle s interpretations of these pieces disorienting  However  as I listened to these covers repeatedly  I can now say that I understand better what the vocalist is after  Not that I always agree with her and with George Duke the arranger producer  but I agree with their choices and their totally new project and enjoy having had my horizons broadened Ms Belle has immense talent  superb technique  and a clear idea of each piece  and she delivers the goods without faltering or compromise  I admire that a lot in a performer  Her choices ask us for a reaction and we are free to accept or reject what she has offered us  When I first heard these renditions  I was quite stand offish  but over time I have come to appreciate them more and more   almost to the point of addiction   and am grateful to Regina and George for their strength of conviction as well as their superior musicianship They do NOT want us to listen in a passive way  they are waiting for our reaction  and God  this is really so so good    That s the true spirit in which an album should be recorded I d encourage you to get this album because it is about strong  powerful and personal interpretations of some great jazz standards by Regina  You may love everything about it  or you may reject aspects of her interpretations  But in any case you will not remain indifferent  you will be enriched by hearing these pieces in a clear and beautifully personal way  That is an important aspect of why I love this imaginative  innovative quality music and why this recording is invaluable And it cannot be said enough that her backing band is incredible  giving this album the necessary push to take it over the edge  It is a success because it skilfully avoids being categorized  Regina has had the courage to break out of the traditional mould and make a successful cross over  which some purists may not approve of   Only time will tell     \n",
            "Actual Prediction positive\n",
            "negative\n",
            "Prediction Numbers\n",
            "[-0.56685422]\n",
            "Performing weight analysis --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence  I have to admit that  at first listening  I found some aspects of Ms Belle s interpretations of these pieces disorienting  However  as I listened to these covers repeatedly  I can now say that I understand better what the vocalist is after  Not that I always agree with her and with George Duke the arranger producer  but I agree with their choices and their totally new project and enjoy having had my horizons broadened Ms Belle has immense talent  superb technique  and a clear idea of each piece  and she delivers the goods without faltering or compromise  I admire that a lot in a performer  Her choices ask us for a reaction and we are free to accept or reject what she has offered us  When I first heard these renditions  I was quite stand offish  but over time I have come to appreciate them more and more   almost to the point of addiction   and am grateful to Regina and George for their strength of conviction as well as their superior musicianship They do NOT want us to listen in a passive way  they are waiting for our reaction  and God  this is really so so good    That s the true spirit in which an album should be recorded I d encourage you to get this album because it is about strong  powerful and personal interpretations of some great jazz standards by Regina  You may love everything about it  or you may reject aspects of her interpretations  But in any case you will not remain indifferent  you will be enriched by hearing these pieces in a clear and beautifully personal way  That is an important aspect of why I love this imaginative  innovative quality music and why this recording is invaluable And it cannot be said enough that her backing band is incredible  giving this album the necessary push to take it over the edge  It is a success because it skilfully avoids being categorized  Regina has had the courage to break out of the traditional mould and make a successful cross over  which some purists may not approve of   Only time will tell     \n",
            "Word Contributing: NOT\n",
            "Weight:-1.43\n",
            "Word Contributing: When\n",
            "Weight:-1.20\n",
            "Word Contributing: point\n",
            "Weight:-1.13\n",
            "Word Contributing: Not\n",
            "Weight:-1.07\n",
            "Word Contributing: However\n",
            "Weight:-0.99\n",
            "Word Contributing: cannot\n",
            "Weight:-0.98\n",
            "Word Contributing: idea\n",
            "Weight:-0.89\n",
            "Word Contributing: But\n",
            "Weight:-0.87\n",
            "Word Contributing: make\n",
            "Weight:-0.80\n",
            "Word Contributing: way\n",
            "Weight:-0.79\n",
            "Word Contributing: take\n",
            "Weight:-0.74\n",
            "Word Contributing: piece\n",
            "Weight:-0.67\n",
            "Word Contributing: almost\n",
            "Weight:-0.64\n",
            "Word Contributing: was\n",
            "Weight:-0.60\n",
            "Word Contributing: better\n",
            "Weight:-0.59\n",
            "Word Contributing: said\n",
            "Weight:-0.54\n",
            "Word Contributing: has\n",
            "Weight:-0.53\n",
            "Word Contributing: but\n",
            "Weight:-0.52\n",
            "Word Contributing: And\n",
            "Weight:-0.48\n",
            "Word Contributing: music\n",
            "Weight:-0.48\n",
            "Word Contributing: having\n",
            "Weight:-0.46\n",
            "Word Contributing: are\n",
            "Weight:-0.41\n",
            "Word Contributing: come\n",
            "Weight:-0.40\n",
            "Word Contributing: with\n",
            "Weight:-0.37\n",
            "Word Contributing: That\n",
            "Weight:-0.36\n",
            "Word Contributing: clear\n",
            "Weight:-0.35\n",
            "Word Contributing: get\n",
            "Weight:-0.32\n",
            "Word Contributing: agree\n",
            "Weight:-0.31\n",
            "Word Contributing: free\n",
            "Weight:-0.31\n",
            "Word Contributing: had\n",
            "Weight:-0.29\n",
            "Word Contributing: offered\n",
            "Weight:-0.27\n",
            "Word Contributing: You\n",
            "Weight:-0.26\n",
            "Word Contributing: album\n",
            "Weight:-0.26\n",
            "Word Contributing: listening\n",
            "Weight:-0.26\n",
            "Word Contributing: want\n",
            "Weight:-0.25\n",
            "Word Contributing: found\n",
            "Weight:-0.24\n",
            "Word Contributing: stand\n",
            "Weight:-0.21\n",
            "Word Contributing: necessary\n",
            "Weight:-0.20\n",
            "Word Contributing: case\n",
            "Weight:-0.19\n",
            "Word Contributing: cross\n",
            "Weight:-0.18\n",
            "Word Contributing: for\n",
            "Weight:-0.17\n",
            "Word Contributing: now\n",
            "Weight:-0.17\n",
            "Word Contributing: pieces\n",
            "Weight:-0.17\n",
            "Word Contributing: say\n",
            "Weight:-0.15\n",
            "Word Contributing: backing\n",
            "Weight:-0.13\n",
            "Word Contributing: them\n",
            "Weight:-0.11\n",
            "Word Contributing: recorded\n",
            "Weight:-0.10\n",
            "Word Contributing: giving\n",
            "Weight:-0.10\n",
            "Word Contributing: project\n",
            "Weight:-0.08\n",
            "Word Contributing: standards\n",
            "Weight:-0.07\n",
            "Word Contributing: band\n",
            "Weight:-0.07\n",
            "Word Contributing: new\n",
            "Weight:-0.06\n",
            "Word Contributing: totally\n",
            "Weight:-0.05\n",
            "Word Contributing: personal\n",
            "Weight:-0.04\n",
            "Word Contributing: important\n",
            "Weight:-0.04\n",
            "Word Contributing: innovative\n",
            "Weight:-0.04\n",
            "Word Contributing: time\n",
            "Weight:-0.03\n",
            "Word Contributing: traditional\n",
            "Weight:-0.01\n",
            "Word Contributing: quite\n",
            "Weight:-0.01\n",
            "Word Contributing: spirit\n",
            "Weight:-0.00\n",
            "Word Contributing: powerful\n",
            "Weight:0.00\n",
            "Word Contributing: They\n",
            "Weight:0.03\n",
            "Word Contributing: appreciate\n",
            "Weight:0.03\n",
            "Word Contributing: quality\n",
            "Weight:0.03\n",
            "Word Contributing: strength\n",
            "Weight:0.04\n",
            "Word Contributing: imaginative\n",
            "Weight:0.08\n",
            "Word Contributing: admire\n",
            "Weight:0.08\n",
            "Word Contributing: Her\n",
            "Weight:0.11\n",
            "Word Contributing: true\n",
            "Weight:0.11\n",
            "Word Contributing: tell\n",
            "Weight:0.12\n",
            "Word Contributing: understand\n",
            "Weight:0.13\n",
            "Word Contributing: break\n",
            "Weight:0.14\n",
            "Word Contributing: compromise\n",
            "Weight:0.16\n",
            "Word Contributing: lot\n",
            "Weight:0.16\n",
            "Word Contributing: successful\n",
            "Weight:0.18\n",
            "Word Contributing: push\n",
            "Weight:0.21\n",
            "Word Contributing: waiting\n",
            "Weight:0.22\n",
            "Word Contributing: listen\n",
            "Weight:0.22\n",
            "Word Contributing: God\n",
            "Weight:0.23\n",
            "Word Contributing: ask\n",
            "Weight:0.23\n",
            "Word Contributing: goods\n",
            "Weight:0.25\n",
            "Word Contributing: really\n",
            "Weight:0.25\n",
            "Word Contributing: recording\n",
            "Weight:0.25\n",
            "Word Contributing: success\n",
            "Weight:0.26\n",
            "Word Contributing: performer\n",
            "Weight:0.26\n",
            "Word Contributing: edge\n",
            "Weight:0.27\n",
            "Word Contributing: George\n",
            "Weight:0.27\n",
            "Word Contributing: choices\n",
            "Weight:0.27\n",
            "Word Contributing: reaction\n",
            "Weight:0.31\n",
            "Word Contributing: encourage\n",
            "Weight:0.31\n",
            "Word Contributing: grateful\n",
            "Weight:0.31\n",
            "Word Contributing: admit\n",
            "Weight:0.32\n",
            "Word Contributing: remain\n",
            "Weight:0.33\n",
            "Word Contributing: delivers\n",
            "Weight:0.33\n",
            "Word Contributing: talent\n",
            "Weight:0.36\n",
            "Word Contributing: repeatedly\n",
            "Weight:0.36\n",
            "Word Contributing: heard\n",
            "Weight:0.39\n",
            "Word Contributing: aspect\n",
            "Weight:0.41\n",
            "Word Contributing: addiction\n",
            "Weight:0.44\n",
            "Word Contributing: Only\n",
            "Weight:0.44\n",
            "Word Contributing: accept\n",
            "Weight:0.46\n",
            "Word Contributing: beautifully\n",
            "Weight:0.48\n",
            "Word Contributing: good\n",
            "Weight:0.48\n",
            "Word Contributing: well\n",
            "Weight:0.50\n",
            "Word Contributing: hearing\n",
            "Weight:0.51\n",
            "Word Contributing: technique\n",
            "Weight:0.51\n",
            "Word Contributing: listened\n",
            "Weight:0.52\n",
            "Word Contributing: superior\n",
            "Weight:0.55\n",
            "Word Contributing: incredible\n",
            "Weight:0.58\n",
            "Word Contributing: superb\n",
            "Weight:0.59\n",
            "Word Contributing: always\n",
            "Weight:0.59\n",
            "Word Contributing: strong\n",
            "Weight:0.60\n",
            "Word Contributing: enjoy\n",
            "Weight:0.62\n",
            "Word Contributing: covers\n",
            "Weight:0.64\n",
            "Word Contributing: invaluable\n",
            "Weight:0.78\n",
            "Word Contributing: aspects\n",
            "Weight:0.79\n",
            "Word Contributing: producer\n",
            "Weight:0.82\n",
            "Word Contributing: jazz\n",
            "Weight:0.82\n",
            "Word Contributing: great\n",
            "Weight:1.43\n",
            "Word Contributing: love\n",
            "Weight:1.67\n",
            "Sum of weights: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sum of weights from decomposition: [-0.56685422]\n"
          ]
        }
      ],
      "source": [
        "perform_LogReg_analysis(index_analyse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Third Example "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_analyse = mlp_wins_indices[12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Forward Pass ----------------\n",
            "77\n",
            "Text,  goal frame is okay but net is a pile of pants and does not last five mins  cant seem to buy replacement\n",
            "Actual Label:  negative\n",
            "Prediction:  negative\n",
            "X_pred (Before Sigmoid):  [[-2.8895723]]\n",
            "X_pred (After Sigmoid):  [[0.05267146]]\n",
            "Neuron Analysis--------------------\n",
            "POSITIVE NEURON ANALYSIS--------\n",
            "Text,  goal frame is okay but net is a pile of pants and does not last five mins  cant seem to buy replacement\n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['Great', 'Excellent', 'pleased', 'delicious', 'Highly']\n",
            "Top words in sentence ['net', 'see', 'does', 'buy', 'but']\n",
            "Neuron:  14\n",
            "Output Weight 0.04227470429196086\n",
            "Contribution 0.04404224783897991\n",
            "---------\n",
            "Top words for the neuron ['going', 'purchased', 'always', 'whole', 'wanted']\n",
            "Top words in sentence ['buy', 'pants', 'replacement', 'goal', 'frame']\n",
            "Neuron:  20\n",
            "Output Weight 0.09217956353206061\n",
            "Contribution 0.026901990922614763\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'perfect', 'amazing', 'love', 'delicious']\n",
            "Top words in sentence ['ram', 'see', 'goal', 'placement', 'men']\n",
            "Neuron:  2\n",
            "Output Weight 0.0\n",
            "Contribution 0.0\n",
            "---------\n",
            "Top words for the neuron ['with', 'This', 'Easy', 'easy', 'instructions']\n",
            "Top words in sentence ['goal', 'does', 'okay', 'placement', 'pile']\n",
            "Neuron:  3\n",
            "Output Weight 0.0\n",
            "Contribution 0.0\n",
            "---------\n",
            "Top words for the neuron ['great', 'Great', 'job', 'perfect', 'are']\n",
            "Top words in sentence ['replacement', 'placement', 'place', 'does', 'ins']\n",
            "Neuron:  6\n",
            "Output Weight 0.0\n",
            "Contribution 0.0\n",
            "---------\n",
            "Top words for the neuron ['found', 'didn', 'call', 'annoying', 'wear']\n",
            "Top words in sentence ['see', 'place', 'net', 'ive', 'okay']\n",
            "Neuron:  8\n",
            "Output Weight 0.0\n",
            "Contribution -0.0\n",
            "---------\n",
            "NEGATIVE NEURON ANALYSIS -------------\n",
            "Text,  goal frame is okay but net is a pile of pants and does not last five mins  cant seem to buy replacement\n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'worst', 'horrible']\n",
            "Top words in sentence ['buy', 'okay', 'mins', 'replace', 'frame']\n",
            "Neuron:  7\n",
            "Output Weight 0.48967360960154394\n",
            "Contribution -0.9190705831653135\n",
            "---------\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'returning', 'disappointment']\n",
            "Top words in sentence ['buy', 'okay', 'rep', 'mins', 'replace']\n",
            "Neuron:  9\n",
            "Output Weight 0.2994794242024877\n",
            "Contribution -0.4333491635499302\n",
            "---------\n",
            "Top words for the neuron ['boring', 'worst', 'terrible', 'useless', 'horrible']\n",
            "Top words in sentence ['buy', 'mins', 'frame', 'does', 'replace']\n",
            "Neuron:  4\n",
            "Output Weight 0.25934636330831573\n",
            "Contribution -0.363089413557831\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'worst', 'waste', 'returning']\n",
            "Top words in sentence ['okay', 'buy', 'does', 'rep', 'mins']\n",
            "Neuron:  15\n",
            "Output Weight 0.24243157608661234\n",
            "Contribution -0.32391075956621707\n",
            "---------\n",
            "Top words for the neuron ['terrible', 'boring', 'waste', 'useless', 'worst']\n",
            "Top words in sentence ['okay', 'rep', 'pile', 'frame', 'does']\n",
            "Neuron:  5\n",
            "Output Weight 0.25893287701467194\n",
            "Contribution -0.2928113820240325\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'waste', 'worst', 'awful']\n",
            "Top words in sentence ['does', 'okay', 'frame', 'seem', 'men']\n",
            "Neuron:  26\n",
            "Output Weight 0.16729763168637696\n",
            "Contribution -0.19702082972197857\n",
            "---------\n",
            "-2.8895723005006597\n",
            "0.0526714553487231\n"
          ]
        }
      ],
      "source": [
        "perform_MLP_analysis(index_analyse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perform forward pass for Logistic Regression --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Review Text goal frame is okay but net is a pile of pants and does not last five mins  cant seem to buy replacement\n",
            "Actual Prediction negative\n",
            "positive\n",
            "Prediction Numbers\n",
            "[0.20961189]\n",
            "Performing weight analysis --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence goal frame is okay but net is a pile of pants and does not last five mins  cant seem to buy replacement\n",
            "Word Contributing: same\n",
            "Weight: -1.0329926668558076\n",
            "Word Contributing: support\n",
            "Weight: -1.0273973834966181\n",
            "Word Contributing: There\n",
            "Weight: -1.0046727724922333\n",
            "Word Contributing: However\n",
            "Weight: -0.9901818655365758\n",
            "Word Contributing: example\n",
            "Weight: -0.9572874083150715\n",
            "Word Contributing: there\n",
            "Weight: -0.8306108487956485\n",
            "Word Contributing: etc\n",
            "Weight: -0.7791712295864883\n",
            "Word Contributing: work\n",
            "Weight: -0.7771918656722246\n",
            "Word Contributing: does\n",
            "Weight: -0.7329061924890657\n",
            "Word Contributing: The\n",
            "Weight: -0.7308517802545683\n",
            "Word Contributing: major\n",
            "Weight: -0.7020899791083896\n",
            "Word Contributing: working\n",
            "Weight: -0.6122519849909912\n",
            "Word Contributing: was\n",
            "Weight: -0.6031317568010689\n",
            "Word Contributing: list\n",
            "Weight: -0.5520598881033529\n",
            "Word Contributing: Also\n",
            "Weight: -0.5444114434215516\n",
            "Word Contributing: but\n",
            "Weight: -0.5192541915843839\n",
            "Word Contributing: look\n",
            "Weight: -0.5146415031661651\n",
            "Word Contributing: For\n",
            "Weight: -0.5117697852666598\n",
            "Word Contributing: don\n",
            "Weight: -0.4124918336739688\n",
            "Word Contributing: are\n",
            "Weight: -0.40732581381784794\n",
            "Word Contributing: Windows\n",
            "Weight: -0.3964870932395416\n",
            "Word Contributing: with\n",
            "Weight: -0.36716353681439934\n",
            "Word Contributing: find\n",
            "Weight: -0.3476149660118765\n",
            "Word Contributing: software\n",
            "Weight: -0.33867246251206057\n",
            "Word Contributing: slow\n",
            "Weight: -0.3369225876990171\n",
            "Word Contributing: design\n",
            "Weight: -0.334544314554443\n",
            "Word Contributing: get\n",
            "Weight: -0.32482662982405236\n",
            "Word Contributing: tiny\n",
            "Weight: -0.31792902628576414\n",
            "Word Contributing: agree\n",
            "Weight: -0.3142193861668543\n",
            "Word Contributing: future\n",
            "Weight: -0.3019037310697173\n",
            "Word Contributing: months\n",
            "Weight: -0.2984387769120629\n",
            "Word Contributing: You\n",
            "Weight: -0.2607568791155882\n",
            "Word Contributing: editing\n",
            "Weight: -0.2442751213103032\n",
            "Word Contributing: editor\n",
            "Weight: -0.23877725822232215\n",
            "Word Contributing: interface\n",
            "Weight: -0.23788730650120413\n",
            "Word Contributing: product\n",
            "Weight: -0.22485905111007198\n",
            "Word Contributing: released\n",
            "Weight: -0.22234348712666646\n",
            "Word Contributing: feature\n",
            "Weight: -0.2042157177589363\n",
            "Word Contributing: aren\n",
            "Weight: -0.1840593006935616\n",
            "Word Contributing: been\n",
            "Weight: -0.18098618706045053\n",
            "Word Contributing: big\n",
            "Weight: -0.17918165638768144\n",
            "Word Contributing: for\n",
            "Weight: -0.1738482821447878\n",
            "Word Contributing: fit\n",
            "Weight: -0.162682654587717\n",
            "Word Contributing: options\n",
            "Weight: -0.16042712770552348\n",
            "Word Contributing: objects\n",
            "Weight: -0.15372019741630458\n",
            "Word Contributing: floor\n",
            "Weight: -0.1490731835290602\n",
            "Word Contributing: seem\n",
            "Weight: -0.1389751758396016\n",
            "Word Contributing: printing\n",
            "Weight: -0.11597217428954537\n",
            "Word Contributing: developed\n",
            "Weight: -0.11172658695220761\n",
            "Word Contributing: them\n",
            "Weight: -0.10799347002344889\n",
            "Word Contributing: applications\n",
            "Weight: -0.10734522894056348\n",
            "Word Contributing: Although\n",
            "Weight: -0.10573551780593006\n",
            "Word Contributing: basically\n",
            "Weight: -0.10005940101583165\n",
            "Word Contributing: paper\n",
            "Weight: -0.07704755267705929\n",
            "Word Contributing: terribly\n",
            "Weight: -0.07395650140499119\n",
            "Word Contributing: ago\n",
            "Weight: -0.06037673622784771\n",
            "Word Contributing: reviews\n",
            "Weight: -0.04914589689501956\n",
            "Word Contributing: very\n",
            "Weight: -0.04685586246418355\n",
            "Word Contributing: short\n",
            "Weight: -0.041721947819592764\n",
            "Word Contributing: add\n",
            "Weight: -0.03745043929199901\n",
            "Word Contributing: moving\n",
            "Weight: -0.01967558639073698\n",
            "Word Contributing: house\n",
            "Weight: -0.01748818934417162\n",
            "Word Contributing: necessarily\n",
            "Weight: 0.0036142207303166275\n",
            "Word Contributing: issues\n",
            "Weight: 0.00952152331176641\n",
            "Word Contributing: couldn\n",
            "Weight: 0.013216978786129097\n",
            "Word Contributing: feel\n",
            "Weight: 0.015490925161918257\n",
            "Word Contributing: buildings\n",
            "Weight: 0.015990914125888756\n",
            "Word Contributing: use\n",
            "Weight: 0.021482679802316367\n",
            "Word Contributing: clean\n",
            "Weight: 0.04525560703485018\n",
            "Word Contributing: print\n",
            "Weight: 0.06669180156446809\n",
            "Word Contributing: Once\n",
            "Weight: 0.07599782265577125\n",
            "Word Contributing: integrated\n",
            "Weight: 0.08953399708015534\n",
            "Word Contributing: positive\n",
            "Weight: 0.09622909352424788\n",
            "Word Contributing: dimension\n",
            "Weight: 0.09970807862869079\n",
            "Word Contributing: things\n",
            "Weight: 0.11593142337559634\n",
            "Word Contributing: surface\n",
            "Weight: 0.11757287916726954\n",
            "Word Contributing: series\n",
            "Weight: 0.13402374706826456\n",
            "Word Contributing: precision\n",
            "Weight: 0.14046481502883212\n",
            "Word Contributing: quickly\n",
            "Weight: 0.14977950724547595\n",
            "Word Contributing: large\n",
            "Weight: 0.15365453725275902\n",
            "Word Contributing: possible\n",
            "Weight: 0.15671258110715905\n",
            "Word Contributing: printed\n",
            "Weight: 0.16591598587836717\n",
            "Word Contributing: text\n",
            "Weight: 0.16758644581489235\n",
            "Word Contributing: brick\n",
            "Weight: 0.19162595328324686\n",
            "Word Contributing: seriously\n",
            "Weight: 0.20261462824940984\n",
            "Word Contributing: upgrade\n",
            "Weight: 0.20848709127654635\n",
            "Word Contributing: create\n",
            "Weight: 0.22144587344768207\n",
            "Word Contributing: floors\n",
            "Weight: 0.2346590549407034\n",
            "Word Contributing: functional\n",
            "Weight: 0.2423131792680962\n",
            "Word Contributing: updated\n",
            "Weight: 0.24787847161734022\n",
            "Word Contributing: limited\n",
            "Weight: 0.25120231750861055\n",
            "Word Contributing: items\n",
            "Weight: 0.2612076698682736\n",
            "Word Contributing: windows\n",
            "Weight: 0.2792061588009994\n",
            "Word Contributing: texture\n",
            "Weight: 0.2794620960873374\n",
            "Word Contributing: furniture\n",
            "Weight: 0.28775732292344675\n",
            "Word Contributing: visual\n",
            "Weight: 0.29345067706014716\n",
            "Word Contributing: snap\n",
            "Weight: 0.29520088067507405\n",
            "Word Contributing: came\n",
            "Weight: 0.3097351295179734\n",
            "Word Contributing: drawing\n",
            "Weight: 0.31043508068372044\n",
            "Word Contributing: intuitive\n",
            "Weight: 0.3140847128803709\n",
            "Word Contributing: user\n",
            "Weight: 0.34868381561854195\n",
            "Word Contributing: approximately\n",
            "Weight: 0.36819611454226964\n",
            "Word Contributing: complicated\n",
            "Weight: 0.37652943726379795\n",
            "Word Contributing: releases\n",
            "Weight: 0.385789719497632\n",
            "Word Contributing: Can\n",
            "Weight: 0.41740553710340755\n",
            "Word Contributing: object\n",
            "Weight: 0.4180741162069062\n",
            "Word Contributing: Hard\n",
            "Weight: 0.44001522331673826\n",
            "Word Contributing: overall\n",
            "Weight: 0.4622902328106477\n",
            "Word Contributing: placement\n",
            "Weight: 0.4689586863123607\n",
            "Word Contributing: feedback\n",
            "Weight: 0.5406690652753251\n",
            "Word Contributing: wall\n",
            "Weight: 0.549115853967082\n",
            "Word Contributing: scale\n",
            "Weight: 0.5517402140911494\n",
            "Word Contributing: library\n",
            "Weight: 0.5583492113018244\n",
            "Word Contributing: degree\n",
            "Weight: 0.5826245010160956\n",
            "Word Contributing: drawings\n",
            "Weight: 0.5912092449571194\n",
            "Word Contributing: preview\n",
            "Weight: 0.6028003573797568\n",
            "Word Contributing: functionality\n",
            "Weight: 0.6160473522160569\n",
            "Word Contributing: application\n",
            "Weight: 0.6515706726699763\n",
            "Word Contributing: provides\n",
            "Weight: 0.6787221860206761\n",
            "Word Contributing: smaller\n",
            "Weight: 0.7488645724961173\n",
            "Word Contributing: Punch\n",
            "Weight: 0.8048681915867962\n",
            "Word Contributing: exterior\n",
            "Weight: 0.8355664099516965\n",
            "Word Contributing: dimensions\n",
            "Weight: 0.8728507229872023\n",
            "Word Contributing: limitations\n",
            "Weight: 0.8951568763655365\n",
            "Word Contributing: easy\n",
            "Weight: 1.600674169836949\n",
            "Sum of weights: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sum of weights from decomposition: [-0.4241365]\n"
          ]
        }
      ],
      "source": [
        "perform_LogReg_analysis(index_analyse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MLP and Logistic Regression both wrong**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Forward Pass ----------------\n",
            "18\n",
            "Text,   It s sometimes hard to think there are really anymore pleasing melodies out there  but thank Halstead & co  for proving me wrong  I am happily pleased with their latest  and very eagerly awaited  effort  Instead of the depths of melancholia and loveliness on  say  OUT OF TUNE  they give us     minutes of what seems like basically   it s good  everything s going just way OK      It s happy  Maybe I ll hear a lyric later that says something else  but for those for whom music is the priority  you can t go wrong with this considered and experienced and artful pop  Four and a half stars \n",
            "Actual Label:  positive\n",
            "Prediction:  negative\n",
            "X_pred (Before Sigmoid):  [[-1.60168766]]\n",
            "X_pred (After Sigmoid):  [[0.16774587]]\n",
            "Neuron Analysis--------------------\n",
            "POSITIVE NEURON ANALYSIS--------\n",
            "Text,   It s sometimes hard to think there are really anymore pleasing melodies out there  but thank Halstead & co  for proving me wrong  I am happily pleased with their latest  and very eagerly awaited  effort  Instead of the depths of melancholia and loveliness on  say  OUT OF TUNE  they give us     minutes of what seems like basically   it s good  everything s going just way OK      It s happy  Maybe I ll hear a lyric later that says something else  but for those for whom music is the priority  you can t go wrong with this considered and experienced and artful pop  Four and a half stars \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['Excellent', 'Highly', 'awesome', 'Great', 'love']\n",
            "Top words in sentence ['love', 'pleased', 'happy', 'with', 'con']\n",
            "Neuron:  19\n",
            "Output Weight 0.4233351431859908\n",
            "Contribution 0.6644344172138104\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Great', 'pleased', 'Highly', 'amazing']\n",
            "Top words in sentence ['pleased', 'love', 'for', 'happy', 'Four']\n",
            "Neuron:  13\n",
            "Output Weight 0.33258196211889784\n",
            "Contribution 0.5512568678486219\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Great', 'Highly', 'awesome', 'delicious']\n",
            "Top words in sentence ['pleased', 'love', 'con', 'thank', 'ever']\n",
            "Neuron:  10\n",
            "Output Weight 0.2458448156168154\n",
            "Contribution 0.4385026350709453\n",
            "---------\n",
            "Top words for the neuron ['Great', 'Excellent', 'pleased', 'delicious', 'Highly']\n",
            "Top words in sentence ['pleased', 'love', 'con', 'art', 'happy']\n",
            "Neuron:  14\n",
            "Output Weight 0.3837929761255051\n",
            "Contribution 0.39983970689993836\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'excellent', 'Great', 'GREAT']\n",
            "Top words in sentence ['love', 'con', 'pleased', 'art', 'good']\n",
            "Neuron:  17\n",
            "Output Weight 0.23682959082802094\n",
            "Contribution 0.39170094330107375\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'awesome', 'delicious', 'Great']\n",
            "Top words in sentence ['pleased', 'love', 'con', 'happy', 'thank']\n",
            "Neuron:  31\n",
            "Output Weight 0.1526518469992634\n",
            "Contribution 0.282590540213422\n",
            "---------\n",
            "NEGATIVE NEURON ANALYSIS -------------\n",
            "Text,   It s sometimes hard to think there are really anymore pleasing melodies out there  but thank Halstead & co  for proving me wrong  I am happily pleased with their latest  and very eagerly awaited  effort  Instead of the depths of melancholia and loveliness on  say  OUT OF TUNE  they give us     minutes of what seems like basically   it s good  everything s going just way OK      It s happy  Maybe I ll hear a lyric later that says something else  but for those for whom music is the priority  you can t go wrong with this considered and experienced and artful pop  Four and a half stars \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'worst', 'horrible']\n",
            "Top words in sentence ['Maybe', 'Instead', 'for', 'like', 'very']\n",
            "Neuron:  7\n",
            "Output Weight 0.6612553408002649\n",
            "Contribution -1.2411131001832152\n",
            "---------\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'returning', 'disappointment']\n",
            "Top words in sentence ['Maybe', 'for', 'Instead', 'very', 'real']\n",
            "Neuron:  9\n",
            "Output Weight 0.5326098457241927\n",
            "Contribution -0.770690780368874\n",
            "---------\n",
            "Top words for the neuron ['boring', 'worst', 'terrible', 'useless', 'horrible']\n",
            "Top words in sentence ['Maybe', 'for', 'with', 'basically', 'music']\n",
            "Neuron:  4\n",
            "Output Weight 0.5335758942533058\n",
            "Contribution -0.7470155203322106\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'worst', 'waste', 'returning']\n",
            "Top words in sentence ['Instead', 'Maybe', 'for', 'thin', 'basically']\n",
            "Neuron:  15\n",
            "Output Weight 0.3870948725029518\n",
            "Contribution -0.5171941551533852\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'waste', 'worst', 'awful']\n",
            "Top words in sentence ['Maybe', 'like', 'very', 'thin', 'music']\n",
            "Neuron:  26\n",
            "Output Weight 0.36029744636334476\n",
            "Contribution -0.42431026138068545\n",
            "---------\n",
            "Top words for the neuron ['terrible', 'boring', 'waste', 'useless', 'worst']\n",
            "Top words in sentence ['Instead', 'for', 'Maybe', 'basically', 'music']\n",
            "Neuron:  5\n",
            "Output Weight 0.34276415127036053\n",
            "Contribution -0.3876110519410086\n",
            "---------\n",
            "-1.6016876605664343\n",
            "0.1677458731759314\n"
          ]
        }
      ],
      "source": [
        "perform_MLP_analysis(both_wrong_indices[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perform forward pass for Logistic Regression --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Review Text  It s sometimes hard to think there are really anymore pleasing melodies out there  but thank Halstead & co  for proving me wrong  I am happily pleased with their latest  and very eagerly awaited  effort  Instead of the depths of melancholia and loveliness on  say  OUT OF TUNE  they give us     minutes of what seems like basically   it s good  everything s going just way OK      It s happy  Maybe I ll hear a lyric later that says something else  but for those for whom music is the priority  you can t go wrong with this considered and experienced and artful pop  Four and a half stars \n",
            "Actual Prediction positive\n",
            "negative\n",
            "Prediction Numbers\n",
            "[-1.24907646]\n",
            "Performing weight analysis --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence  It s sometimes hard to think there are really anymore pleasing melodies out there  but thank Halstead & co  for proving me wrong  I am happily pleased with their latest  and very eagerly awaited  effort  Instead of the depths of melancholia and loveliness on  say  OUT OF TUNE  they give us     minutes of what seems like basically   it s good  everything s going just way OK      It s happy  Maybe I ll hear a lyric later that says something else  but for those for whom music is the priority  you can t go wrong with this considered and experienced and artful pop  Four and a half stars \n",
            "Word Contributing: doesn\n",
            "Weight: -0.9161962888989316\n",
            "Word Contributing: work\n",
            "Weight: -0.7771918656722246\n",
            "Word Contributing: was\n",
            "Weight: -0.6031317568010689\n",
            "Word Contributing: has\n",
            "Weight: -0.5299125968118237\n",
            "Word Contributing: but\n",
            "Weight: -0.5192541915843839\n",
            "Word Contributing: months\n",
            "Weight: -0.2984387769120629\n",
            "Word Contributing: sometimes\n",
            "Weight: -0.19865760174094216\n",
            "Word Contributing: connection\n",
            "Weight: -0.1670219832921045\n",
            "Word Contributing: owned\n",
            "Weight: -0.10006883641684997\n",
            "Word Contributing: ago\n",
            "Weight: -0.06037673622784771\n",
            "Word Contributing: very\n",
            "Weight: -0.04685586246418355\n",
            "Word Contributing: looking\n",
            "Weight: -0.0454463482740979\n",
            "Word Contributing: time\n",
            "Weight: -0.0276710668857968\n",
            "Word Contributing: friendly\n",
            "Weight: -0.01092145090881118\n",
            "Word Contributing: Yeah\n",
            "Weight: 0.2094167986594842\n",
            "Word Contributing: frustrated\n",
            "Weight: 0.3301907476762957\n",
            "Word Contributing: user\n",
            "Weight: 0.34868381561854195\n",
            "Word Contributing: good\n",
            "Weight: 0.479759117875591\n",
            "Word Contributing: well\n",
            "Weight: 0.5001897808514062\n",
            "Word Contributing: bluetooth\n",
            "Weight: 0.6550814052354015\n",
            "Word Contributing: Samsung\n",
            "Weight: 0.8972182763477509\n",
            "Sum of weights: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sum of weights from decomposition: [-0.84662189]\n"
          ]
        }
      ],
      "source": [
        "perform_LogReg_analysis(both_wrong_indices[2])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
