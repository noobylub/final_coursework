{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobylub/final_coursework/blob/main/coursework_data_import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqwi4b5geG6"
      },
      "source": [
        "**Experiment**\n",
        "<br>\n",
        "This research seeks to compare One-Hot-Encoding against Multi-Layer Perceptron, which has be de-facto for many ML problems. \n",
        "<br>\n",
        "The following experiement will be performed and evaluated: \n",
        "\n",
        "*   One hot encoding (OHE), sigmoid\n",
        "*   Multi Layer Perceptron (MLP), sigmoid\n",
        "*   OHE, softmax  \n",
        "*   MLP, softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the Data and Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhNWdx7l84yP",
        "outputId": "8563f28d-5f96-41bb-c5b6-4a9adba9bb8e"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 21.2M  100 21.2M    0     0  5163k      0  0:00:04  0:00:03  0:00:01 5162k   0     0  5649k      0  0:00:03  0:00:03 --:--:-- 5648k\n"
          ]
        }
      ],
      "source": [
        "# Run this when editing in code editor \n",
        "!curl -O https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YHzNVVHQPd9",
        "outputId": "bf37b091-809e-4b00-f8c3-83c688764170"
      },
      "outputs": [],
      "source": [
        "# Example of what the data looks like\n",
        "!head -n5 Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_eYghCK5OX"
      },
      "source": [
        "**Data loading and pre-processing**\n",
        "<br>\n",
        "Below we preprocess the data from the raw file \"Compiled_reviews.txt\"\n",
        "<br>\n",
        "We remove any unwanted characters, to improve the integrity of the text corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Li-IcrXi9O8G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reviews=[]\n",
        "sentiment_ratings=[]\n",
        "product_types=[]\n",
        "helpfulness_ratings=[]\n",
        "\n",
        "with open(\"Compiled_Reviews.txt\") as f:\n",
        "   for line in f.readlines()[1:]:\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # remove punctuation/numbers and replace it with a space\n",
        "        fields[0] = re.sub(r'[.,!?;:()\\[\\]{}\\-â€”\\'\\/\\\"\\\"\\d+]', \" \",fields[0])\n",
        "        reviews.append(fields[0])\n",
        "        sentiment_ratings.append(fields[1])\n",
        "        \n",
        "        helpfulness_ratings.append(fields[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qV1XhmNXI4"
      },
      "source": [
        "**Data Analysis**\n",
        "<br/>\n",
        "Below we see what the data looks like after pre-processing\n",
        "<br/>\n",
        "Data analysis can also be shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMAE3UTkK-jO",
        "outputId": "6c0af6b2-c7fe-43d7-e8a7-87965c784bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is   This is a wonderful album  that evokes memories of the    s folk boom  yet contains original songs  I was amazed at the fantastic harmonies and musical arrangements Anyone who loves the movie   A Mighty Wind   and who loves folk music will fall in love with this album  I know I did \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   On one hand  this CD is a straight ahead instrumental rocker  but Johnny A really shows how great he is with ballads  such as his covers of   Wichita Lineman    and   Yes it Is     In fact  those two ballads alone are worth the price of the CD by themselves But Johnny A can flat kick your ass  too   He s a biker and his tunes like Oh Yeah  In the Wind and Two Wheel Horse are named for his other hobby   And they rock  but there s nothing cliched or tired in his style   He always seems to be looking for new ways to say something I saw him in person at the Triple Door in Seattle sometime in February      in a power trio format and he played most of the tunes on this album   The guy is one amazing guitar player   He played his signature Gibson hollow body  fitted with a vibrato tailpiece  Bigsby  It was like the old Chet Atkins   country gentleman   model  and he utilized a battery of foot pedal effects  coming eventually through a pair of Marshall combo amps   The guy had some of the best clean tones I ve ever heard from anyone  anywhere Basically  Johnny A is a guitarist who has complete command of the instrument   And he s got a rocking soul that cuts loose on originals and covers alike in a style that s all his own  and that s saying something these days   If you love great rocking rockabilly guitar  combined with really cool ballad playing in the power trio format  this CD is just what you re looking for   In fact  I guarantee you ll be knocked out Five stars  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is   this band reminds me of the thrill i first got when i listened to an Atreyu Album  It dies today rip off the former bands style  but they are still a very good band   In the over crowded metalcore market of today  that is a rarity   My only complaint  is that the vocalist has a beautiful singing voice  as heard on   The Radiance   but more often then not goes for the screams and growls that are associated with this type of music  still     material though Favorite songs    The Radiance      Freak Gasoline Fight      Our Disintigration    and  THe Caitliff Choir Defeatism \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Like I said I would  I finally got around to purchasing this CD  I especially like tracks       Cheap Trick is solid as always  I have been a long time fan and enjoy all their releases  If I do have any criticism at all  it is the fact that some songs sound like recycled Trick riffs  I guess it s hard to create new stuff after     years  Anyway  there are some great tunes on this  Buy it like I did  \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Ok good CD  im not suprised  Ok jaheim may not have the b best voice but his music is good and it goes will with the voice that he has   Yes yall this album does use profanity but the songs actually have meanings  I like that daddy thing song  and like a dj  This album looks at issues from a mans  point of view  Ya know we ve heard the angry woman and how the man did t his and that now jaheim looks at it from a males view  Ok yall i know he is not the first one to do this but he did it well  Also this is a break from that       that is played on the radio  \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   Review by Mike Watson With the recent slew of Victory Records releases  not to mention the wide variety  one can be to say the least skeptical of any of their assortment of new releases   However  picking through all of the less notable releases from the label  the up and coming band  The Forecast  have stepped to the plate and scored a home run on Late Night Conversations   The Forecast are a perfect blend of male and female vocals over a very modern rock soundtrack  executed flawlessly over a ten song full length Late Night Conversations is an album confronting different topics ranging from relationships to the teen trend of mutilation and loathing   The harmonies arranged by both vocalists are undeniably catching and well written and the music follows closely in suit leaving your head bobbing back and forth to the beat and your feet tapping almost involuntarily  The Forecast is not only a great recorded band  but bring the same toe tapping energy to the stage  with both great stage presence and cohesion as a band   The Forecast will defiantly be one of the new big bands in poppier rock music  and checking their Victory Records debut out is a must for fans of the genre  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is  great anniversary present  fav song    I love the way you love me    my husband loves to listen to it in the car\n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "index = 0\n",
        "import re\n",
        "from collections import Counter\n",
        "for index in range(len(reviews)):\n",
        "  print(\"Review is \",reviews[index])\n",
        "  print(\"Sentiment \", sentiment_ratings[index])\n",
        "  print(\"Helpfullness is \", helpfulness_ratings[index])\n",
        "  print(\"-----------\")\n",
        "  if(index >5):\n",
        "    break;\n",
        "# helpfulness_ratings[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment distribution:\n",
            "positive: 20972 (57.38%)\n",
            "negative: 15576 (42.62%)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentiment_distribution = Counter(sentiment_ratings)\n",
        "total_data = len(sentiment_ratings)\n",
        "\n",
        "print(\"Sentiment distribution:\")\n",
        "for sentiment, count in sentiment_distribution.items():\n",
        "    percentage = (count / total_data) * 100\n",
        "    print(f\"{sentiment}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(sentiment_distribution.keys(), sentiment_distribution.values())\n",
        "plt.xlabel('Helpfulness Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution in Full Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class neutral: 10321 (28.24%)\n",
            "Class helpful: 20351 (55.68%)\n",
            "Class unhelpful: 5876 (16.08%)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Class Distribution Analysis \n",
        "class_distribution = Counter(helpfulness_ratings)\n",
        "total_data = len(helpfulness_ratings)\n",
        "\n",
        "\n",
        "for rating, count in class_distribution.items():\n",
        "    percentage = (count / total_data) * 100\n",
        "    print(f\"Class {rating}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(class_distribution.keys(), class_distribution.values())\n",
        "plt.xlabel('Helpfulness Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution in Full Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Fc8KrpQhYV"
      },
      "source": [
        "**Tokenising Corpus Dataset**\n",
        "<br/>\n",
        "Tokenising the words all the words in the reviews database. This will then be used to create the list of features, which would be the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TjHzxsgNNLH4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenising it by spaces\n",
        "tokenised_set = []\n",
        "for review in reviews:\n",
        "  # Basically, re.split(' ') results in an array of words split by spaces\n",
        "  # Then iterate through that array of words and append it individually to tokenised_set\n",
        "  [tokenised_set.append(tokens) for tokens in re.split(' ', review)]\n",
        "\n",
        "counts = Counter(tokenised_set)\n",
        "so_with_content=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "so_with_content=list(zip(*so_with_content))[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We want to focus on content words\n",
        "function_words = []\n",
        "with open(\"function_words.txt\") as f:\n",
        "    function_words = f.read().splitlines()\n",
        "so: list[str] = []\n",
        "for word in so_with_content:\n",
        "    if word not in function_words and len(word) > 2:\n",
        "        so.append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Creating the sparse embedding**\n",
        "<br/>\n",
        "This is a simple sparse embedding of filtered content words of corpus. This will be used as the main features for all classification model. \n",
        "\n",
        "**Ensuring Reproducibility**\n",
        "<br/>\n",
        "To ensure reproducibility, we will set the random seed to 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# 8000 Features\n",
        "word_list = so[0:8000]\n",
        "M = np.zeros((len(reviews), len(word_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "  for(j,word) in enumerate(word_list):\n",
        "    if word in rev:\n",
        "      M[i,j]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36548, 8000)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 36548 examples, with 8000 features (words occurence)\n",
        "M.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ints = np.random.choice(len(reviews), int(len(reviews)*0.6), replace=False)\n",
        "test_train_ints = list(set(range(0, len(reviews))) - set(train_ints))\n",
        "test_ints = np.random.choice(test_train_ints, int(len(test_train_ints)*0.5), replace=False)\n",
        "final_test_ints = list(set(test_train_ints) - set(test_ints))  # Use test_train_ints, not range()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training test 21928\n",
            "Validation test 7310\n",
            "Final test 7310\n",
            "Total 36548\n"
          ]
        }
      ],
      "source": [
        "print(\"Training test\", len(train_ints))\n",
        "print(\"Validation test\", len(test_ints))\n",
        "print(\"Final test\", len(final_test_ints))\n",
        "print(\"Total\", len(train_ints)+len(test_ints)+len(final_test_ints))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-EDhKyxxtMl"
      },
      "source": [
        "**Classifiers for Sentiment Analysis**\n",
        "<br>\n",
        "Multiple classifiers will be run, and tested againts each other for sentiment analysis classifier.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yW_OPROzSFl"
      },
      "source": [
        "Experiment for 8000 features, meaning 8000 words\n",
        "<br>\n",
        "Below we divide the data into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "D5zci15pCO7t"
      },
      "outputs": [],
      "source": [
        "# Divide the features by the training indices\n",
        "# Select all rows that are in the indices of the respective lists and select all the rows\n",
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "M_final_test = M[final_test_ints,]\n",
        "sentiment_labels = [sentiment_ratings[i] for i in train_ints]\n",
        "sentiment_labels_test = [sentiment_ratings[i] for i in test_ints]\n",
        "sentiment_labels_final_test = [sentiment_ratings[i] for i in final_test_ints]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'positive': 20972, 'negative': 15576})\n",
            "Counter({'positive': 4160, 'negative': 3150})\n",
            "Counter({'positive': 4154, 'negative': 3156})\n",
            "Counter({'positive': 12658, 'negative': 9270})\n"
          ]
        }
      ],
      "source": [
        "# Class Distribution Test to check\n",
        "class_distribution = Counter(sentiment_ratings)\n",
        "class_distribution_test = Counter(sentiment_labels_test)\n",
        "class_distribution_final_test = Counter(sentiment_labels_final_test)\n",
        "class_distribution_train = Counter(sentiment_labels)\n",
        "\n",
        "print(class_distribution)\n",
        "print(class_distribution_test)\n",
        "print(class_distribution_final_test)\n",
        "print(class_distribution_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUfBV2eCHOeK",
        "outputId": "dad11401-36e1-4801-85f1-7b57602092fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21928, 8000)\n",
            "(7310, 8000)\n",
            "(7310, 8000)\n",
            "21928\n",
            "7310\n",
            "7310\n"
          ]
        }
      ],
      "source": [
        "print(M_train.shape)\n",
        "print(M_test.shape)\n",
        "print(M_final_test.shape)\n",
        "# Sentiment Labels are ordered list\n",
        "print(len(sentiment_labels))\n",
        "print(len(sentiment_labels_test))\n",
        "print(len(sentiment_labels_final_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Function for Classifiers**\n",
        "<br/>\n",
        "This function will be called multiple times to test out different hyperparameters and their effects. \n",
        "<br/>\n",
        "In each training, the function will perform a test on the sentiment_labels_test after finishing training on sentiment_labels_train.\n",
        "<br/>\n",
        "The best performing models will be tested further with the last 20 percent to test whether they generalise well or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run Logistic Regression with Sigmoid\n",
        "def logistic_regresssion_sigmoid(x_dataset,y_dataset,x_test,y_test,num_features,n_iters=1000,lr=0.4,random_seed=42):\n",
        "\n",
        "    weights = np.random.rand(num_features)\n",
        "    bias=np.random.rand(1)\n",
        "    logistic_loss=[]\n",
        "    num_samples=len(y_dataset)\n",
        "    for i in range(n_iters):\n",
        "        # Basically you are multiplying all the values of M_train with the weights\n",
        "        # It would be similar to this: z= bias + (x[0]*weights[0] + x[1]*weights[1])\n",
        "        # The values here would be 21928, 5000 and 5000, 1, leading to a matrix of 21928, 1\n",
        "        z= x_dataset.dot(weights) + bias\n",
        "        # print(z)\n",
        "        # (1 / (1+np.exp(-z))) we use sigmoid because we only need to know whether it is positive or negative, two possible values\n",
        "        q = (1 / (1+np.exp(-z)))\n",
        "        # print(q)\n",
        "        eps=0.00001\n",
        "        # Binary Cross Entropy Loss\n",
        "        loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))/num_samples\n",
        "        \n",
        "        \n",
        "        logistic_loss.append(loss)\n",
        "        # print(logistic_loss)\n",
        "        # We then make the prediction, if it is below a certain number, 0.5 it is negative and vice versa\n",
        "        y_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "        # For logistic regression one shot encoder= dw1 = np.dot(x[0],q-y)/num_samples\n",
        "        # dw1 = np.do(x[0], q-y)/num_samples\n",
        "\n",
        "        dw = (q-y_dataset).dot(x_dataset)/num_samples\n",
        "        db = sum(q-y_dataset)/num_samples\n",
        "        weights = weights - dw*lr\n",
        "        bias = bias - db*lr\n",
        "\n",
        "    \n",
        "    # Model test on validation dataset \n",
        "    result = LogReg_Sigmoid_Test(weights,bias,x_test,y_test)\n",
        "    return weights,bias, result, logistic_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to invoke the function and test results \n",
        "# For this specific function, we expect the labels to be already in string format\n",
        "def LogReg_Sigmoid_Test(weights,bias,test_dataset,y_test:list):\n",
        "    # Perform Forward Propagation\n",
        "    z= test_dataset.dot(weights) + bias\n",
        "    q = (1 / (1+np.exp(-z)))\n",
        "    x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "    \n",
        "    \n",
        "\n",
        "    # # Accuracy\n",
        "    # y_final_test = [int(l=='positive') for l in sentiment_labels_final_test]\n",
        "    # acc_test = [int(yp == y_final_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "    # print(\"Final Test Accuracy: \", sum(acc_test)/len(acc_test))\n",
        "    # y_test_compare = [\"positive\" if s == 1 else \"negative\" for s in y_test ]\n",
        "    x_labels=[\"positive\" if s == 1 else \"negative\" for s in x_test_pred]\n",
        "    \n",
        "    # TP\n",
        "    true_positives=sum([int(yt == \"positive\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "\n",
        "    true_negatives=sum([int(yt == \"negative\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    false_positives = sum([int(yt == \"negative\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "    # FN: actual is POSITIVE, predicted is NEGATIVE  \n",
        "    false_negatives = sum([int(yt == \"positive\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    \n",
        "    # print(\"True Positives: \", true_positives)\n",
        "    # print(\"False Positives: \", false_positives)\n",
        "    # print(\"False Negatives: \", false_negatives)\n",
        "    # print(\"True Negatives: \", true_negatives)\n",
        "    # Precision\n",
        "    accuracy = (true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
        "    precision = true_positives/(true_positives+false_positives)\n",
        "    recall = true_positives/(true_positives+false_negatives)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(\"--------------\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1: \", f1)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "    return precision, recall, f1, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Different Features forLogistic Regression with Logistic Regression and MLP**\n",
        "<br/>\n",
        "- 8000 features \n",
        "- Learning Rate \n",
        "    - 0.4\n",
        "    - 0.2\n",
        "    - 0.1\n",
        "    - 0.05\n",
        "    <br/>\n",
        "- This will all be done by running multiple classifiers and storing the results in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of hyperparameters to be used for training the model\n",
        "learning_rates = [0.4,0.2,0.1,0.05]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.4\n",
            "--------------\n",
            "Precision:  0.7965184662432369\n",
            "Recall:  0.8139423076923077\n",
            "F1:  0.8051361312566876\n",
            "accuracy:  0.7757865937072503\n",
            "-----\n",
            "Learning rate:  0.2\n",
            "--------------\n",
            "Precision:  0.7635906806761078\n",
            "Recall:  0.8036057692307692\n",
            "F1:  0.7830873740922932\n",
            "accuracy:  0.7466484268125855\n",
            "-----\n",
            "Learning rate:  0.1\n",
            "--------------\n",
            "Precision:  0.738289443020275\n",
            "Recall:  0.7615384615384615\n",
            "F1:  0.7497337593184238\n",
            "accuracy:  0.7106703146374829\n",
            "-----\n",
            "Learning rate:  0.05\n",
            "--------------\n",
            "Precision:  0.7034883720930233\n",
            "Recall:  0.6689903846153846\n",
            "F1:  0.6858058156727452\n",
            "accuracy:  0.6511627906976745\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# invoking the function\n",
        "y=[int(l == \"positive\") for l in sentiment_labels]\n",
        "LogReg_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = logistic_regresssion_sigmoid(M_train,y,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    LogReg_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Result for One Hot Encoding**\n",
        "<br/>\n",
        "The results will include F1 score, precision, recall, and accuracy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MLP_sigmoid(x_dataset,y_dataset,x_test,y_test,num_features,n_iters=1000,lr=0.4):\n",
        "    \n",
        "\n",
        "    num_features= M_train.shape[1]\n",
        "    hidden_size = 32\n",
        "\n",
        "    \n",
        "    \n",
        "    # Weight initialization with He/Xavier weight initialization technique to encourage ReLU activation\n",
        "    # Np.random will not work\n",
        "    limit_1 = np.sqrt(6 / (num_features ))\n",
        "    weights_0_1 = np.random.uniform(-limit_1, limit_1, (num_features, hidden_size))\n",
        "    limit_2 = np.sqrt(6 / (hidden_size ))\n",
        "    weights_1_2 = np.random.uniform(-limit_2, limit_2, (hidden_size,1))\n",
        "\n",
        "\n",
        "    loss_history = []\n",
        "    \n",
        "    N = x_dataset.shape[0] # Number of training samples\n",
        "\n",
        "    for iteration in range(n_iters):\n",
        "\n",
        "        layer_2_error = 0\n",
        "        layer_0 = x_dataset\n",
        "\n",
        "        ## Add forward pass\n",
        "        layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "    \n",
        "\n",
        "\n",
        "        # Then apply sigmoid\n",
        "        layer_2_s = 1/(1+np.exp(-layer_2))\n",
        "\n",
        "    \n",
        "        eps = 1e-8\n",
        "        q = np.clip(layer_2_s, eps, 1 - eps)\n",
        "        # BCE Cross Entropy Loss with clipping\n",
        "        loss = (-np.sum(true_labels * np.log2(q) + (1 - true_labels) * np.log2(1 - q)))/N\n",
        "        loss_history.append(loss)\n",
        "        \n",
        "\n",
        "        ## Add backward pass and update weights\n",
        "        layer_2_diff = (layer_2_s - y_dataset)\n",
        "\n",
        "        z1 = np.dot(layer_0, weights_0_1)\n",
        "        relu_grad = (z1 > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "        hidden_delta = np.dot(layer_2_diff, weights_1_2.T) * relu_grad\n",
        "\n",
        "        # Normalize weight updates by N\n",
        "        weights_1_2 -= lr * (np.dot(layer_1.T, layer_2_diff) / N)\n",
        "        weights_0_1 -= lr * (np.dot(layer_0.T, hidden_delta) / N)\n",
        "\n",
        "    result = test_models_MLP_Sigmoid(weights_0_1,weights_1_2,x_test,y_test)\n",
        "    return weights_0_1,weights_1_2,result, loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to invoke the function and test results \n",
        "# For this specific function, we expect the labels to be already in string format\n",
        "def test_models_MLP_Sigmoid(weights_MLP_1,weights_MLP_2,x_dataset,y_test:list):\n",
        "    # Forward propagation\n",
        "    layer_0_final_test = x_dataset\n",
        "    layer_1_final_test = np.maximum(np.dot(layer_0_final_test, weights_MLP_1), 0)\n",
        "    layer_2_final_test = np.dot(layer_1_final_test, weights_MLP_2)\n",
        "    layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "    #Converting probabilities\n",
        "    x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "\n",
        "    \n",
        "\n",
        "    # # Accuracy\n",
        "    x_labels=[\"positive\" if s == 1 else \"negative\" for s in x_pred]\n",
        "    \n",
        "    # TP\n",
        "    true_positives=sum([int(yt == \"positive\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "\n",
        "    true_negatives=sum([int(yt == \"negative\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    false_positives = sum([int(yt == \"negative\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    # FN: actual is POSITIVE, predicted is NEGATIVE  \n",
        "    false_negatives = sum([int(yt == \"positive\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Precision\n",
        "    accuracy = (true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
        "    precision = true_positives/(true_positives+false_positives)\n",
        "    recall = true_positives/(true_positives+false_negatives)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(\"----------\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1: \", f1)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "    return precision, recall, f1, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*******\n",
            "Learning rate:  0.4\n",
            "----------\n",
            "Precision:  0.8202294942626435\n",
            "Recall:  0.9278846153846154\n",
            "F1:  0.8707421610647417\n",
            "accuracy:  0.8432284541723666\n",
            "-----\n",
            "*******\n",
            "Learning rate:  0.2\n",
            "----------\n",
            "Precision:  0.8611306591602158\n",
            "Recall:  0.8824519230769231\n",
            "F1:  0.8716609284103051\n",
            "accuracy:  0.8521203830369357\n",
            "-----\n",
            "*******\n",
            "Learning rate:  0.1\n",
            "----------\n",
            "Precision:  0.8573099415204678\n",
            "Recall:  0.8810096153846154\n",
            "F1:  0.8689982216953172\n",
            "accuracy:  0.8488372093023255\n",
            "-----\n",
            "*******\n",
            "Learning rate:  0.05\n",
            "----------\n",
            "Precision:  0.8503147586850082\n",
            "Recall:  0.8766826923076924\n",
            "F1:  0.8632974316487159\n",
            "accuracy:  0.8419972640218878\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).reshape(-1,1)\n",
        "# invoking the function\n",
        "MLP_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"*******\")\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = MLP_sigmoid(M_train,true_labels,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    MLP_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpfqtooD0JFV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Classification for Helpfulness** \n",
        "<br>\n",
        "- Logistic Regression \n",
        "- Multi-Layer Perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Divide the Helpfullness Y into 3 classes: 0, 1, and 2, and to three sets: train, validation, and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort to ensure consistent ordering\n",
        "unique_labels = sorted(list(set(helpfulness_ratings)))\n",
        "unique_one_hot = np.diag(np.ones(len(unique_labels)))\n",
        "\n",
        "\n",
        "labels_train = [helpfulness_ratings[i] for i in train_ints]\n",
        "labels_test = [helpfulness_ratings[i] for i in test_ints]\n",
        "labels_dev = [helpfulness_ratings[i] for i in final_test_ints]\n",
        "\n",
        "# Make sure you're using the right label variables\n",
        "y_train = np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_train]]).T\n",
        "y_test = np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_test]]).T\n",
        "y_dev = np.array([list(unique_one_hot[k]) for k in [unique_labels.index(x) for x in labels_dev]]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'helpful': 12170, 'neutral': 6218, 'unhelpful': 3540})\n",
            "Counter({'helpful': 4078, 'neutral': 2080, 'unhelpful': 1152})\n",
            "Counter({'helpful': 4103, 'neutral': 2023, 'unhelpful': 1184})\n"
          ]
        }
      ],
      "source": [
        "# Dataset example \n",
        "train_distribution = Counter(labels_train)\n",
        "print(train_distribution)\n",
        "test_distribution = Counter(labels_test)\n",
        "print(test_distribution)\n",
        "dev_distribution = Counter(labels_dev)\n",
        "print(dev_distribution)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 21928)\n",
            "(3, 7310)\n",
            "(3, 7310)\n"
          ]
        }
      ],
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_dev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LogisticRegression_Softmax(x_train_data, y_train_data,x_test,y_test,lr =0.1,n_iters=1000,random_seed=42):\n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "\n",
        "    num_features = x_train_data.shape[1]\n",
        "    num_classes = y_train_data.shape[0]\n",
        "    num_samples = x_train_data.shape[0]\n",
        "\n",
        "    # Initialize weights with correct dimensions: (num_features, num_classes)\n",
        "    weights = np.random.rand(num_features, num_classes) # (8000,3)\n",
        "    bias = np.zeros(num_classes) # (3,)\n",
        "    \n",
        "    logistic_loss=[]\n",
        "\n",
        "\n",
        "\n",
        "    # # x and y now refer to the training data for intent classification\n",
        "    x_train_data = x_train_data  # Shape (num_samples, num_features) (21928, 8000)\n",
        "    y_train_targets = y_train_data.T # Shape (num_samples, num_classes) (3, 21928)\n",
        "    # print(x_train_data.shape)\n",
        "    # print(y_train_targets.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(n_iters):\n",
        "\n",
        "        # z = x.dot(weights) expects x to be (num_samples, num_features) and weights to be (num_features, num_classes)\n",
        "        z= x_train_data.dot(weights) + bias\n",
        "        \n",
        "        # Softmax \n",
        "        exp_z = np.exp(z)\n",
        "        q = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "        # exp_z = np.exp(z)\n",
        "        # q = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "        \n",
        "        \n",
        "        # Calculate loss using y_train_targets\n",
        "        eps = 1e-12\n",
        "        # Softmax cross entropy loss with clipping to prevent log(0)\n",
        "        loss = (-np.sum(y_train_targets*np.log2(np.clip(q, eps, 1.0)))) / num_samples # avoid log(0)\n",
        "    \n",
        "        logistic_loss.append(loss)\n",
        "\n",
        "        # dw = x.T.dot((q-y)) expects x.T to be (num_features, num_samples) and (q-y) to be (num_samples, num_classes)\n",
        "        # Here x_train_data.T is (num_features, num_samples)\n",
        "        db = np.sum((q-y_train_targets), axis=0)/num_samples\n",
        "        dw=x_train_data.T.dot((q-y_train_targets))/num_samples\n",
        "        weights=(weights - (dw*lr))\n",
        "        bias=(bias - (db*lr))\n",
        "    result = LogReg_Softmax_Test(weights,bias,x_test,y_test)\n",
        "    return weights,bias,result,logistic_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "def LogReg_Softmax_Test(weights,bias,x_dataset,y_dataset):\n",
        "    # Forward pass on test data\n",
        "    z_test = x_dataset.dot(weights) + bias\n",
        "    # Perform theSoftmax\n",
        "    exp_z_test = np.exp(z_test)\n",
        "    q_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "    # Get predictions, basically get the class with highest probability\n",
        "    y_pred_test = np.argmax(q_test, axis=1)  # Shape: (7310,)\n",
        "\n",
        "    # Get true labels\n",
        "    y_test_targets = y_dataset.T  \n",
        "    y_true_test = np.argmax(y_test_targets, axis=1)  # Shape: (7310,)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy_test = np.mean(y_pred_test == y_true_test)\n",
        "    print(f\"Test Accuracy: {accuracy_test * 100:.2f}%\")\n",
        "    TP = []\n",
        "    FP = []\n",
        "    FN = []\n",
        "    for j in range(3):\n",
        "        TP.append(np.sum(np.array([int(s == j and y_true_test[i] == j) for i,s in enumerate(y_pred_test)])))\n",
        "        FP.append(np.sum(np.array([int(s == j and y_true_test[i] != j) for i,s in enumerate(y_pred_test)])))\n",
        "        FN.append(np.sum(np.array([int(s != j and y_true_test[i] == j) for i,s in enumerate(y_pred_test)])))\n",
        "    \n",
        "    precision = np.array(TP)/(np.array(TP)+np.array(FP))\n",
        "    recall = np.array(TP)/(np.array(TP)+np.array(FN))\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(precision)\n",
        "    print(recall)\n",
        "    print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.4\n",
            "Test Accuracy: 52.38%\n",
            "[0.61472419 0.36960179 0.35253456]\n",
            "[0.70230505 0.31682692 0.265625  ]\n",
            "[0.65560261 0.34118561 0.3029703 ]\n",
            "-----\n",
            "Learning rate:  0.2\n",
            "Test Accuracy: 51.11%\n",
            "[0.60481617 0.35097493 0.33912037]\n",
            "[0.68979892 0.30288462 0.25434028]\n",
            "[0.64451827 0.32516129 0.2906746 ]\n",
            "-----\n",
            "Learning rate:  0.1\n",
            "Test Accuracy: 49.71%\n",
            "[0.59649891 0.32845528 0.33743017]\n",
            "[0.66846493 0.29134615 0.26215278]\n",
            "[0.63043478 0.30878981 0.29506595]\n",
            "-----\n",
            "Learning rate:  0.05\n",
            "Test Accuracy: 48.39%\n",
            "[0.58892064 0.31626667 0.29909707]\n",
            "[0.65693968 0.28509615 0.23003472]\n",
            "[0.62107337 0.29987358 0.26005888]\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "\n",
        "LogReg_models_softmax = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,loss = LogisticRegression_Softmax(M_train,y_train,M_test,y_test,lr=lr) \n",
        "    print(\"-----\")\n",
        "    LogReg_models_softmax[lr] = (weights,bias,result,loss)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MultiLayer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MLP_softmax(x_train,y_train,x_test,y_test,lr=0.1,hidden_size=16,n_iters=1000,):\n",
        "    # Add more layers for hte multilayer perceptron\n",
        "    num_features= x_train.shape[1]\n",
        "    num_classes = 3\n",
        "\n",
        "    helpfulness_labels = y_train # 3,2198\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Intitiliasation method based on Xavier's proposed method \n",
        "    # https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf#page=4.98\n",
        "    limit_1 = np.sqrt(6 / (num_features ))\n",
        "    weights_0_1 = np.random.uniform(-limit_1, limit_1, (num_features, hidden_size))\n",
        "    limit_2 = np.sqrt(6 / (hidden_size ))\n",
        "    weights_1_2 = np.random.uniform(-limit_2, limit_2, (hidden_size,num_classes))\n",
        "    y_train_target = y_train.T\n",
        "\n",
        "    loss_history = []\n",
        "    \n",
        "    num_samples = x_train.shape[0] # Number of training samples\n",
        "\n",
        "    for iteration in range(n_iters):\n",
        "\n",
        "        layer_2_error = 0\n",
        "        layer_0 = x_train\n",
        "\n",
        "        ## Add forward pass\n",
        "        layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "    \n",
        "\n",
        "\n",
        "        # Then apply sigmoid\n",
        "        # layer_2_s = 1/(1+np.exp(-layer_2))\n",
        "        exp_z = np.exp(layer_2)\n",
        "        layer_2_s = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "        q = layer_2_s\n",
        "\n",
        "\n",
        "    \n",
        "        eps = 1e-8\n",
        "         # Softmax cross entropy loss with clipping to prevent log(0)\n",
        "        loss = -(np.sum(y_train.T*np.log2(np.clip(q, eps, 1.0))))/num_samples  # avoid log(0)\n",
        "        loss_history.append(loss)\n",
        "        \n",
        "\n",
        "        ## Add backward pass and update weights\n",
        "        layer_2_diff = (layer_2_s - y_train_target)\n",
        "\n",
        "        z1 = np.dot(layer_0, weights_0_1)\n",
        "        relu_grad = (z1 > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "        hidden_delta = np.dot(layer_2_diff, weights_1_2.T) * relu_grad\n",
        "\n",
        "        # Normalize weight updates by N\n",
        "        weights_1_2 -= lr   * (np.dot(layer_1.T, layer_2_diff) /num_samples)\n",
        "        weights_0_1 -= lr * (np.dot(layer_0.T, hidden_delta) / num_samples)\n",
        "    result = MLP_softmax_test(weights_0_1,weights_1_2,x_test,y_test)    \n",
        "    return weights_0_1, weights_1_2,result, loss_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MLP_softmax_test(weights_0_1, weights_1_2, x_dataset,y_dataset):\n",
        "    layer_0_test = x_dataset    \n",
        "    layer_1_test = np.maximum(np.dot(layer_0_test, weights_0_1), 0)  # ReLU activation\n",
        "    layer_2_test = np.dot(layer_1_test, weights_1_2)\n",
        "\n",
        "    # Apply softmax\n",
        "    exp_z_test = np.exp(layer_2_test)\n",
        "    layer_2_s_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "\n",
        "    # Get predictions (class with highest probability)\n",
        "    y_pred_test = np.argmax(layer_2_s_test, axis=1)\n",
        "\n",
        "    # Get true labels\n",
        "    y_true_test = np.argmax(y_dataset.T, axis=1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy_test = np.mean(y_pred_test == y_true_test)\n",
        "    print(f\"Test Accuracy: {accuracy_test * 100:.2f}%\")\n",
        "    TP = []\n",
        "    FP = []\n",
        "    FN = []\n",
        "    for j in range(3):\n",
        "        TP.append(np.sum(np.array([int(s == j and y_true_test[i] == j) for i,s in enumerate(y_pred_test)])))\n",
        "        FP.append(np.sum(np.array([int(s == j and y_true_test[i] != j) for i,s in enumerate(y_pred_test)])))\n",
        "        FN.append(np.sum(np.array([int(s != j and y_true_test[i] == j) for i,s in enumerate(y_pred_test)])))\n",
        "    \n",
        "    precision = np.array(TP)/(np.array(TP)+np.array(FP))\n",
        "    recall = np.array(TP)/(np.array(TP)+np.array(FN))\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(precision)\n",
        "    print(recall)\n",
        "    print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.4\n",
            "Test Accuracy: 47.28%\n",
            "[0.66738973 0.34464752 0.38468551]\n",
            "[0.45267288 0.57115385 0.36631944]\n",
            "[0.53945061 0.42988963 0.3752779 ]\n",
            "-----\n",
            "Learning rate:  0.2\n",
            "Test Accuracy: 51.22%\n",
            "[0.64084314 0.37582588 0.40221402]\n",
            "[0.57405591 0.46490385 0.37847222]\n",
            "[0.60561376 0.41564582 0.38998211]\n",
            "-----\n",
            "Learning rate:  0.1\n",
            "Test Accuracy: 55.55%\n",
            "[0.61622589 0.41955193 0.42574257]\n",
            "[0.75993134 0.29711538 0.29861111]\n",
            "[0.68057538 0.34787504 0.35102041]\n",
            "-----\n",
            "Learning rate:  0.05\n",
            "Test Accuracy: 57.22%\n",
            "[0.59870281 0.448737   0.45032051]\n",
            "[0.88278568 0.14519231 0.24392361]\n",
            "[0.71350709 0.21939702 0.31644144]\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "MLP_models_softmax = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,loss = MLP_softmax(M_train,y_train,M_test,y_test,lr=lr) \n",
        "    print(\"-----\")\n",
        "    MLP_models_softmax[lr] = (weights,bias,result,loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'MLP_softmax_test' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMLP_softmax_test\u001b[49m(MLP_models_softmax[\u001b[38;5;241m0.05\u001b[39m][\u001b[38;5;241m0\u001b[39m],MLP_models_softmax[\u001b[38;5;241m0.05\u001b[39m][\u001b[38;5;241m1\u001b[39m],M_train,y_train)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# MLP_models_softmax[0.05]\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MLP_softmax_test' is not defined"
          ]
        }
      ],
      "source": [
        "MLP_softmax_test(MLP_models_softmax[0.05][0],MLP_models_softmax[0.05][1],M_train,y_train)\n",
        "# MLP_models_softmax[0.05]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Further Analysis on Result**\n",
        "<br/>\n",
        "Below we perform further analysis on the result, comparing the two models. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Methods to perform forward propagation to conduct further error slice analysis \n",
        "def LogReg_ForwardPass(weights, bias, x_dataset, Sigmoid:bool):\n",
        "    if(Sigmoid):\n",
        "        z= x_dataset.dot(weights) + bias\n",
        "        q = (1 / (1+np.exp(-z)))\n",
        "        x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "        return x_test_pred, q\n",
        "    else:\n",
        "       z_test = x_dataset.dot(weights) + bias\n",
        "       # Perform theSoftmax\n",
        "       exp_z_test = np.exp(z_test)\n",
        "       q_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "       # Get predictions, basically get the class with highest probability\n",
        "       y_test_pred = np.argmax(q_test, axis=1)  # Shape: (7310,)\n",
        "       return y_test_pred,q_test\n",
        "\n",
        "def MLP_ForwardPass(weights_0_1,weights_1_2,x_dataset, Sigmoid:bool):\n",
        "    if(Sigmoid):\n",
        "        layer_0_final_test = x_dataset\n",
        "        layer_1_final_test = np.maximum(np.dot(layer_0_final_test, weights_0_1), 0)\n",
        "        layer_2_final_test = np.dot(layer_1_final_test, weights_1_2)\n",
        "        layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "        #Converting probabilities\n",
        "        x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "        return x_pred, layer_2_s_final_test\n",
        "    else:\n",
        "        layer_0_test = x_dataset    \n",
        "        layer_1_test = np.maximum(np.dot(layer_0_test, weights_0_1), 0)  # ReLU activation\n",
        "        layer_2_test = np.dot(layer_1_test, weights_1_2)\n",
        "\n",
        "        # Apply softmax\n",
        "        exp_z_test = np.exp(layer_2_test)\n",
        "        layer_2_s_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "\n",
        "        # Get predictions (class with highest probability)\n",
        "        y_pred_test = np.argmax(layer_2_s_test, axis=1)\n",
        "        return y_pred_test, layer_2_s_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conducting Error Analysis**\n",
        "<br/>\n",
        "- What can MLP capture that Logistic Regression cannot or vice versa ?\n",
        "<br/>\n",
        "- Can MLP gather some context despite having a one-hot-encoded features, context where Logistic Regression fails ? \n",
        "<br/>\n",
        "- If an example has words with heavy weight according to Logistic Regression, does an MLP model capture those examples correctly ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'LogReg_models_sigmoid' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mLogReg_models_sigmoid\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(LogReg_models_softmax)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(MLP_models_sigmoid)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LogReg_models_sigmoid' is not defined"
          ]
        }
      ],
      "source": [
        "print(LogReg_models_sigmoid)\n",
        "print(LogReg_models_softmax)\n",
        "print(MLP_models_sigmoid)\n",
        "print(MLP_models_softmax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'weights' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Error slice analysis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# What can MLP capture that Logistic Regression cannot and vice versa ? \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get predictions from both models\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m logreg_preds, logreg_probs \u001b[38;5;241m=\u001b[39m LogReg_ForwardPass(\u001b[43mweights\u001b[49m, bias, M_test, Sigmoid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m mlp_preds, mlp_probs \u001b[38;5;241m=\u001b[39m MLP_ForwardPass(w0, w1, M_test, Sigmoid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get true labels\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
          ]
        }
      ],
      "source": [
        "# Error slice analysis\n",
        "# What can MLP capture that Logistic Regression cannot and vice versa ? \n",
        "\n",
        "# Get predictions from both models\n",
        "logreg_preds, logreg_probs = LogReg_ForwardPass(weights, bias, M_test, Sigmoid=True)\n",
        "mlp_preds, mlp_probs = MLP_ForwardPass(w0, w1, M_test, Sigmoid=True)\n",
        "\n",
        "# Get true labels\n",
        "true_labels = np.array([int(l == \"positive\") for l in sentiment_labels_test])\n",
        "\n",
        "# Create error slices\n",
        "mlp_correct = (mlp_preds.flatten() == true_labels)\n",
        "logreg_correct = (logreg_preds == true_labels)\n",
        "\n",
        "mlp_wins = mlp_correct & ~logreg_correct\n",
        "logreg_wins = logreg_correct & ~mlp_correct\n",
        " \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
