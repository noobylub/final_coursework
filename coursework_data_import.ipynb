{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noobylub/final_coursework/blob/main/coursework_data_import.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqwi4b5geG6"
      },
      "source": [
        "**Experiment**\n",
        "<br>\n",
        "This research seeks to compare One-Hot-Encoding against Multi-Layer Perceptron, which has be de-facto for many ML problems. \n",
        "<br>\n",
        "The following experiement will be performed and evaluated: \n",
        "\n",
        "*   One hot encoding (OHE), sigmoid\n",
        "*   Multi Layer Perceptron (MLP), sigmoid\n",
        "*   OHE, softmax  \n",
        "*   MLP, softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Retrieving the Data and Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhNWdx7l84yP",
        "outputId": "8563f28d-5f96-41bb-c5b6-4a9adba9bb8e"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 21.2M  100 21.2M    0     0  21.5M      0 --:--:-- --:--:-- --:--:-- 21.5M\n"
          ]
        }
      ],
      "source": [
        "# Run this when editing in code editor \n",
        "!curl -O https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/coursework/Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YHzNVVHQPd9",
        "outputId": "bf37b091-809e-4b00-f8c3-83c688764170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REVIEW\tRATING\tPRODUCT_TYPE\tHELPFUL\n",
            "\"This is a wonderful album, that evokes memories of the 60's folk boom, yet contains original songs. I was amazed at the fantastic harmonies and musical arrangements.Anyone who loves the movie \"\"A Mighty Wind\"\" and who loves folk music will fall in love with this album. I know I did\"\tpositive\tmusic\tneutral\n",
            "\"On one hand, this CD is a straight ahead instrumental rocker, but Johnny A really shows how great he is with ballads, such as his covers of \"\"Wichita Lineman,\"\" and \"\"Yes it Is.\"\"  In fact, those two ballads alone are worth the price of the CD by themselves.But Johnny A can flat kick your ass, too.  He's a biker and his tunes like Oh Yeah, In the Wind and Two Wheel Horse are named for his other hobby.  And they rock, but there's nothing cliched or tired in his style.  He always seems to be looking for new ways to say something.I saw him in person at the Triple Door in Seattle sometime in February 2005 in a power trio format and he played most of the tunes on this album.  The guy is one amazing guitar player.  He played his signature Gibson hollow body, fitted with a vibrato tailpiece (Bigsby? It was like the old Chet Atkins \"\"country gentleman\"\" model) and he utilized a battery of foot pedal effects, coming eventually through a pair of Marshall combo amps.  The guy had some of the best clean tones I've ever heard from anyone, anywhere.Basically, Johnny A is a guitarist who has complete command of the instrument.  And he's got a rocking soul that cuts loose on originals and covers alike in a style that's all his own, and that's saying something these days.  If you love great rocking/rockabilly guitar, combined with really cool ballad playing in the power trio format, this CD is just what you're looking for.  In fact, I guarantee you'll be knocked out.Five stars.\"\tpositive\tmusic\thelpful\n",
            "\"this band reminds me of the thrill i first got when i listened to an Atreyu Album. It dies today rip off the former bands style, but they are still a very good band.  In the over-crowded metalcore market of today, that is a rarity.  My only complaint, is that the vocalist has a beautiful singing voice (as heard on \"\"The Radiance), but more often then not goes for the screams and growls that are associated with this type of music. still 5/5 material though.Favorite songs: \"\"The Radiance,\"\" \"\"Freak Gasoline Fight,\"\" \"\"Our Disintigration,\"\" and :THe Caitliff Choir:Defeatism\"\tpositive\tmusic\tunhelpful\n",
            "\"Like I said I would, I finally got around to purchasing this CD. I especially like tracks 9-12. Cheap Trick is solid as always. I have been a long time fan and enjoy all their releases. If I do have any criticism at all, it is the fact that some songs sound like recycled Trick riffs. I guess it's hard to create new stuff after 30+ years. Anyway, there are some great tunes on this. Buy it like I did!\"\tpositive\tmusic\tunhelpful\n"
          ]
        }
      ],
      "source": [
        "# Example of what the data looks like\n",
        "!head -n5 Compiled_Reviews.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_eYghCK5OX"
      },
      "source": [
        "**Data loading and pre-processing**\n",
        "<br>\n",
        "Below we preprocess the data from the raw file \"Compiled_reviews.txt\"\n",
        "<br>\n",
        "We remove any unwanted characters, to improve the integrity of the text corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Li-IcrXi9O8G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reviews=[]\n",
        "sentiment_ratings=[]\n",
        "product_types=[]\n",
        "helpfulness_ratings=[]\n",
        "\n",
        "with open(\"Compiled_Reviews.txt\") as f:\n",
        "   for line in f.readlines()[1:]:\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # remove punctuation/numbers and replace it with a space\n",
        "        fields[0] = re.sub(r'[.,!?;:()\\[\\]{}\\-â€”\\'\\/\\\"\\\"\\d+]', \" \",fields[0])\n",
        "        reviews.append(fields[0])\n",
        "        sentiment_ratings.append(fields[1])\n",
        "        \n",
        "        helpfulness_ratings.append(fields[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qV1XhmNXI4"
      },
      "source": [
        "**Data Analysis**\n",
        "<br/>\n",
        "Below we see what the data looks like after pre-processing\n",
        "<br/>\n",
        "Data analysis can also be shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMAE3UTkK-jO",
        "outputId": "6c0af6b2-c7fe-43d7-e8a7-87965c784bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is   This is a wonderful album  that evokes memories of the    s folk boom  yet contains original songs  I was amazed at the fantastic harmonies and musical arrangements Anyone who loves the movie   A Mighty Wind   and who loves folk music will fall in love with this album  I know I did \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   On one hand  this CD is a straight ahead instrumental rocker  but Johnny A really shows how great he is with ballads  such as his covers of   Wichita Lineman    and   Yes it Is     In fact  those two ballads alone are worth the price of the CD by themselves But Johnny A can flat kick your ass  too   He s a biker and his tunes like Oh Yeah  In the Wind and Two Wheel Horse are named for his other hobby   And they rock  but there s nothing cliched or tired in his style   He always seems to be looking for new ways to say something I saw him in person at the Triple Door in Seattle sometime in February      in a power trio format and he played most of the tunes on this album   The guy is one amazing guitar player   He played his signature Gibson hollow body  fitted with a vibrato tailpiece  Bigsby  It was like the old Chet Atkins   country gentleman   model  and he utilized a battery of foot pedal effects  coming eventually through a pair of Marshall combo amps   The guy had some of the best clean tones I ve ever heard from anyone  anywhere Basically  Johnny A is a guitarist who has complete command of the instrument   And he s got a rocking soul that cuts loose on originals and covers alike in a style that s all his own  and that s saying something these days   If you love great rocking rockabilly guitar  combined with really cool ballad playing in the power trio format  this CD is just what you re looking for   In fact  I guarantee you ll be knocked out Five stars  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is   this band reminds me of the thrill i first got when i listened to an Atreyu Album  It dies today rip off the former bands style  but they are still a very good band   In the over crowded metalcore market of today  that is a rarity   My only complaint  is that the vocalist has a beautiful singing voice  as heard on   The Radiance   but more often then not goes for the screams and growls that are associated with this type of music  still     material though Favorite songs    The Radiance      Freak Gasoline Fight      Our Disintigration    and  THe Caitliff Choir Defeatism \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Like I said I would  I finally got around to purchasing this CD  I especially like tracks       Cheap Trick is solid as always  I have been a long time fan and enjoy all their releases  If I do have any criticism at all  it is the fact that some songs sound like recycled Trick riffs  I guess it s hard to create new stuff after     years  Anyway  there are some great tunes on this  Buy it like I did  \n",
            "Sentiment  positive\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   Ok good CD  im not suprised  Ok jaheim may not have the b best voice but his music is good and it goes will with the voice that he has   Yes yall this album does use profanity but the songs actually have meanings  I like that daddy thing song  and like a dj  This album looks at issues from a mans  point of view  Ya know we ve heard the angry woman and how the man did t his and that now jaheim looks at it from a males view  Ok yall i know he is not the first one to do this but he did it well  Also this is a break from that       that is played on the radio  \n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   Review by Mike Watson With the recent slew of Victory Records releases  not to mention the wide variety  one can be to say the least skeptical of any of their assortment of new releases   However  picking through all of the less notable releases from the label  the up and coming band  The Forecast  have stepped to the plate and scored a home run on Late Night Conversations   The Forecast are a perfect blend of male and female vocals over a very modern rock soundtrack  executed flawlessly over a ten song full length Late Night Conversations is an album confronting different topics ranging from relationships to the teen trend of mutilation and loathing   The harmonies arranged by both vocalists are undeniably catching and well written and the music follows closely in suit leaving your head bobbing back and forth to the beat and your feet tapping almost involuntarily  The Forecast is not only a great recorded band  but bring the same toe tapping energy to the stage  with both great stage presence and cohesion as a band   The Forecast will defiantly be one of the new big bands in poppier rock music  and checking their Victory Records debut out is a must for fans of the genre  \n",
            "Sentiment  positive\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is  great anniversary present  fav song    I love the way you love me    my husband loves to listen to it in the car\n",
            "Sentiment  positive\n",
            "Helpfullness is  neutral\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "index = 0\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Positive review\n",
        "for index in range(len(reviews)):\n",
        "  print(\"Review is \",reviews[index])\n",
        "  print(\"Sentiment \", sentiment_ratings[index])\n",
        "  print(\"Helpfullness is \", helpfulness_ratings[index])\n",
        "  print(\"-----------\")\n",
        "  if(index >5):\n",
        "    break;\n",
        "# helpfulness_ratings[0:5]\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review is   I ve always held the philosophy you are what you re entertained by  So  those who like Ludacris are definitely telling on themselves about their views of life and women  And speaking of women   Ludacris never met one who wasn t a h o Ludacris is the epitome of the sexual hypocrisy that plagues men  you demean and look down on women for being h oes  yet you hope every woman is a h o so you ll have no problems getting laid  But then  you re so disgusted with the fact the woman behaves too much like you  so you go and find a   nice girl   because you can respect her and marry her and procreate with her  But then you cheat on her with a   bad girl   because h oes need h oes  but then you can t respect the   bad girl   because she s a slut  so you go back to the   good girl    But she s not nasty enough for you  so then   around and around we go  Women are basically being punished for men not being able to make up their minds about what the heck they want from us He s OBSESSED with sex and h oes  It makes you wonder if he has hang ups about it  Maybe Ludapis saw his mother selling herself in a backalley for a dollar and it scarred his outlook on women forever  I guess if she had actually RAISED him and shown him how to respect women   ALL women  then he probably could have grown up to be a respectable guy instead of the prurient dummy he is What s the music like  Well I ll give him   stars for mentioning TLC in What s Your Fantasy  That s it Addendum          Some may wonder why I m   wasting  my time giving Ludacris negative reviews  Well  first of all  if only people who liked the product were the only ones to review it  everything on Amazon would have   stars  which would be an inaccurate sign of its quality Secondly  I don t like Ludacris  I would give Ludacris more respect if I felt he gave my gender more respect  but he doesn t  My diatribe above shows what my problem with him is  and I feel that argument is valid  There s no way to critique Ludacris  music without critiquing his hypocrisy regarding women  Why should I like him when he seems to have nothing but contempt for us  I m biased against him because he s biased against me  So he gets a bad rating out of s p i t e \n",
            "Sentiment  negative\n",
            "Helpfullness is  unhelpful\n",
            "-----------\n",
            "Review is   someone get this band a producer and put them in a decent studio  please   the recording and mixing on this disc is flat out atrocious    download a few tracks and hear for yourself  feburary  and     with an anchor  show promise otherwise it s taking back sunday meets hawthorn heights meets fallout boy meets      you get the picture \n",
            "Sentiment  negative\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is  Tihs Album is not all that good when it came out in still not good till this day  I like all his other albums better I don t Know why he decided to release this crap But the only song I really Like is hip hip qurtables\n",
            "Sentiment  negative\n",
            "Helpfullness is  neutral\n",
            "-----------\n",
            "Review is   this industry is   goin down  u call this an album     LO \n",
            "Sentiment  negative\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is  I m sorry but the guy below me doesn t know music if he was slapped with a Gang starr cd  This mess is ridiculous  I wouldn t even want to hear this garbage in the klub let alone banging in my car  They are the modern day version of Young mc    here today gone tomorrow\n",
            "Sentiment  negative\n",
            "Helpfullness is  helpful\n",
            "-----------\n",
            "Review is   This is not what I expected from such a gifted artist as Jaheim  did he change producers  The music and the man with the voice didn t go together well for me   This is one I will not be purchasing  it is a serious down grade from his previous two albums  Better luck next time  Jaheim I was truly disappointed  \n",
            "Sentiment  negative\n",
            "Helpfullness is  neutral\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "total_printed = 0\n",
        "for index in range(len(reviews)):\n",
        "  if(sentiment_ratings[index] == 'negative'):\n",
        "    print(\"Review is \",reviews[index])\n",
        "    print(\"Sentiment \", sentiment_ratings[index])\n",
        "    print(\"Helpfullness is \", helpfulness_ratings[index])\n",
        "    print(\"-----------\")\n",
        "    total_printed += 1\n",
        "  if(total_printed > 5):\n",
        "    break;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment distribution:\n",
            "positive: 20972 (57.38%)\n",
            "negative: 15576 (42.62%)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAHWCAYAAACFXRQ+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTo0lEQVR4nO3deVxV1f7/8fcR5aAyOYIkAs7gPCShOSWKSqU3KzVzumrZxXnIKAfSupheNTPL270lVnotG8zUVMQpk0wxNE1NDaVSMMcjDqiwf3/0Zf88G0dCQXs9H4/9eLjXWmftz97I8e1mnY3NMAxDAAAAAExFCroAAAAAoLAhJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkA8iTwMBA9enTp6DL+NNiYmJks9nuyLFatWqlVq1amfvr1q2TzWbTJ598ckeO36dPHwUGBt6RY13p4MGDstlsiouLu+PHvhlXuy42m00xMTEFUg+AwoGQDMDJgQMH9Oyzz6py5cpyc3OTp6enmjVrppkzZ+r8+fMFXd51xcXFyWazmZubm5v8/PwUERGhN954Q2fOnMmX4xw+fFgxMTFKTk7Ol/nyU2GuLT+1atXK6Wt95bZnz547UkNO+M/ZihUrprJly6pp06Z68cUXlZqamue5C9vXcfny5fynAX85RQu6AACFx7Jly/TEE0/IbrerV69eql27ti5evKiNGzdq9OjR2rVrl955552CLvOGJk6cqKCgIF26dElpaWlat26dhg0bpunTp2vJkiWqW7euOXbs2LF64YUXbmn+w4cP6+WXX1ZgYKDq169/069btWrVLR0nL65X23/+8x9lZ2ff9hqsAgICdP78eRUrVixf561YsaJiY2Nztfv5+eXrcW6ke/fu6tixo7Kzs3Xy5Elt2bJFr7/+umbOnKl3331X3bp1u+U58/p37HZZvny5Zs+eTVDGXwohGYAkKSUlRd26dVNAQIDWrFmjChUqmH1RUVHav3+/li1bVoAV3rwOHTqocePG5n50dLTWrFmjhx9+WI8++qh2796t4sWLS5KKFi2qokVv71vhuXPnVKJECbm6ut7W49xIfofUm5VzVz+/eXl56emnn873eW9Vw4YNc9Vx6NAhtWvXTr1791ZwcLDq1atXQNUByCuWWwCQJE2ZMkUZGRl69913nQJyjqpVq2ro0KHXfP2JEyc0atQo1alTR+7u7vL09FSHDh20ffv2XGNnzZqlWrVqqUSJEipVqpQaN26sBQsWmP1nzpzRsGHDFBgYKLvdrvLly6tt27batm1bns/voYce0rhx43To0CF9+OGHZvvV1iTHx8frwQcflLe3t9zd3VWjRg29+OKLkv5YR3z//fdLkvr27Wv+qD1nvW2rVq1Uu3ZtJSUlqUWLFipRooT5Wuua5BxZWVl68cUX5evrq5IlS+rRRx/VL7/84jTmWmvAr5zzRrVdbe3t2bNnNXLkSPn7+8tut6tGjRr617/+JcMwnMbZbDYNGjRIixcvVu3atWW321WrVi2tWLHi6hf8Cldbk9ynTx+5u7vrt99+U+fOneXu7q5y5cpp1KhRysrKuuGcN5Kz9ObgwYNO7TnrwNetW/enj3E9AQEBiouL08WLFzVlyhSz/Wa+T270dfz666/1xBNPqFKlSrLb7fL399fw4cNzLYdKS0tT3759VbFiRdntdlWoUEGdOnXKdU2++uorNW/eXCVLlpSHh4ciIyO1a9cus79Pnz6aPXu2JDktLwHuddxJBiBJ+vLLL1W5cmU1bdo0T6//+eeftXjxYj3xxBMKCgpSenq6/v3vf6tly5b68ccfzR+B/+c//9GQIUP0+OOPa+jQobpw4YJ27NihzZs366mnnpIkDRw4UJ988okGDRqkkJAQHT9+XBs3btTu3bvVsGHDPJ9jz5499eKLL2rVqlUaMGDAVcfs2rVLDz/8sOrWrauJEyfKbrdr//79+uabbyRJwcHBmjhxosaPH69nnnlGzZs3lySn63b8+HF16NBB3bp109NPPy0fH5/r1vXqq6/KZrNpzJgxOnr0qF5//XWFh4crOTnZvON9M26mtisZhqFHH31Ua9euVb9+/VS/fn2tXLlSo0eP1m+//aYZM2Y4jd+4caM+++wz/eMf/5CHh4feeOMNdenSRampqSpTpsxN15kjKytLERERCg0N1b/+9S+tXr1a06ZNU5UqVfTcc8/d1OuPHTvm1Obm5iZ3d/dbruV2CAsLU5UqVRQfH2+23cz3yY2+josWLdK5c+f03HPPqUyZMvruu+80a9Ys/frrr1q0aJF5rC5dumjXrl0aPHiwAgMDdfToUcXHxys1NdX8z9IHH3yg3r17KyIiQq+99prOnTunt99+Ww8++KC+//57BQYG6tlnn9Xhw4cVHx+vDz744M5dQKCgGQD+8k6fPm1IMjp16nTTrwkICDB69+5t7l+4cMHIyspyGpOSkmLY7XZj4sSJZlunTp2MWrVqXXduLy8vIyoq6qZryTF37lxDkrFly5brzt2gQQNzf8KECcaVb4UzZswwJBm///77NefYsmWLIcmYO3durr6WLVsakow5c+Zcta9ly5bm/tq1aw1Jxn333Wc4HA6z/eOPPzYkGTNnzjTbrNf7WnNer7bevXsbAQEB5v7ixYsNScYrr7ziNO7xxx83bDabsX//frNNkuHq6urUtn37dkOSMWvWrFzHulJKSkqumnr37m1Icvq7YRiG0aBBA6NRo0bXnc8w/v91tm451yjn70JKSorT63Ku+dq1a51qufK65JzvhAkTbuq8pk6des0xnTp1MiQZp0+fNgzj5r9Prvd1PHfuXK622NhYw2azGYcOHTIMwzBOnjx5w9rOnDljeHt7GwMGDHBqT0tLM7y8vJzao6KiDCID/mpYbgFADodDkuTh4ZHnOex2u4oU+eMtJSsrS8ePHzeXKly5TMLb21u//vqrtmzZcs25vL29tXnzZh0+fDjP9VyLu7v7dZ9y4e3tLUn64osv8vwhN7vdrr59+970+F69ejld+8cff1wVKlTQ8uXL83T8m7V8+XK5uLhoyJAhTu0jR46UYRj66quvnNrDw8NVpUoVc79u3bry9PTUzz//nOcaBg4c6LTfvHnzm54vMDBQ8fHxTtvzzz+f51puh5y72jl/5272++R6rvzpwtmzZ3Xs2DE1bdpUhmHo+++/N8e4urpq3bp1Onny5FXniY+P16lTp9S9e3cdO3bM3FxcXBQaGqq1a9fm+byBewEhGYA8PT0l6U89Ii07O1szZsxQtWrVZLfbVbZsWZUrV047duzQ6dOnzXFjxoyRu7u7mjRpomrVqikqKspcypBjypQp2rlzp/z9/dWkSRPFxMT8qSB2pYyMjOv+Z6Br165q1qyZ+vfvLx8fH3Xr1k0ff/zxLQXm++6775Y+pFetWjWnfZvNpqpVq+ZaO5rfDh06JD8/v1zXIzg42Oy/UqVKlXLNUapUqWuGsBtxc3NTuXLl8jxfyZIlFR4e7rSFhITkqZbbJSMjQ9L//w/ozX6fXE9qaqr69Omj0qVLm2u5W7ZsKUnmHHa7Xa+99pq++uor+fj4qEWLFpoyZYrS0tLMefbt2yfpj/X65cqVc9pWrVqlo0eP5tt1AO5GhGQA8vT0lJ+fn3bu3JnnOf75z39qxIgRatGihT788EOtXLlS8fHxqlWrllPADA4O1t69e7Vw4UI9+OCD+vTTT/Xggw9qwoQJ5pgnn3xSP//8s2bNmiU/Pz9NnTpVtWrVynVn81b9+uuvOn36tKpWrXrNMcWLF9eGDRu0evVq9ezZUzt27FDXrl3Vtm3bm/5A2a2sI75Z1/qgVH58yO1mubi4XLXdsHzI78/Olx8Kw/WSpJ07d6p8+fLmf0Rv9vvkWrKystS2bVstW7ZMY8aM0eLFixUfH29+qO/KOYYNG6affvpJsbGxcnNz07hx4xQcHGzebc4Z+8EHH+S6Ix8fH68vvvgin68GcHfhg3sAJEkPP/yw3nnnHSUmJiosLOyWX//JJ5+odevWevfdd53aT506pbJlyzq1lSxZUl27dlXXrl118eJFPfbYY3r11VcVHR1tPiqsQoUK+sc//qF//OMfOnr0qBo2bKhXX31VHTp0yPM55nzoKCIi4rrjihQpojZt2qhNmzaaPn26/vnPf+qll17S2rVrFR4enu+f7M+5o5fDMAzt37/f6XnOpUqV0qlTp3K99tChQ6pcubK5fyu1BQQEaPXq1Tpz5ozT3eScX8YREBBw03MVNqVKlZKkXNfMenf8dkpMTNSBAwecHg93s98n1/o6/vDDD/rpp580b9489erVy2y/8sOBV6pSpYpGjhypkSNHat++fapfv76mTZumDz/80Fw6U758eYWHh1/3XHiaBf6KuJMMQJL0/PPPq2TJkurfv7/S09Nz9R84cEAzZ8685utdXFxy3VFctGiRfvvtN6e248ePO+27uroqJCREhmHo0qVLysrKyvVj5/Lly8vPz0+ZmZm3elqmNWvWaNKkSQoKClKPHj2uOe7EiRO52nJ+mUPO8UuWLCkpdwDLq/fff99pqcsnn3yiI0eOOP2HoEqVKvr222918eJFs23p0qW5HhV3K7V17NhRWVlZevPNN53aZ8yYIZvN9qf+Q1LQcgLghg0bzLasrKw79stwDh06pD59+sjV1VWjR48222/2++RaX8ecu+9XzmEYRq7vzXPnzunChQtObVWqVJGHh4f59zgiIkKenp765z//qUuXLuU6h99///2G9QD3Mu4kA5D0xz+gCxYsUNeuXRUcHOz0G/c2bdqkRYsWXfU5vTkefvhhTZw4UX379lXTpk31ww8/aP78+U53OSWpXbt28vX1VbNmzeTj46Pdu3frzTffVGRkpDw8PHTq1ClVrFhRjz/+uOrVqyd3d3etXr1aW7Zs0bRp027qXL766ivt2bNHly9fVnp6utasWaP4+HgFBARoyZIl1/3FFhMnTtSGDRsUGRmpgIAAHT16VG+99ZYqVqyoBx980LxW3t7emjNnjjw8PFSyZEmFhoYqKCjopuqzKl26tB588EH17dtX6enpev3111W1alWnx9T1799fn3zyidq3b68nn3xSBw4ccLobmONWanvkkUfUunVrvfTSSzp48KDq1aunVatW6YsvvtCwYcNyzX03qVWrlh544AFFR0frxIkTKl26tBYuXKjLly/n+7G2bdumDz/8UNnZ2Tp16pS2bNmiTz/9VDabTR988IHTTwRu9vvkWl/HmjVrqkqVKho1apR+++03eXp66tNPP821jvunn35SmzZt9OSTTyokJERFixbV559/rvT0dPM3AHp6eurtt99Wz5491bBhQ3Xr1k3lypVTamqqli1bpmbNmpn/gWrUqJEkaciQIYqIiJCLi0uefpMgcFcpsOdqACiUfvrpJ2PAgAFGYGCg4erqanh4eBjNmjUzZs2aZVy4cMEcd7VHwI0cOdKoUKGCUbx4caNZs2ZGYmJirkeU/fvf/zZatGhhlClTxrDb7UaVKlWM0aNHm4/IyszMNEaPHm3Uq1fP8PDwMEqWLGnUq1fPeOutt25Ye85jv3I2V1dXw9fX12jbtq0xc+ZMp8es5bA+Ai4hIcHo1KmT4efnZ7i6uhp+fn5G9+7djZ9++snpdV988YUREhJiFC1a1OlRXS1btrzmI+6u9Qi4//3vf0Z0dLRRvnx5o3jx4kZkZKT5KK8rTZs2zbjvvvsMu91uNGvWzNi6dWuuOa9X29UedXbmzBlj+PDhhp+fn1GsWDGjWrVqxtSpU43s7GyncZKu+li+az2a7krXegRcyZIlc421fj2u5XrXOceBAweM8PBww263Gz4+PsaLL75oxMfH5/sj4HK2okWLGqVLlzZCQ0ON6Ojoq34Nb/b7xDCu/XX88ccfjfDwcMPd3d0oW7asMWDAAPNxfDljjh07ZkRFRRk1a9Y0SpYsaXh5eRmhoaHGxx9/nKumtWvXGhEREYaXl5fh5uZmVKlSxejTp4+xdetWc8zly5eNwYMHG+XKlTNsNhuPg8Nfgs0w8viJCwAAAOAexZpkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAW/TCSfZGdn6/Dhw/Lw8ODXdwIAABRChmHozJkz8vPzU5Ei179XTEjOJ4cPH5a/v39BlwEAAIAb+OWXX1SxYsXrjiEk5xMPDw9Jf1x0T0/PAq4GAAAAVg6HQ/7+/mZuux5Ccj7JWWLh6elJSAYAACjEbmZpLB/cAwAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAIuiBV0A8i7whWUFXQKAO+Dg5MiCLgEA/nK4kwwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAo0JMfGxur++++Xh4eHypcvr86dO2vv3r1OYy5cuKCoqCiVKVNG7u7u6tKli9LT053GpKamKjIyUiVKlFD58uU1evRoXb582WnMunXr1LBhQ9ntdlWtWlVxcXG56pk9e7YCAwPl5uam0NBQfffdd/l+zgAAACj8CjQkr1+/XlFRUfr2228VHx+vS5cuqV27djp79qw5Zvjw4fryyy+1aNEirV+/XocPH9Zjjz1m9mdlZSkyMlIXL17Upk2bNG/ePMXFxWn8+PHmmJSUFEVGRqp169ZKTk7WsGHD1L9/f61cudIc89FHH2nEiBGaMGGCtm3bpnr16ikiIkJHjx69MxcDAAAAhYbNMAyjoIvI8fvvv6t8+fJav369WrRoodOnT6tcuXJasGCBHn/8cUnSnj17FBwcrMTERD3wwAP66quv9PDDD+vw4cPy8fGRJM2ZM0djxozR77//LldXV40ZM0bLli3Tzp07zWN169ZNp06d0ooVKyRJoaGhuv/++/Xmm29KkrKzs+Xv76/BgwfrhRdeuGHtDodDXl5eOn36tDw9PfP70lxV4AvL7shxABSsg5MjC7oEALgn3EpeK1Rrkk+fPi1JKl26tCQpKSlJly5dUnh4uDmmZs2aqlSpkhITEyVJiYmJqlOnjhmQJSkiIkIOh0O7du0yx1w5R86YnDkuXryopKQkpzFFihRReHi4OcYqMzNTDofDaQMAAMC9odCE5OzsbA0bNkzNmjVT7dq1JUlpaWlydXWVt7e301gfHx+lpaWZY64MyDn9OX3XG+NwOHT+/HkdO3ZMWVlZVx2TM4dVbGysvLy8zM3f3z9vJw4AAIBCp9CE5KioKO3cuVMLFy4s6FJuSnR0tE6fPm1uv/zyS0GXBAAAgHxStKALkKRBgwZp6dKl2rBhgypWrGi2+/r66uLFizp16pTT3eT09HT5+vqaY6xPoch5+sWVY6xPxEhPT5enp6eKFy8uFxcXubi4XHVMzhxWdrtddrs9bycMAACAQq1A7yQbhqFBgwbp888/15o1axQUFOTU36hRIxUrVkwJCQlm2969e5WamqqwsDBJUlhYmH744Qenp1DEx8fL09NTISEh5pgr58gZkzOHq6urGjVq5DQmOztbCQkJ5hgAAAD8dRToneSoqCgtWLBAX3zxhTw8PMz1v15eXipevLi8vLzUr18/jRgxQqVLl5anp6cGDx6ssLAwPfDAA5Kkdu3aKSQkRD179tSUKVOUlpamsWPHKioqyrzTO3DgQL355pt6/vnn9fe//11r1qzRxx9/rGXL/v/TIUaMGKHevXurcePGatKkiV5//XWdPXtWffv2vfMXBgAAAAWqQEPy22+/LUlq1aqVU/vcuXPVp08fSdKMGTNUpEgRdenSRZmZmYqIiNBbb71ljnVxcdHSpUv13HPPKSwsTCVLllTv3r01ceJEc0xQUJCWLVum4cOHa+bMmapYsaL++9//KiIiwhzTtWtX/f777xo/frzS0tJUv359rVixIteH+QAAAHDvK1TPSb6b8ZxkALcLz0kGgPxx1z4nGQAAACgMCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwKNCRv2LBBjzzyiPz8/GSz2bR48WKnfpvNdtVt6tSp5pjAwMBc/ZMnT3aaZ8eOHWrevLnc3Nzk7++vKVOm5Kpl0aJFqlmzptzc3FSnTh0tX778tpwzAAAACr8CDclnz55VvXr1NHv27Kv2HzlyxGl77733ZLPZ1KVLF6dxEydOdBo3ePBgs8/hcKhdu3YKCAhQUlKSpk6dqpiYGL3zzjvmmE2bNql79+7q16+fvv/+e3Xu3FmdO3fWzp07b8+JAwAAoFArWpAH79Chgzp06HDNfl9fX6f9L774Qq1bt1blypWd2j08PHKNzTF//nxdvHhR7733nlxdXVWrVi0lJydr+vTpeuaZZyRJM2fOVPv27TV69GhJ0qRJkxQfH68333xTc+bM+TOnCAAAgLvQXbMmOT09XcuWLVO/fv1y9U2ePFllypRRgwYNNHXqVF2+fNnsS0xMVIsWLeTq6mq2RUREaO/evTp58qQ5Jjw83GnOiIgIJSYmXrOezMxMORwOpw0AAAD3hgK9k3wr5s2bJw8PDz322GNO7UOGDFHDhg1VunRpbdq0SdHR0Tpy5IimT58uSUpLS1NQUJDTa3x8fMy+UqVKKS0tzWy7ckxaWto164mNjdXLL7+cH6cGAACAQuauCcnvvfeeevToITc3N6f2ESNGmH+uW7euXF1d9eyzzyo2NlZ2u/221RMdHe10bIfDIX9//9t2PAAAANw5d0VI/vrrr7V371599NFHNxwbGhqqy5cv6+DBg6pRo4Z8fX2Vnp7uNCZnP2cd87XGXGudsyTZ7fbbGsIBAABQcO6KNcnvvvuuGjVqpHr16t1wbHJysooUKaLy5ctLksLCwrRhwwZdunTJHBMfH68aNWqoVKlS5piEhASneeLj4xUWFpaPZwEAAIC7RYGG5IyMDCUnJys5OVmSlJKSouTkZKWmpppjHA6HFi1apP79++d6fWJiol5//XVt375dP//8s+bPn6/hw4fr6aefNgPwU089JVdXV/Xr10+7du3SRx99pJkzZzotlRg6dKhWrFihadOmac+ePYqJidHWrVs1aNCg23sBAAAAUCgV6HKLrVu3qnXr1uZ+TnDt3bu34uLiJEkLFy6UYRjq3r17rtfb7XYtXLhQMTExyszMVFBQkIYPH+4UgL28vLRq1SpFRUWpUaNGKlu2rMaPH28+/k2SmjZtqgULFmjs2LF68cUXVa1aNS1evFi1a9e+TWcOAACAwsxmGIZR0EXcCxwOh7y8vHT69Gl5enrekWMGvrDsjhwHQME6ODmyoEsAgHvCreS1u2JNMgAAAHAnEZIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAi6IFXQAAAFcT+MKygi4BwB1wcHJkQZdwVdxJBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgUaEjesGGDHnnkEfn5+clms2nx4sVO/X369JHNZnPa2rdv7zTmxIkT6tGjhzw9PeXt7a1+/fopIyPDacyOHTvUvHlzubm5yd/fX1OmTMlVy6JFi1SzZk25ubmpTp06Wr58eb6fLwAAAO4OBRqSz549q3r16mn27NnXHNO+fXsdOXLE3P73v/859ffo0UO7du1SfHy8li5dqg0bNuiZZ54x+x0Oh9q1a6eAgAAlJSVp6tSpiomJ0TvvvGOO2bRpk7p3765+/frp+++/V+fOndW5c2ft3Lkz/08aAAAAhV6B/jKRDh06qEOHDtcdY7fb5evre9W+3bt3a8WKFdqyZYsaN24sSZo1a5Y6duyof/3rX/Lz89P8+fN18eJFvffee3J1dVWtWrWUnJys6dOnm2F65syZat++vUaPHi1JmjRpkuLj4/Xmm29qzpw5Vz12ZmamMjMzzX2Hw3HL5w8AAIDCqdCvSV63bp3Kly+vGjVq6LnnntPx48fNvsTERHl7e5sBWZLCw8NVpEgRbd682RzTokULubq6mmMiIiK0d+9enTx50hwTHh7udNyIiAglJiZes67Y2Fh5eXmZm7+/f76cLwAAAApeoQ7J7du31/vvv6+EhAS99tprWr9+vTp06KCsrCxJUlpamsqXL+/0mqJFi6p06dJKS0szx/j4+DiNydm/0Zic/quJjo7W6dOnze2XX375cycLAACAQqNAl1vcSLdu3cw/16lTR3Xr1lWVKlW0bt06tWnTpgAr+2MZiN1uL9AaAAAAcHsU6jvJVpUrV1bZsmW1f/9+SZKvr6+OHj3qNOby5cs6ceKEuY7Z19dX6enpTmNy9m805lproQEAAHBvu6tC8q+//qrjx4+rQoUKkqSwsDCdOnVKSUlJ5pg1a9YoOztboaGh5pgNGzbo0qVL5pj4+HjVqFFDpUqVMsckJCQ4HSs+Pl5hYWG3+5QAAABQCBVoSM7IyFBycrKSk5MlSSkpKUpOTlZqaqoyMjI0evRoffvttzp48KASEhLUqVMnVa1aVREREZKk4OBgtW/fXgMGDNB3332nb775RoMGDVK3bt3k5+cnSXrqqafk6uqqfv36adeuXfroo480c+ZMjRgxwqxj6NChWrFihaZNm6Y9e/YoJiZGW7du1aBBg+74NQEAAEDBK9CQvHXrVjVo0EANGjSQJI0YMUINGjTQ+PHj5eLioh07dujRRx9V9erV1a9fPzVq1Ehff/2101rg+fPnq2bNmmrTpo06duyoBx980OkZyF5eXlq1apVSUlLUqFEjjRw5UuPHj3d6lnLTpk21YMECvfPOO6pXr54++eQTLV68WLVr175zFwMAAACFhs0wDKOgi7gXOBwOeXl56fTp0/L09Lwjxwx8YdkdOQ6AgnVwcmRBl1AgeI8D/hru5HvcreS1u2pNMgAAAHAnEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwKJAQ/KGDRv0yCOPyM/PTzabTYsXLzb7Ll26pDFjxqhOnToqWbKk/Pz81KtXLx0+fNhpjsDAQNlsNqdt8uTJTmN27Nih5s2by83NTf7+/poyZUquWhYtWqSaNWvKzc1NderU0fLly2/LOQMAAKDwK9CQfPbsWdWrV0+zZ8/O1Xfu3Dlt27ZN48aN07Zt2/TZZ59p7969evTRR3ONnThxoo4cOWJugwcPNvscDofatWungIAAJSUlaerUqYqJidE777xjjtm0aZO6d++ufv366fvvv1fnzp3VuXNn7dy58/acOAAAAAq1ogV58A4dOqhDhw5X7fPy8lJ8fLxT25tvvqkmTZooNTVVlSpVMts9PDzk6+t71Xnmz5+vixcv6r333pOrq6tq1aql5ORkTZ8+Xc8884wkaebMmWrfvr1Gjx4tSZo0aZLi4+P15ptvas6cOflxqgAAALiL3FVrkk+fPi2bzSZvb2+n9smTJ6tMmTJq0KCBpk6dqsuXL5t9iYmJatGihVxdXc22iIgI7d27VydPnjTHhIeHO80ZERGhxMTEa9aSmZkph8PhtAEAAODeUKB3km/FhQsXNGbMGHXv3l2enp5m+5AhQ9SwYUOVLl1amzZtUnR0tI4cOaLp06dLktLS0hQUFOQ0l4+Pj9lXqlQppaWlmW1XjklLS7tmPbGxsXr55Zfz6/QAAABQiNwVIfnSpUt68sknZRiG3n77bae+ESNGmH+uW7euXF1d9eyzzyo2NlZ2u/221RQdHe10bIfDIX9//9t2PAAAANw5hT4k5wTkQ4cOac2aNU53ka8mNDRUly9f1sGDB1WjRg35+voqPT3daUzOfs465muNudY6Z0my2+23NYQDAACg4BTqNck5AXnfvn1avXq1ypQpc8PXJCcnq0iRIipfvrwkKSwsTBs2bNClS5fMMfHx8apRo4ZKlSpljklISHCaJz4+XmFhYfl4NgAAALhbFOid5IyMDO3fv9/cT0lJUXJyskqXLq0KFSro8ccf17Zt27R06VJlZWWZa4RLly4tV1dXJSYmavPmzWrdurU8PDyUmJio4cOH6+mnnzYD8FNPPaWXX35Z/fr105gxY7Rz507NnDlTM2bMMI87dOhQtWzZUtOmTVNkZKQWLlyorVu3Oj0mDgAAAH8dBRqSt27dqtatW5v7OWt8e/furZiYGC1ZskSSVL9+fafXrV27Vq1atZLdbtfChQsVExOjzMxMBQUFafjw4U5rhb28vLRq1SpFRUWpUaNGKlu2rMaPH28+/k2SmjZtqgULFmjs2LF68cUXVa1aNS1evFi1a9e+jWcPAACAwspmGIZR0EXcCxwOh7y8vHT69OkbrpvOL4EvLLsjxwFQsA5OjizoEgoE73HAX8OdfI+7lbxWqNckAwAAAAUhTyG5cuXKOn78eK72U6dOqXLlyn+6KAAAAKAg5SkkHzx4UFlZWbnaMzMz9dtvv/3pogAAAICCdEsf3Mv5IJ0krVy5Ul5eXuZ+VlaWEhISFBgYmG/FAQAAAAXhlkJy586dJUk2m029e/d26itWrJgCAwM1bdq0fCsOAAAAKAi3FJKzs7MlSUFBQdqyZYvKli17W4oCAAAAClKenpOckpKS33UAAAAAhUaef5lIQkKCEhISdPToUfMOc4733nvvTxcGAAAAFJQ8heSXX35ZEydOVOPGjVWhQgXZbLb8rgsAAAAoMHkKyXPmzFFcXJx69uyZ3/UAAAAABS5Pz0m+ePGimjZtmt+1AAAAAIVCnkJy//79tWDBgvyuBQAAACgU8rTc4sKFC3rnnXe0evVq1a1bV8WKFXPqnz59er4UBwAAABSEPIXkHTt2qH79+pKknTt3OvXxIT4AAADc7fIUkteuXZvfdQAAAACFRp7WJAMAAAD3sjzdSW7duvV1l1WsWbMmzwUBAAAABS1PITlnPXKOS5cuKTk5WTt37lTv3r3zoy4AAACgwOQpJM+YMeOq7TExMcrIyPhTBQEAAAAFLV/XJD/99NN677338nNKAAAA4I7L15CcmJgoNze3/JwSAAAAuOPytNzisccec9o3DENHjhzR1q1bNW7cuHwpDAAAACgoeQrJXl5eTvtFihRRjRo1NHHiRLVr1y5fCgMAAAAKSp5C8ty5c/O7DgAAAKDQyFNIzpGUlKTdu3dLkmrVqqUGDRrkS1EAAABAQcpTSD569Ki6deumdevWydvbW5J06tQptW7dWgsXLlS5cuXys0YAAADgjsrT0y0GDx6sM2fOaNeuXTpx4oROnDihnTt3yuFwaMiQIfldIwAAAHBH5elO8ooVK7R69WoFBwebbSEhIZo9ezYf3AMAAMBdL093krOzs1WsWLFc7cWKFVN2dvafLgoAAAAoSHkKyQ899JCGDh2qw4cPm22//fabhg8frjZt2uRbcQAAAEBByFNIfvPNN+VwOBQYGKgqVaqoSpUqCgoKksPh0KxZs/K7RgAAAOCOytOaZH9/f23btk2rV6/Wnj17JEnBwcEKDw/P1+IAAACAgnBLd5LXrFmjkJAQORwO2Ww2tW3bVoMHD9bgwYN1//33q1atWvr6669vV60AAADAHXFLIfn111/XgAED5OnpmavPy8tLzz77rKZPn37T823YsEGPPPKI/Pz8ZLPZtHjxYqd+wzA0fvx4VahQQcWLF1d4eLj27dvnNObEiRPq0aOHPD095e3trX79+ikjI8NpzI4dO9S8eXO5ubnJ399fU6ZMyVXLokWLVLNmTbm5ualOnTpavnz5TZ8HAAAA7i23FJK3b9+u9u3bX7O/Xbt2SkpKuun5zp49q3r16mn27NlX7Z8yZYreeOMNzZkzR5s3b1bJkiUVERGhCxcumGN69OihXbt2KT4+XkuXLtWGDRv0zDPPmP0Oh0Pt2rVTQECAkpKSNHXqVMXExOidd94xx2zatEndu3dXv3799P3336tz587q3Lmzdu7cedPnAgAAgHuHzTAM42YHu7m5aefOnapatepV+/fv3686dero/Pnzt16IzabPP/9cnTt3lvTHXWQ/Pz+NHDlSo0aNkiSdPn1aPj4+iouLU7du3bR7926FhIRoy5Ytaty4saQ/nuHcsWNH/frrr/Lz89Pbb7+tl156SWlpaXJ1dZUkvfDCC1q8eLG5nrpr1646e/asli5datbzwAMPqH79+pozZ85N1e9wOOTl5aXTp09f9U777RD4wrI7chwABevg5MiCLqFA8B4H/DXcyfe4W8lrt3Qn+b777rvu3dUdO3aoQoUKtzLlNaWkpCgtLc3pw4BeXl4KDQ1VYmKiJCkxMVHe3t5mQJak8PBwFSlSRJs3bzbHtGjRwgzIkhQREaG9e/fq5MmT5hjrhw4jIiLM41xNZmamHA6H0wYAAIB7wy2F5I4dO2rcuHFOyx1ynD9/XhMmTNDDDz+cL4WlpaVJknx8fJzafXx8zL60tDSVL1/eqb9o0aIqXbq005irzXHlMa41Jqf/amJjY+Xl5WVu/v7+t3qKAAAAKKRu6RFwY8eO1Weffabq1atr0KBBqlGjhiRpz549mj17trKysvTSSy/dlkILm+joaI0YMcLcdzgcBGUAAIB7xC2FZB8fH23atEnPPfecoqOjlbOc2WazKSIiQrNnz851RzavfH19JUnp6elOSzjS09NVv359c8zRo0edXnf58mWdOHHCfL2vr6/S09OdxuTs32hMTv/V2O122e32PJwZAAAACrtb/o17AQEBWr58uY4dO6bNmzfr22+/1bFjx7R8+XIFBQXlW2FBQUHy9fVVQkKC2eZwOLR582aFhYVJksLCwnTq1CmnJ2qsWbNG2dnZCg0NNcds2LBBly5dMsfEx8erRo0aKlWqlDnmyuPkjMk5DgAAAP5a8vQb9ySpVKlSuv/++//UwTMyMrR//35zPyUlRcnJySpdurQqVaqkYcOG6ZVXXlG1atUUFBSkcePGyc/Pz3wCRnBwsNq3b68BAwZozpw5unTpkgYNGqRu3brJz89PkvTUU0/p5ZdfVr9+/TRmzBjt3LlTM2fO1IwZM8zjDh06VC1bttS0adMUGRmphQsXauvWrU6PiQMAAMBfR55Dcn7YunWrWrdube7nrPHt3bu34uLi9Pzzz+vs2bN65plndOrUKT344INasWKF3NzczNfMnz9fgwYNUps2bVSkSBF16dJFb7zxhtnv5eWlVatWKSoqSo0aNVLZsmU1fvx4p2cpN23aVAsWLNDYsWP14osvqlq1alq8eLFq1659B64CAAAACptbek4yro3nJAO4XXhOMoB72T3xnGQAAADgr4CQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAotCH5MDAQNlstlxbVFSUJKlVq1a5+gYOHOg0R2pqqiIjI1WiRAmVL19eo0eP1uXLl53GrFu3Tg0bNpTdblfVqlUVFxd3p04RAAAAhUzRgi7gRrZs2aKsrCxzf+fOnWrbtq2eeOIJs23AgAGaOHGiuV+iRAnzz1lZWYqMjJSvr682bdqkI0eOqFevXipWrJj++c9/SpJSUlIUGRmpgQMHav78+UpISFD//v1VoUIFRURE3IGzBAAAQGFS6ENyuXLlnPYnT56sKlWqqGXLlmZbiRIl5Ovre9XXr1q1Sj/++KNWr14tHx8f1a9fX5MmTdKYMWMUExMjV1dXzZkzR0FBQZo2bZokKTg4WBs3btSMGTMIyQAAAH9BhX65xZUuXryoDz/8UH//+99ls9nM9vnz56ts2bKqXbu2oqOjde7cObMvMTFRderUkY+Pj9kWEREhh8OhXbt2mWPCw8OdjhUREaHExMRr1pKZmSmHw+G0AQAA4N5Q6O8kX2nx4sU6deqU+vTpY7Y99dRTCggIkJ+fn3bs2KExY8Zo7969+uyzzyRJaWlpTgFZkrmflpZ23TEOh0Pnz59X8eLFc9USGxurl19+OT9PDwAAAIXEXRWS3333XXXo0EF+fn5m2zPPPGP+uU6dOqpQoYLatGmjAwcOqEqVKretlujoaI0YMcLcdzgc8vf3v23HAwAAwJ1z14TkQ4cOafXq1eYd4msJDQ2VJO3fv19VqlSRr6+vvvvuO6cx6enpkmSuY/b19TXbrhzj6el51bvIkmS322W32/N0LgAAACjc7po1yXPnzlX58uUVGRl53XHJycmSpAoVKkiSwsLC9MMPP+jo0aPmmPj4eHl6eiokJMQck5CQ4DRPfHy8wsLC8vEMAAAAcLe4K0Jydna25s6dq969e6to0f9/8/vAgQOaNGmSkpKSdPDgQS1ZskS9evVSixYtVLduXUlSu3btFBISop49e2r79u1auXKlxo4dq6ioKPNO8MCBA/Xzzz/r+eef1549e/TWW2/p448/1vDhwwvkfAEAAFCw7oqQvHr1aqWmpurvf/+7U7urq6tWr16tdu3aqWbNmho5cqS6dOmiL7/80hzj4uKipUuXysXFRWFhYXr66afVq1cvp+cqBwUFadmyZYqPj1e9evU0bdo0/fe//+XxbwAAAH9Rd8Wa5Hbt2skwjFzt/v7+Wr9+/Q1fHxAQoOXLl193TKtWrfT999/nuUYAAADcO+6KO8kAAADAnURIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAACLQh2SY2JiZLPZnLaaNWua/RcuXFBUVJTKlCkjd3d3denSRenp6U5zpKamKjIyUiVKlFD58uU1evRoXb582WnMunXr1LBhQ9ntdlWtWlVxcXF34vQAAABQSBXqkCxJtWrV0pEjR8xt48aNZt/w4cP15ZdfatGiRVq/fr0OHz6sxx57zOzPyspSZGSkLl68qE2bNmnevHmKi4vT+PHjzTEpKSmKjIxU69atlZycrGHDhql///5auXLlHT1PAAAAFB5FC7qAGylatKh8fX1ztZ8+fVrvvvuuFixYoIceekiSNHfuXAUHB+vbb7/VAw88oFWrVunHH3/U6tWr5ePjo/r162vSpEkaM2aMYmJi5Orqqjlz5igoKEjTpk2TJAUHB2vjxo2aMWOGIiIi7ui5AgAAoHAo9HeS9+3bJz8/P1WuXFk9evRQamqqJCkpKUmXLl1SeHi4ObZmzZqqVKmSEhMTJUmJiYmqU6eOfHx8zDERERFyOBzatWuXOebKOXLG5MxxLZmZmXI4HE4bAAAA7g2FOiSHhoYqLi5OK1as0Ntvv62UlBQ1b95cZ86cUVpamlxdXeXt7e30Gh8fH6WlpUmS0tLSnAJyTn9O3/XGOBwOnT9//pq1xcbGysvLy9z8/f3/7OkCAACgkCjUyy06dOhg/rlu3boKDQ1VQECAPv74YxUvXrwAK5Oio6M1YsQIc9/hcBCUAQAA7hGF+k6ylbe3t6pXr679+/fL19dXFy9e1KlTp5zGpKenm2uYfX19cz3tImf/RmM8PT2vG8Ttdrs8PT2dNgAAANwb7qqQnJGRoQMHDqhChQpq1KiRihUrpoSEBLN/7969Sk1NVVhYmCQpLCxMP/zwg44ePWqOiY+Pl6enp0JCQswxV86RMyZnDgAAAPz1FOqQPGrUKK1fv14HDx7Upk2b9Le//U0uLi7q3r27vLy81K9fP40YMUJr165VUlKS+vbtq7CwMD3wwAOSpHbt2ikkJEQ9e/bU9u3btXLlSo0dO1ZRUVGy2+2SpIEDB+rnn3/W888/rz179uitt97Sxx9/rOHDhxfkqQMAAKAAFeo1yb/++qu6d++u48ePq1y5cnrwwQf17bffqly5cpKkGTNmqEiRIurSpYsyMzMVERGht956y3y9i4uLli5dqueee05hYWEqWbKkevfurYkTJ5pjgoKCtGzZMg0fPlwzZ85UxYoV9d///pfHvwEAAPyF2QzDMAq6iHuBw+GQl5eXTp8+fcfWJwe+sOyOHAdAwTo4ObKgSygQvMcBfw138j3uVvJaoV5uAQAAABQEQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAItCHZJjY2N1//33y8PDQ+XLl1fnzp21d+9epzGtWrWSzWZz2gYOHOg0JjU1VZGRkSpRooTKly+v0aNH6/Lly05j1q1bp4YNG8put6tq1aqKi4u73acHAACAQqpQh+T169crKipK3377reLj43Xp0iW1a9dOZ8+edRo3YMAAHTlyxNymTJli9mVlZSkyMlIXL17Upk2bNG/ePMXFxWn8+PHmmJSUFEVGRqp169ZKTk7WsGHD1L9/f61cufKOnSsAAAAKj6IFXcD1rFixwmk/Li5O5cuXV1JSklq0aGG2lyhRQr6+vledY9WqVfrxxx+1evVq+fj4qH79+po0aZLGjBmjmJgYubq6as6cOQoKCtK0adMkScHBwdq4caNmzJihiIiI23eCAAAAKJQK9Z1kq9OnT0uSSpcu7dQ+f/58lS1bVrVr11Z0dLTOnTtn9iUmJqpOnTry8fEx2yIiIuRwOLRr1y5zTHh4uNOcERERSkxMvGYtmZmZcjgcThsAAADuDYX6TvKVsrOzNWzYMDVr1ky1a9c225966ikFBATIz89PO3bs0JgxY7R371599tlnkqS0tDSngCzJ3E9LS7vuGIfDofPnz6t48eK56omNjdXLL7+cr+cIAACAwuGuCclRUVHauXOnNm7c6NT+zDPPmH+uU6eOKlSooDZt2ujAgQOqUqXKbasnOjpaI0aMMPcdDof8/f1v2/EAAABw59wVyy0GDRqkpUuXau3atapYseJ1x4aGhkqS9u/fL0ny9fVVenq605ic/Zx1zNca4+npedW7yJJkt9vl6enptAEAAODeUKhDsmEYGjRokD7//HOtWbNGQUFBN3xNcnKyJKlChQqSpLCwMP3www86evSoOSY+Pl6enp4KCQkxxyQkJDjNEx8fr7CwsHw6EwAAANxNCnVIjoqK0ocffqgFCxbIw8NDaWlpSktL0/nz5yVJBw4c0KRJk5SUlKSDBw9qyZIl6tWrl1q0aKG6detKktq1a6eQkBD17NlT27dv18qVKzV27FhFRUXJbrdLkgYOHKiff/5Zzz//vPbs2aO33npLH3/8sYYPH15g5w4AAICCU6hD8ttvv63Tp0+rVatWqlChgrl99NFHkiRXV1etXr1a7dq1U82aNTVy5Eh16dJFX375pTmHi4uLli5dKhcXF4WFhenpp59Wr169NHHiRHNMUFCQli1bpvj4eNWrV0/Tpk3Tf//7Xx7/BgAA8BdVqD+4ZxjGdfv9/f21fv36G84TEBCg5cuXX3dMq1at9P33399SfQAAALg3Feo7yQAAAEBBICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJAMAAAAWhGQAAADAgpAMAAAAWBCSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACwIyQAAAIAFIRkAAACwICQDAAAAFoRkAAAAwIKQDAAAAFgQkgEAAAALQjIAAABgQUgGAAAALAjJAAAAgAUhGQAAALAgJFvMnj1bgYGBcnNzU2hoqL777ruCLgkAAAB3GCH5Ch999JFGjBihCRMmaNu2bapXr54iIiJ09OjRgi4NAAAAdxAh+QrTp0/XgAED1LdvX4WEhGjOnDkqUaKE3nvvvYIuDQAAAHdQ0YIuoLC4ePGikpKSFB0dbbYVKVJE4eHhSkxMzDU+MzNTmZmZ5v7p06clSQ6H4/YX+3+yM8/dsWMBKDh38n2lMOE9DvhruJPvcTnHMgzjhmMJyf/n2LFjysrKko+Pj1O7j4+P9uzZk2t8bGysXn755Vzt/v7+t61GAH9NXq8XdAUAcPsUxHvcmTNn5OXldd0xhOQ8io6O1ogRI8z97OxsnThxQmXKlJHNZivAynAvczgc8vf31y+//CJPT8+CLgcA8hXvcbjdDMPQmTNn5Ofnd8OxhOT/U7ZsWbm4uCg9Pd2pPT09Xb6+vrnG2+122e12pzZvb+/bWSJg8vT05B8QAPcs3uNwO93oDnIOPrj3f1xdXdWoUSMlJCSYbdnZ2UpISFBYWFgBVgYAAIA7jTvJVxgxYoR69+6txo0bq0mTJnr99dd19uxZ9e3bt6BLAwAAwB1ESL5C165d9fvvv2v8+PFKS0tT/fr1tWLFilwf5gMKit1u14QJE3It9QGAewHvcShMbMbNPAMDAAAA+AthTTIAAABgQUgGAAAALAjJAAAAgAUhGbgLrFu3TjabTadOnbruuMDAQL3++ut3pCYAKEgxMTGqX79+QZeBexgf3APuAhcvXtSJEyfk4+Mjm82muLg4DRs2LFdo/v3331WyZEmVKFGiYAoFgNvAZrPp888/V+fOnc22jIwMZWZmqkyZMgVXGO5pPAIOuAu4urpe9Tc/WpUrV+4OVAMABc/d3V3u7u4FXQbuYSy3APJJq1atNGjQIA0aNEheXl4qW7asxo0bp5wf1pw8eVK9evVSqVKlVKJECXXo0EH79u0zX3/o0CE98sgjKlWqlEqWLKlatWpp+fLlkpyXW6xbt059+/bV6dOnZbPZZLPZFBMTI8l5ucVTTz2lrl27OtV46dIllS1bVu+//76kP36rZGxsrIKCglS8eHHVq1dPn3zyyW2+UgDuFq1atdKQIUP0/PPPq3Tp0vL19TXfbyTp1KlT6t+/v8qVKydPT0899NBD2r59u9Mcr7zyisqXLy8PDw/1799fL7zwgtMyiS1btqht27YqW7asvLy81LJlS23bts3sDwwMlCT97W9/k81mM/evXG6xatUqubm55frp2tChQ/XQQw+Z+xs3blTz5s1VvHhx+fv7a8iQITp79uyfvk64NxGSgXw0b948FS1aVN99951mzpyp6dOn67///a8kqU+fPtq6dauWLFmixMREGYahjh076tKlS5KkqKgoZWZmasOGDfrhhx/02muvXfUuSdOmTfX666/L09NTR44c0ZEjRzRq1Khc43r06KEvv/xSGRkZZtvKlSt17tw5/e1vf5MkxcbG6v3339ecOXO0a9cuDR8+XE8//bTWr19/Oy4PgLvQvHnzVLJkSW3evFlTpkzRxIkTFR8fL0l64okndPToUX311VdKSkpSw4YN1aZNG504cUKSNH/+fL366qt67bXXlJSUpEqVKuntt992mv/MmTPq3bu3Nm7cqG+//VbVqlVTx44ddebMGUl/hGhJmjt3ro4cOWLuX6lNmzby9vbWp59+arZlZWXpo48+Uo8ePSRJBw4cUPv27dWlSxft2LFDH330kTZu3KhBgwbl/0XDvcEAkC9atmxpBAcHG9nZ2WbbmDFjjODgYOOnn34yJBnffPON2Xfs2DGjePHixscff2wYhmHUqVPHiImJuerca9euNSQZJ0+eNAzDMObOnWt4eXnlGhcQEGDMmDHDMAzDuHTpklG2bFnj/fffN/u7d+9udO3a1TAMw7hw4YJRokQJY9OmTU5z9OvXz+jevfstnz+Ae0/Lli2NBx980Knt/vvvN8aMGWN8/fXXhqenp3HhwgWn/ipVqhj//ve/DcMwjNDQUCMqKsqpv1mzZka9evWuecysrCzDw8PD+PLLL802Scbnn3/uNG7ChAlO8wwdOtR46KGHzP2VK1cadrvdfN/s16+f8cwzzzjN8fXXXxtFihQxzp8/f8168NfFnWQgHz3wwAOy2WzmflhYmPbt26cff/xRRYsWVWhoqNlXpkwZ1ahRQ7t375YkDRkyRK+88oqaNWumCRMmaMeOHX+qlqJFi+rJJ5/U/PnzJUlnz57VF198Yd5V2b9/v86dO6e2bduaa/vc3d31/vvv68CBA3/q2ADuHXXr1nXar1Chgo4ePart27crIyNDZcqUcXoPSUlJMd9D9u7dqyZNmji93rqfnp6uAQMGqFq1avLy8pKnp6cyMjKUmpp6S3X26NFD69at0+HDhyX9cRc7MjJS3t7ekqTt27crLi7OqdaIiAhlZ2crJSXllo6FvwY+uAcUEv3791dERISWLVumVatWKTY2VtOmTdPgwYPzPGePHj3UsmVLHT16VPHx8SpevLjat28vSeYyjGXLlum+++5zep3dbs/7iQC4pxQrVsxp32azKTs7WxkZGapQoYLWrVuX6zU5wfRm9O7dW8ePH9fMmTMVEBAgu92usLAwXbx48ZbqvP/++1WlShUtXLhQzz33nD7//HPFxcWZ/RkZGXr22Wc1ZMiQXK+tVKnSLR0Lfw2EZCAfbd682Wk/Z31dSEiILl++rM2bN6tp06aSpOPHj2vv3r0KCQkxx/v7+2vgwIEaOHCgoqOj9Z///OeqIdnV1VVZWVk3rKdp06by9/fXRx99pK+++kpPPPGE+Q9eSEiI7Ha7UlNT1bJlyz9z2gD+gho2bKi0tDQVLVrU/DCdVY0aNbRlyxb16tXLbLOuKf7mm2/01ltvqWPHjpKkX375RceOHXMaU6xYsZt6z+vRo4fmz5+vihUrqkiRIoqMjHSq98cff1TVqlVv9hTxF8dyCyAfpaamasSIEdq7d6/+97//adasWRo6dKiqVaumTp06acCAAdq4caO2b9+up59+Wvfdd586deokSRo2bJhWrlyplJQUbdu2TWvXrlVwcPBVjxMYGKiMjAwlJCTo2LFjOnfu3DVreuqppzRnzhzFx8ebSy0kycPDQ6NGjdLw4cM1b948HThwQNu2bdOsWbM0b968/L0wAO454eHhCgsLU+fOnbVq1SodPHhQmzZt0ksvvaStW7dKkgYPHqx3331X8+bN0759+/TKK69ox44dTsvSqlWrpg8++EC7d+/W5s2b1aNHDxUvXtzpWIGBgUpISFBaWppOnjx5zZp69Oihbdu26dVXX9Xjjz/u9FOxMWPGaNOmTRo0aJCSk5O1b98+ffHFF3xwD9dESAbyUa9evXT+/Hk1adJEUVFRGjp0qJ555hlJf3wyu1GjRnr44YcVFhYmwzC0fPly885uVlaWoqKiFBwcrPbt26t69ep66623rnqcpk2bauDAgeratavKlSunKVOmXLOmHj166Mcff9R9992nZs2aOfVNmjRJ48aNU2xsrHncZcuWKSgoKJ+uCIB7lc1m0/Lly9WiRQv17dtX1atXV7du3XTo0CH5+PhI+uP9Jzo6WqNGjVLDhg2VkpKiPn36yM3NzZzn3Xff1cmTJ9WwYUP17NlTQ4YMUfny5Z2ONW3aNMXHx8vf318NGjS4Zk1Vq1ZVkyZNtGPHDqebAtIfa6vXr1+vn376Sc2bN1eDBg00fvx4+fn55eNVwb2E37gH5JNWrVqpfv36/FpoALiOtm3bytfXVx988EFBlwJcF2uSAQDAbXHu3DnNmTNHERERcnFx0f/+9z+tXr3afM4yUJgRkgEAwG2RsyTj1Vdf1YULF1SjRg19+umnCg8PL+jSgBtiuQUAAABgwQf3AAAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkA0A+WrdunWw2m06dOlXQpeQ7m82mxYsX3/bjXO0aLl68WFWrVpWLi4uGDRumuLg4eXt739ZjAvhrIyQDuOf8/vvveu6551SpUiXZ7Xb5+voqIiJC33zzTb4ep1WrVho2bJhTW9OmTXXkyBF5eXnl67Hyok+fPurcufNNjU1LS9PgwYNVuXJl2e12+fv765FHHlFCQsLtLfIqrnYNn332WT3++OP65ZdfNGnSJHXt2lU//fTTbTtmfodwAHcffpkIgHtOly5ddPHiRc2bN0+VK1dWenq6EhISdPz48dt+bFdXV/n6+t724+SngwcPqlmzZvL29tbUqVNVp04dXbp0SStXrlRUVJT27NlzR+uxXsOMjAwdPXpUERER8vPzM9uLFy9+244JADIA4B5y8uRJQ5Kxbt26G47r16+fUbZsWcPDw8No3bq1kZycbPZPmDDBqFevnvH+++8bAQEBhqenp9G1a1fD4XAYhmEYvXv3NiQ5bSkpKcbatWsNScbJkycNwzCMuXPnGl5eXsaXX35pVK9e3ShevLjRpUsX4+zZs0ZcXJwREBBgeHt7G4MHDzYuX75sHv/ChQvGyJEjDT8/P6NEiRJGkyZNjLVr15r9OfOuWLHCqFmzplGyZEkjIiLCOHz4sFm/tb4rX3+lDh06GPfdd5+RkZFx1euUQ5Lx+eefm/vPP/+8Ua1aNaN48eJGUFCQMXbsWOPixYtmf3JystGqVSvD3d3d8PDwMBo2bGhs2bLFMAzDOHjwoPHwww8b3t7eRokSJYyQkBBj2bJlhmEYTtcw58/W88g5/ystWbLEaNy4sWG3240yZcoYnTt3Nvvef/99o1GjRoa7u7vh4+NjdO/e3UhPTzf7b3TMCRMmGIZhGCdOnDB69uxpeHt7G8WLFzfat29v/PTTTzf9dQFw92C5BYB7iru7u9zd3bV48WJlZmZec9wTTzyho0eP6quvvlJSUpIaNmyoNm3a6MSJE+aYAwcOaPHixVq6dKmWLl2q9evXa/LkyZKkmTNnKiwsTAMGDNCRI0d05MgR+fv7X/VY586d0xtvvKGFCxdqxYoVWrdunf72t79p+fLlWr58uT744AP9+9//1ieffGK+ZtCgQUpMTNTChQu1Y8cOPfHEE2rfvr327dvnNO+//vUvffDBB9qwYYNSU1M1atQoSdKoUaP05JNPqn379mZ9TZs2zVXbiRMntGLFCkVFRalkyZK5+q+35MDDw0NxcXH68ccfNXPmTP3nP//RjBkzzP4ePXqoYsWK2rJli5KSkvTCCy+oWLFikqSoqChlZmZqw4YN+uGHH/Taa6/J3d091zGaNm2qvXv3SpI+/fTTa57HsmXL9Le//U0dO3bU999/r4SEBDVp0sTsv3TpkiZNmqTt27dr8eLFOnjwoPr06XPV82ratKlef/11eXp6mtcu57r26dNHW7du1ZIlS5SYmCjDMNSxY0ddunTJfP31vi4A7iIFndIBIL998sknRqlSpQw3NzejadOmRnR0tLF9+3az/+uvvzY8PT2NCxcuOL2uSpUqxr///W/DMP64E1uiRAnzzrFhGMbo0aON0NBQc79ly5bG0KFDnea42p1kScb+/fvNMc8++6xRokQJ48yZM2ZbRESE8eyzzxqGYRiHDh0yXFxcjN9++81p7jZt2hjR0dHXnHf27NmGj4+Pud+7d2+jU6dO171WmzdvNiQZn3322XXHGUbuO8lWU6dONRo1amTue3h4GHFxcVcdW6dOHSMmJuaqfdZrmPPTgavdSc8RFhZm9OjR44bnkGPLli2GJPNrcK2fAFzpp59+MiQZ33zzjdl27Ngxo3jx4sbHH39svu5GXxcAdwfuJAO453Tp0kWHDx/WkiVL1L59e61bt04NGzZUXFycJGn79u3KyMhQmTJlzDvP7u7uSklJ0YEDB8x5AgMD5eHhYe5XqFBBR48eveV6SpQooSpVqpj7Pj4+CgwMdLpz6uPjY879ww8/KCsrS9WrV3eqb/369U71WefNS32GYdzy+eT46KOP1KxZM/n6+srd3V1jx45Vamqq2T9ixAj1799f4eHhmjx5slPtQ4YM0SuvvKJmzZppwoQJ2rFjR57rkKTk5GS1adPmmv1JSUl65JFHVKlSJXl4eKhly5aS5FTvjezevVtFixZVaGio2VamTBnVqFFDu3fvNtvy4+sCoOARkgHck9zc3NS2bVuNGzdOmzZtUp8+fTRhwgRJf3wQrEKFCkpOTnba9u7dq9GjR5tz5CwNyGGz2ZSdnX3LtVxtnuvNnZGRIRcXFyUlJTnVt3v3bs2cOfO6895q6K1WrZpsNtstfzgvMTFRPXr0UMeOHbV06VJ9//33eumll3Tx4kVzTExMjHbt2qXIyEitWbNGISEh+vzzzyVJ/fv3188//6yePXvqhx9+UOPGjTVr1qxbquFK1/sQ39mzZxURESFPT0/Nnz9fW7ZsMeu4st78kh9fFwAFj5AM4C8hJCREZ8+elSQ1bNhQaWlpKlq0qKpWreq0lS1b9qbndHV1VVZWVr7X2qBBA2VlZeno0aO56ruVJzDcTH2lS5dWRESEZs+ebV6fK13rucGbNm1SQECAXnrpJTVu3FjVqlXToUOHco2rXr26hg8frlWrVumxxx7T3LlzzT5/f38NHDhQn332mUaOHKn//Oc/N31uVnXr1r3m4+r27Nmj48ePa/LkyWrevLlq1qx5wzu7V7t2wcHBunz5sjZv3my2HT9+XHv37lVISEieawdQOBGSAdxTjh8/roceekgffvihduzYoZSUFC1atEhTpkxRp06dJEnh4eEKCwtT586dtWrVKh08eFCbNm3SSy+9pK1bt970sQIDA7V582YdPHhQx44dy9Nd5qupXr26evTooV69eumzzz5TSkqKvvvuO8XGxmrZsmW3VN+OHTu0d+9eHTt2zOnDZVeaPXu2srKy1KRJE3366afat2+fdu/erTfeeENhYWFXfU21atWUmpqqhQsX6sCBA3rjjTfMu7OSdP78eQ0aNEjr1q3ToUOH9M0332jLli0KDg6WJA0bNkwrV65USkqKtm3bprVr15p9eTFhwgT973//04QJE7R7927zw4CSVKlSJbm6umrWrFn6+eeftWTJEk2aNOm68wUGBiojI0MJCQk6duyYzp07p2rVqqlTp04aMGCANm7cqO3bt+vpp5/WfffdZ/7dAnDvICQDuKe4u7srNDRUM2bMUIsWLVS7dm2NGzdOAwYM0Jtvvinpjx9/L1++XC1atFDfvn1VvXp1devWTYcOHZKPj89NH2vUqFFycXFRSEiIypUrd0vrW29k7ty56tWrl0aOHKkaNWqoc+fO2rJliypVqnTTcwwYMEA1atRQ48aNVa5cuWv+MpXKlStr27Ztat26tUaOHKnatWurbdu2SkhI0Ntvv33V1zz66KMaPny4Bg0apPr162vTpk0aN26c2e/i4qLjx4+rV69eql69up588kl16NBBL7/8siQpKytLUVFRCg4OVvv27VW9enW99dZbt3CFnLVq1UqLFi3SkiVLVL9+fT300EP67rvvJEnlypVTXFycFi1apJCQEE2ePFn/+te/rjtf06ZNNXDgQHXt2lXlypXTlClTJP3xdWnUqJEefvhhhYWFyTAMLV++PNcSCwB3P5vBQikAAADACXeSAQAAAAtCMgAAAGBBSAYAAAAsCMkAAACABSEZAAAAsCAkAwAAABaEZAAAAMCCkAwAAABYEJIBAAAAC0IyAAAAYEFIBgAAACz+H7jEUXLhX6EmAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentiment_distribution = Counter(sentiment_ratings)\n",
        "total_data = len(sentiment_ratings)\n",
        "\n",
        "print(\"Sentiment distribution:\")\n",
        "for sentiment, count in sentiment_distribution.items():\n",
        "    percentage = (count / total_data) * 100\n",
        "    print(f\"{sentiment}: {count} ({percentage:.2f}%)\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(sentiment_distribution.keys(), sentiment_distribution.values())\n",
        "plt.xlabel('Sentiment Classificaiton')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution in Full Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Fc8KrpQhYV"
      },
      "source": [
        "**Tokenising Corpus Dataset**\n",
        "<br/>\n",
        "Tokenising the words all the words in the reviews database. This will then be used to create the list of features, which would be the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TjHzxsgNNLH4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenising it by spaces\n",
        "tokenised_set = []\n",
        "for review in reviews:\n",
        "  # Basically, re.split(' ') results in an array of words split by spaces\n",
        "  # Then iterate through that array of words and append it individually to tokenised_set\n",
        "  [tokenised_set.append(tokens) for tokens in re.split(' ', review)]\n",
        "\n",
        "counts = Counter(tokenised_set)\n",
        "so_with_content=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "so_with_content=list(zip(*so_with_content))[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We want to focus on content words\n",
        "function_words = []\n",
        "with open(\"function_words.txt\") as f:\n",
        "    function_words = f.read().splitlines()\n",
        "so: list[str] = []\n",
        "for word in so_with_content:\n",
        "    if word not in function_words and len(word) > 2:\n",
        "        so.append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Creating the sparse embedding**\n",
        "<br/>\n",
        "This is a simple sparse embedding of filtered content words of corpus. This will be used as the main features for all classification model. \n",
        "\n",
        "**Ensuring Reproducibility**\n",
        "<br/>\n",
        "To ensure reproducibility, we will set the random seed to 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random \n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# 8000 Features\n",
        "word_list = so[0:8000]\n",
        "M = np.zeros((len(reviews), len(word_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "  # Ensure we are looking at word\n",
        "  rev = rev.split(' ')\n",
        "  for(j,word) in enumerate(word_list):\n",
        "    if word in rev:\n",
        "      M[i,j]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36548, 8000)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 36548 examples, with 8000 features (words occurence)\n",
        "M.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ints = np.random.choice(len(reviews), int(len(reviews)*0.6), replace=False)\n",
        "test_train_ints = list(set(range(0, len(reviews))) - set(train_ints))\n",
        "test_ints = np.random.choice(test_train_ints, int(len(test_train_ints)*0.5), replace=False)\n",
        "final_test_ints = list(set(test_train_ints) - set(test_ints))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training test 21928\n",
            "Validation test 7310\n",
            "Final test 7310\n",
            "Total 36548\n"
          ]
        }
      ],
      "source": [
        "print(\"Training test\", len(train_ints))\n",
        "print(\"Validation test\", len(test_ints))\n",
        "print(\"Final test\", len(final_test_ints))\n",
        "print(\"Total\", len(train_ints)+len(test_ints)+len(final_test_ints))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-EDhKyxxtMl"
      },
      "source": [
        "**Classifiers for Sentiment Analysis**\n",
        "<br>\n",
        "Multiple classifiers will be run, and tested againts each other for sentiment analysis classifier.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yW_OPROzSFl"
      },
      "source": [
        "Experiment for 8000 features, meaning 8000 words\n",
        "<br>\n",
        "Below we divide the data into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "D5zci15pCO7t"
      },
      "outputs": [],
      "source": [
        "# Divide the features by the training indices\n",
        "# Select all rows that are in the indices of the respective lists and select all the rows\n",
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "M_final_test = M[final_test_ints,]\n",
        "sentiment_labels = [sentiment_ratings[i] for i in train_ints]\n",
        "sentiment_labels_test = [sentiment_ratings[i] for i in test_ints]\n",
        "sentiment_labels_final_test = [sentiment_ratings[i] for i in final_test_ints]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'positive': 20972, 'negative': 15576})\n",
            "Counter({'positive': 4190, 'negative': 3120})\n",
            "Counter({'positive': 4228, 'negative': 3082})\n",
            "Counter({'positive': 12554, 'negative': 9374})\n"
          ]
        }
      ],
      "source": [
        "# Class Distribution Test to check\n",
        "class_distribution = Counter(sentiment_ratings)\n",
        "class_distribution_test = Counter(sentiment_labels_test)\n",
        "class_distribution_final_test = Counter(sentiment_labels_final_test)\n",
        "class_distribution_train = Counter(sentiment_labels)\n",
        "\n",
        "print(class_distribution)\n",
        "print(class_distribution_test)\n",
        "print(class_distribution_final_test)\n",
        "print(class_distribution_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUfBV2eCHOeK",
        "outputId": "dad11401-36e1-4801-85f1-7b57602092fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21928, 8000)\n",
            "(7310, 8000)\n",
            "(7310, 8000)\n",
            "21928\n",
            "7310\n",
            "7310\n"
          ]
        }
      ],
      "source": [
        "print(M_train.shape)\n",
        "print(M_test.shape)\n",
        "print(M_final_test.shape)\n",
        "# Sentiment Labels are ordered list\n",
        "print(len(sentiment_labels))\n",
        "print(len(sentiment_labels_test))\n",
        "print(len(sentiment_labels_final_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Function for Classifiers**\n",
        "<br/>\n",
        "This function will be called multiple times to test out different hyperparameters and their effects. \n",
        "<br/>\n",
        "In each training, the function will perform a test on the sentiment_labels_test after finishing training on sentiment_labels_train.\n",
        "<br/>\n",
        "The best performing models will be tested further with the last 20 percent to test whether they generalise well or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to run Logistic Regression with Sigmoid\n",
        "def logistic_regresssion_sigmoid(x_dataset,y_dataset,x_test,y_test,num_features,n_iters=1000,lr=0.4,random_seed=42):\n",
        "    np.random.seed(random_seed)\n",
        "    weights = np.random.rand(num_features)\n",
        "    bias=np.random.rand(1)\n",
        "    logistic_loss=[]\n",
        "    num_samples=len(y_dataset)\n",
        "    for i in range(n_iters):\n",
        "        # Basically you are multiplying all the values of M_train with the weights\n",
        "        # It would be similar to this: z= bias + (x[0]*weights[0] + x[1]*weights[1])\n",
        "        # The values here would be 21928, 5000 and 5000, 1, leading to a matrix of 21928, 1\n",
        "        z= x_dataset.dot(weights) + bias\n",
        "        # print(z)\n",
        "        # (1 / (1+np.exp(-z))) we use sigmoid because we only need to know whether it is positive or negative, two possible values\n",
        "        q = (1 / (1+np.exp(-z)))\n",
        "        # print(q)\n",
        "        eps=0.00001\n",
        "        # Binary Cross Entropy Loss\n",
        "        loss = -np.sum((y_dataset*np.log2(q+eps)+(np.ones(len(y_dataset))-y_dataset)*np.log2(np.ones(len(y_dataset))-q+eps)))/num_samples\n",
        "        \n",
        "        \n",
        "        logistic_loss.append(loss)\n",
        "        # print(logistic_loss)\n",
        "        # We then make the prediction, if it is below a certain number, 0.5 it is negative and vice versa\n",
        "        y_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "        # For logistic regression one shot encoder= dw1 = np.dot(x[0],q-y)/num_samples\n",
        "        # dw1 = np.do(x[0], q-y)/num_samples\n",
        "\n",
        "        dw = (q-y_dataset).dot(x_dataset)/num_samples\n",
        "        db = np.sum(q-y_dataset)/num_samples\n",
        "        weights = weights - dw*lr\n",
        "        bias = bias - db*lr\n",
        "\n",
        "    \n",
        "    # Model test on validation dataset \n",
        "    result = LogReg_Sigmoid_Test(weights,bias,x_test,y_test)\n",
        "    return weights,bias, result, logistic_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to invoke the function and test results \n",
        "# For this specific function, we expect the labels to be already in string format\n",
        "def LogReg_Sigmoid_Test(weights,bias,test_dataset,y_test:list):\n",
        "    # Perform Forward Propagation\n",
        "    z= test_dataset.dot(weights) + bias\n",
        "    q = (1 / (1+np.exp(-z)))\n",
        "    x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "    \n",
        "    \n",
        "\n",
        "    # # Accuracy\n",
        "    # y_final_test = [int(l=='positive') for l in sentiment_labels_final_test]\n",
        "    # acc_test = [int(yp == y_final_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "    # print(\"Final Test Accuracy: \", sum(acc_test)/len(acc_test))\n",
        "    # y_test_compare = [\"positive\" if s == 1 else \"negative\" for s in y_test ]\n",
        "    x_labels=[\"positive\" if s == 1 else \"negative\" for s in x_test_pred]\n",
        "    \n",
        "    # TP\n",
        "    true_positives=sum([int(yt == \"positive\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "\n",
        "    true_negatives=sum([int(yt == \"negative\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    false_positives = sum([int(yt == \"negative\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "    # FN: actual is POSITIVE, predicted is NEGATIVE  \n",
        "    false_negatives = sum([int(yt == \"positive\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    \n",
        "    # print(\"True Positives: \", true_positives)\n",
        "    # print(\"False Positives: \", false_positives)\n",
        "    # print(\"False Negatives: \", false_negatives)\n",
        "    # print(\"True Negatives: \", true_negatives)\n",
        "    # Precision\n",
        "    accuracy = (true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
        "    precision = true_positives/(true_positives+false_positives)\n",
        "    recall = true_positives/(true_positives+false_negatives)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(\"--------------\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1: \", f1)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "    return precision, recall, f1, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Different Features forLogistic Regression with Logistic Regression and MLP**\n",
        "<br/>\n",
        "- 8000 features \n",
        "- Learning Rate \n",
        "    - 0.4\n",
        "    - 0.2\n",
        "    - 0.1\n",
        "    - 0.05\n",
        "    <br/>\n",
        "- This will all be done by running multiple classifiers and storing the results in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of hyperparameters to be used for training the model\n",
        "learning_rates = [0.4,0.2,0.1,0.05]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.4\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [130], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m learning_rates:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate: \u001b[39m\u001b[38;5;124m\"\u001b[39m,lr)\n\u001b[0;32m----> 6\u001b[0m     weights,bias,result,logistic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_regresssion_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentiment_labels_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     LogReg_models_sigmoid[lr] \u001b[38;5;241m=\u001b[39m (weights,bias,result,logistic_loss)\n",
            "Cell \u001b[0;32mIn [127], line 30\u001b[0m, in \u001b[0;36mlogistic_regresssion_sigmoid\u001b[0;34m(x_dataset, y_dataset, x_test, y_test, num_features, n_iters, lr, random_seed)\u001b[0m\n\u001b[1;32m     25\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mint\u001b[39m(ql \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ql \u001b[38;5;129;01min\u001b[39;00m q]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# For logistic regression one shot encoder= dw1 = np.dot(x[0],q-y)/num_samples\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# dw1 = np.do(x[0], q-y)/num_samples\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m dw \u001b[38;5;241m=\u001b[39m (q\u001b[38;5;241m-\u001b[39my_dataset)\u001b[38;5;241m.\u001b[39mdot(x_dataset)\u001b[38;5;241m/\u001b[39mnum_samples\n\u001b[1;32m     31\u001b[0m db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(q\u001b[38;5;241m-\u001b[39my_dataset)\u001b[38;5;241m/\u001b[39mnum_samples\n\u001b[1;32m     32\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m-\u001b[39m dw\u001b[38;5;241m*\u001b[39mlr\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# invoking the function\n",
        "y=[int(l == \"positive\") for l in sentiment_labels]\n",
        "LogReg_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = logistic_regresssion_sigmoid(M_train,y,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    LogReg_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Multi-Layer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MLP_sigmoid(x_dataset,y_dataset,x_test,y_test,num_features,n_iters=1000,lr=0.4,random_seed=42):\n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    num_features= x_dataset.shape[1]\n",
        "    hidden_size = 32\n",
        "\n",
        "    \n",
        "    \n",
        "    # Weight initialization with He/Xavier weight initialization technique to encourage ReLU activation\n",
        "    # Np.random will not work\n",
        "    limit_1 = np.sqrt(6 / (num_features ))\n",
        "    weights_0_1 = np.random.uniform(-limit_1, limit_1, (num_features, hidden_size))\n",
        "    limit_2 = np.sqrt(6 / (hidden_size ))\n",
        "    weights_1_2 = np.random.uniform(-limit_2, limit_2, (hidden_size,1))\n",
        "\n",
        "\n",
        "    loss_history = []\n",
        "    \n",
        "    N = x_dataset.shape[0] # Number of training samples\n",
        "\n",
        "    for iteration in range(n_iters):\n",
        "\n",
        "        layer_2_error = 0\n",
        "        layer_0 = x_dataset\n",
        "\n",
        "        ## Add forward pass\n",
        "        layer_1 = np.maximum(np.dot(layer_0,weights_0_1),0)\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "    \n",
        "\n",
        "\n",
        "        # Then apply sigmoid\n",
        "        layer_2_s = 1/(1+np.exp(-layer_2))\n",
        "\n",
        "    \n",
        "        eps = 1e-8\n",
        "        q = np.clip(layer_2_s, eps, 1 - eps)\n",
        "        # BCE Cross Entropy Loss with clipping\n",
        "        loss = (-np.sum(y_dataset * np.log2(q) + (1 - y_dataset) * np.log2(1 - q)))/N\n",
        "        \n",
        "        loss_history.append(loss)\n",
        "        print(loss)\n",
        "\n",
        "        ## Add backward pass and update weights\n",
        "        layer_2_diff = (layer_2_s - y_dataset)\n",
        "\n",
        "        z1 = np.dot(layer_0, weights_0_1)\n",
        "        relu_grad = (z1 > 0).astype(float)\n",
        "\n",
        "\n",
        "\n",
        "        hidden_delta = np.dot(layer_2_diff, weights_1_2.T) * relu_grad\n",
        "\n",
        "        # Normalize weight updates by N\n",
        "        weights_1_2 -= lr * (np.dot(layer_1.T, layer_2_diff) / N)\n",
        "        weights_0_1 -= lr * (np.dot(layer_0.T, hidden_delta) / N)\n",
        "\n",
        "    result = test_models_MLP_Sigmoid(weights_0_1,weights_1_2,x_test,y_test)\n",
        "    return weights_0_1,weights_1_2,result, loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code to invoke the function and test results \n",
        "# For this specific function, we expect the labels to be already in string format\n",
        "def test_models_MLP_Sigmoid(weights_MLP_1,weights_MLP_2,x_dataset,y_test:list):\n",
        "    # Forward propagation\n",
        "    layer_0_final_test = x_dataset\n",
        "    layer_1_final_test = np.maximum(np.dot(layer_0_final_test, weights_MLP_1), 0)\n",
        "    layer_2_final_test = np.dot(layer_1_final_test, weights_MLP_2)\n",
        "    layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "    #Converting probabilities\n",
        "    x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "\n",
        "    \n",
        "\n",
        "    # # Accuracy\n",
        "    x_labels=[\"positive\" if s == 1 else \"negative\" for s in x_pred]\n",
        "    \n",
        "    # TP\n",
        "    true_positives=np.sum([int(yt == \"positive\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "\n",
        "    true_negatives=np.sum([int(yt == \"negative\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    false_positives = np.sum([int(yt == \"negative\" and x_labels[s] == \"positive\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    # FN: actual is POSITIVE, predicted is NEGATIVE  \n",
        "    false_negatives = np.sum([int(yt == \"positive\" and x_labels[s] == \"negative\") for s,yt in enumerate(y_test)])\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Precision\n",
        "    accuracy = (true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
        "    precision = true_positives/(true_positives+false_positives)\n",
        "    recall = true_positives/(true_positives+false_negatives)\n",
        "    f1 = 2*precision*recall/(precision+recall)\n",
        "    print(\"----------\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F1: \", f1)\n",
        "    print(\"accuracy: \", accuracy)\n",
        "    return precision, recall, f1, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*******\n",
            "Learning rate:  0.4\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [121], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*******\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate: \u001b[39m\u001b[38;5;124m\"\u001b[39m,lr)\n\u001b[0;32m----> 7\u001b[0m weights,bias,result,logistic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mMLP_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentiment_labels_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m MLP_models_sigmoid[lr] \u001b[38;5;241m=\u001b[39m (weights,bias,result,logistic_loss)\n",
            "Cell \u001b[0;32mIn [119], line 47\u001b[0m, in \u001b[0;36mMLP_sigmoid\u001b[0;34m(x_dataset, y_dataset, x_test, y_test, num_features, n_iters, lr)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m## Add backward pass and update weights\u001b[39;00m\n\u001b[1;32m     45\u001b[0m layer_2_diff \u001b[38;5;241m=\u001b[39m (layer_2_s \u001b[38;5;241m-\u001b[39m y_dataset)\n\u001b[0;32m---> 47\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_0_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m relu_grad \u001b[38;5;241m=\u001b[39m (z1 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     52\u001b[0m hidden_delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(layer_2_diff, weights_1_2\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m*\u001b[39m relu_grad\n",
            "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).reshape(-1,1)\n",
        "# invoking the function\n",
        "MLP_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"*******\")\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = MLP_sigmoid(M_train,true_labels,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    MLP_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Further Analysis on Result**\n",
        "<br/>\n",
        "Below we gather the best perfoming models and perform further analysis on the result, comparing the two models, MLP and Logistic Regression. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "import builtins\n",
        "sum = builtins.sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate:  0.4\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [126], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m learning_rates:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate: \u001b[39m\u001b[38;5;124m\"\u001b[39m,lr)\n\u001b[0;32m---> 12\u001b[0m     weights,bias,result,logistic_loss \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_regresssion_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentiment_labels_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     LogReg_models_sigmoid[lr] \u001b[38;5;241m=\u001b[39m (weights,bias,result,logistic_loss)\n",
            "Cell \u001b[0;32mIn [86], line 30\u001b[0m, in \u001b[0;36mlogistic_regresssion_sigmoid\u001b[0;34m(x_dataset, y_dataset, x_test, y_test, num_features, n_iters, lr, random_seed)\u001b[0m\n\u001b[1;32m     25\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mint\u001b[39m(ql \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ql \u001b[38;5;129;01min\u001b[39;00m q]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# For logistic regression one shot encoder= dw1 = np.dot(x[0],q-y)/num_samples\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# dw1 = np.do(x[0], q-y)/num_samples\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m dw \u001b[38;5;241m=\u001b[39m (q\u001b[38;5;241m-\u001b[39my_dataset)\u001b[38;5;241m.\u001b[39mdot(x_dataset)\u001b[38;5;241m/\u001b[39mnum_samples\n\u001b[1;32m     31\u001b[0m db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(q\u001b[38;5;241m-\u001b[39my_dataset)\u001b[38;5;241m/\u001b[39mnum_samples\n\u001b[1;32m     32\u001b[0m weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m-\u001b[39m dw\u001b[38;5;241m*\u001b[39mlr\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training and gathering the best performing models\n",
        "LogReg_models_sigmoid = {}\n",
        "LogReg_models_softmax = {}\n",
        "MLP_models_sigmoid = {}\n",
        "MLP_models_softmax = {}\n",
        "\n",
        "# 0.4 as a learning rate seems to result in best perfomance for MLP and Logistic Regression binary classification\n",
        "learning_rates = [0.4]\n",
        "y=[int(l == \"positive\") for l in sentiment_labels]\n",
        "for lr in learning_rates:\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = logistic_regresssion_sigmoid(M_train,y,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    LogReg_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "\n",
        "# learning_rates = [0.4,0.2]\n",
        "true_labels = np.array([int(l=='positive') for l in sentiment_labels]).reshape(-1,1)\n",
        "# invoking the function\n",
        "MLP_models_sigmoid = {}\n",
        "for lr in learning_rates:\n",
        "    print(\"*******\")\n",
        "    print(\"Learning rate: \",lr)\n",
        "    weights,bias,result,logistic_loss = MLP_sigmoid(M_train,true_labels,M_test,sentiment_labels_test,lr=lr,num_features=8000)\n",
        "    print(\"-----\")\n",
        "    MLP_models_sigmoid[lr] = (weights,bias,result,logistic_loss)\n",
        "\n",
        "# learning_rates = [0.4]\n",
        "# LogReg_models_softmax = {}\n",
        "# for lr in learning_rates:\n",
        "#     print(\"Learning rate: \",lr)\n",
        "#     weights,bias,result,loss = LogisticRegression_Softmax(M_train,y_train,M_test,y_test,lr=lr) \n",
        "#     print(\"-----\")\n",
        "#     LogReg_models_softmax[lr] = (weights,bias,result,loss)\n",
        "\n",
        "# learning_rates = [0.05]\n",
        "# MLP_models_softmax = {}\n",
        "# for lr in learning_rates:\n",
        "#     print(\"Learning rate: \",lr)\n",
        "#     weights,bias,result,loss = MLP_softmax(M_train,y_train,M_test,y_test,lr=lr) \n",
        "#     print(\"-----\")\n",
        "#     MLP_models_softmax[lr] = (weights,bias,result,loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Methods to perform forward propagation to conduct further error slice analysis \n",
        "def LogReg_ForwardPass(weights, bias, x_dataset, Sigmoid:bool):\n",
        "    if(Sigmoid):\n",
        "        z= x_dataset.dot(weights) + bias\n",
        "        q = (1 / (1+np.exp(-z)))\n",
        "        x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "        return x_test_pred, q\n",
        "    else:\n",
        "       z_test = x_dataset.dot(weights) + bias\n",
        "       # Perform theSoftmax\n",
        "       exp_z_test = np.exp(z_test)\n",
        "       q_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "       # Get predictions, basically get the class with highest probability\n",
        "       y_test_pred = np.argmax(q_test, axis=1)  # Shape: (7310,)\n",
        "       return y_test_pred,q_test\n",
        "\n",
        "def MLP_ForwardPass(weights_0_1,weights_1_2,x_dataset, Sigmoid:bool):\n",
        "    if(Sigmoid):\n",
        "        layer_0_final_test = x_dataset\n",
        "        layer_1_final_test = np.maximum(np.dot(layer_0_final_test, weights_0_1), 0)\n",
        "        layer_2_final_test = np.dot(layer_1_final_test, weights_1_2)\n",
        "        layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "        #Converting probabilities\n",
        "        x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "        return x_pred, layer_2_s_final_test\n",
        "    else:\n",
        "        layer_0_test = x_dataset    \n",
        "        layer_1_test = np.maximum(np.dot(layer_0_test, weights_0_1), 0)  # ReLU activation\n",
        "        layer_2_test = np.dot(layer_1_test, weights_1_2)\n",
        "\n",
        "        # Apply softmax\n",
        "        exp_z_test = np.exp(layer_2_test)\n",
        "        layer_2_s_test = exp_z_test / np.sum(exp_z_test, axis=1, keepdims=True)\n",
        "\n",
        "        # Get predictions (class with highest probability)\n",
        "        y_pred_test = np.argmax(layer_2_s_test, axis=1)\n",
        "        return y_pred_test, layer_2_s_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conducting Error Analysis**\n",
        "<br/>\n",
        "- What can MLP capture that Logistic Regression cannot or vice versa ?\n",
        "<br/>\n",
        "- How do they differ in how they encode features, and do MLP and Logistic Regression Models assign the right weight to the right features ?  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "0.4",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [125], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get predictions from both models\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Sigmoids\u001b[39;00m\n\u001b[1;32m      6\u001b[0m logregSigmoid_preds, logregSigmoid_probs \u001b[38;5;241m=\u001b[39m LogReg_ForwardPass(LogReg_models_sigmoid[lr][\u001b[38;5;241m0\u001b[39m], LogReg_models_sigmoid[lr][\u001b[38;5;241m1\u001b[39m], M_test, Sigmoid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m mlpSigmoid_preds, mlpSigmoid_probs \u001b[38;5;241m=\u001b[39m MLP_ForwardPass(\u001b[43mMLP_models_sigmoid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m], MLP_models_sigmoid[lr][\u001b[38;5;241m1\u001b[39m], M_test, Sigmoid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# testing MLP Sigmoid \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# result = test_models_MLP_Sigmoid(MLP_models_sigmoid[lr][0], MLP_models_sigmoid[lr][1],M_test,sentiment_labels_test)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Get true labels for sentiment analysis\u001b[39;00m\n\u001b[1;32m     18\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(l \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m sentiment_labels_test])\n",
            "\u001b[0;31mKeyError\u001b[0m: 0.4"
          ]
        }
      ],
      "source": [
        "# Error slice analysis\n",
        "# What can MLP capture that Logistic Regression cannot and vice versa ? \n",
        "lr = 0.4\n",
        "# Get predictions from both models\n",
        "# Sigmoids\n",
        "logregSigmoid_preds, logregSigmoid_probs = LogReg_ForwardPass(LogReg_models_sigmoid[lr][0], LogReg_models_sigmoid[lr][1], M_test, Sigmoid=True)\n",
        "mlpSigmoid_preds, mlpSigmoid_probs = MLP_ForwardPass(MLP_models_sigmoid[lr][0], MLP_models_sigmoid[lr][1], M_test, Sigmoid=True)\n",
        "# testing MLP Sigmoid \n",
        "# result = test_models_MLP_Sigmoid(MLP_models_sigmoid[lr][0], MLP_models_sigmoid[lr][1],M_test,sentiment_labels_test)\n",
        "\n",
        "# # Softmax\n",
        "# logregSoftmax_preds, logregSoftmax_probs = LogReg_ForwardPass(LogReg_models_softmax[lr][0], LogReg_models_softmax[lr][1], M_test, Sigmoid=False)\n",
        "# lr = 0.05\n",
        "# mlpSoftmax_preds, mlpSoftmax_probs = MLP_ForwardPass(MLP_models_softmax[lr][0], MLP_models_softmax[lr][1], M_test, Sigmoid=False)\n",
        "\n",
        "\n",
        "# Get true labels for sentiment analysis\n",
        "true_labels = np.array([int(l == \"positive\") for l in sentiment_labels_test])\n",
        "\n",
        "# # Create error slices\n",
        "mlpSigmoid_correct = (mlpSigmoid_preds.flatten() == true_labels)\n",
        "logregSigmoid_correct = (np.array(logregSigmoid_preds).flatten() == true_labels)\n",
        "\n",
        "# print(mlpWins)\n",
        "# Create error slices \n",
        "mlp_wins_sigmoid = [mlpSigmoid_correct[i] and not logregSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "logreg_wins_sigmoid = [logregSigmoid_correct[i] and not mlpSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "both_correct_sigmoid = [mlpSigmoid_correct[i] and logregSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "both_wrong_sigmoid = [not mlpSigmoid_correct[i] and not logregSigmoid_correct[i] for i in range(len(mlpSigmoid_correct))]\n",
        "\n",
        "# Get indices where MLP wins\n",
        "mlp_wins_indices = []\n",
        "for index,value in enumerate(mlp_wins_sigmoid):\n",
        "    if (value == True):\n",
        "        mlp_wins_indices.append(index)\n",
        "# mlp_wins_indices = [i for i in range(len(mlp_wins_sigmoid)) if mlp_wins_sigmoid[i]]\n",
        "\n",
        "# Get indices where LogReg wins\n",
        "logreg_wins_indices = []\n",
        "for index,value in enumerate(logreg_wins_sigmoid):\n",
        "    if (value == True):\n",
        "        logreg_wins_indices.append(index)\n",
        "\n",
        "# Get indices where both correct\n",
        "both_correct_indices = []\n",
        "for index,value in enumerate(both_correct_sigmoid):\n",
        "    if (value == True):\n",
        "        both_correct_indices.append(index)\n",
        "\n",
        "# Get indices where both wrong\n",
        "both_wrong_indices = []\n",
        "for index,value in enumerate(both_wrong_sigmoid):\n",
        "    if (value == True):\n",
        "        both_wrong_indices.append(index)\n",
        "\n",
        "# print(sum([x == 'True' for x in mlp_wins_indices]))\n",
        "\n",
        "# Statistics of comparison between the. two models \n",
        "print(f\"MLP wins : {np.sum(mlp_wins_sigmoid)} ({np.sum(mlp_wins_sigmoid)/len(mlp_wins_sigmoid)*100:.2f}%)\")\n",
        "print(f\"LogReg wins : {np.sum(logreg_wins_sigmoid)} ({np.sum(logreg_wins_sigmoid)/len(logreg_wins_sigmoid)*100:.2f}%)\")\n",
        "print(f\"Both correct: {np.sum(both_correct_sigmoid)} ({np.sum(both_correct_sigmoid)/len(both_correct_sigmoid)*100:.2f}%)\")\n",
        "print(f\"Both wrong: {np.sum(both_wrong_sigmoid)} ({np.sum(both_wrong_sigmoid)/len(both_wrong_sigmoid)*100:.2f}%)\")\n",
        "print(f\"Total examples: {len(mlp_wins_sigmoid)}\")\n",
        "\n",
        "\n",
        "# This is where \n",
        "for index,idx in enumerate(logreg_wins_indices[:5]):\n",
        "    print(index)\n",
        "    # print(test_ints[idx])\n",
        "    test_example_idx = test_ints[idx]  # Map to original review index\n",
        "    print(test_example_idx)\n",
        "    # print(f\"\\n=== Example {idx} (Review #{test_example_idx}) ===\")\n",
        "    print(f\"Review: {reviews[test_example_idx]}\")\n",
        "    print(f\"True label: {sentiment_labels_test[idx]}\")\n",
        "    print(f\"MLP prediction: {'positive' if mlpSigmoid_preds.flatten()[idx] == 1 else 'negative'}\")\n",
        "    print(f\"LogReg prediction: {'positive' if logregSigmoid_preds[idx] == 1 else 'negative'}\")\n",
        "    print(f\"MLP confidence: {mlpSigmoid_probs.flatten()[idx]:.3f}\")\n",
        "    print(f\"LogReg confidence: {logregSigmoid_probs[idx]:.3f}\")\n",
        "    print(\"-------\")\n",
        "# Print statistics for error slices\n",
        "\n",
        "\n",
        "# Print statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before conducting forward propagation analysis, it is worthwile to look at how MLP and Logistic Regression encode different, or assign weights differently. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top words contributing to negative sentiment\n",
            "Word:  NOT -1.8832349980906231\n",
            "Word:  disappointed -1.6376199308415602\n",
            "Word:  bad -1.5465934852573422\n",
            "Word:  poor -1.5299166117454928\n",
            "Word:  return -1.504330086446005\n",
            "Word:  waste -1.4046782804404672\n",
            "Word:  money -1.338414674961026\n",
            "Word:  point -1.3167564323697052\n",
            "Word:  support -1.2498251719647002\n",
            "Word:  cannot -1.2263769846937227\n",
            "Word:  worse -1.1892531455051139\n",
            "Word:  called -1.1858039259878796\n",
            "Word:  instead -1.169703513712682\n",
            "Word:  were -1.1647370511073678\n",
            "Word:  worst -1.1255493771616447\n",
            "Word:  completely -1.1193322092875388\n",
            "Word:  However -1.0981647286922598\n",
            "Word:  There -1.0637467336550144\n",
            "Word:  Not -1.0404313964266452\n",
            "Word:  then -1.005815900201774\n",
            "--------\n",
            "Top words contributing to positive sentiment\n",
            "Word:  patio 1.0940184127386048\n",
            "Word:  wonderful 1.1064772480117975\n",
            "Word:  nicely 1.11055859551451\n",
            "Word:  happy 1.1134307232191354\n",
            "Word:  Thanks 1.1184812884590023\n",
            "Word:  Highly 1.1476421202141873\n",
            "Word:  price 1.1571854889166222\n",
            "Word:  satisfied 1.2000770877151525\n",
            "Word:  job 1.2049367548395866\n",
            "Word:  delicious 1.2183735345853077\n",
            "Word:  loves 1.3499780918702704\n",
            "Word:  best 1.3536705890934382\n",
            "Word:  Great 1.3908485676815703\n",
            "Word:  amazing 1.4379626914481678\n",
            "Word:  perfect 1.4649319575777924\n",
            "Word:  easy 1.5171666395369068\n",
            "Word:  great 1.5171914609874724\n",
            "Word:  awesome 1.5465342293072262\n",
            "Word:  pleased 1.6058766687733554\n",
            "Word:  love 1.6754258071301014\n"
          ]
        }
      ],
      "source": [
        "# Get the highest weights amont LogReg Models Sigmoid \n",
        "# Simply assign weights to each word, and those words can contribute or not it depends. \n",
        "indices = [i for i in (LogReg_models_sigmoid[0.4][0])]\n",
        "word_weight = []\n",
        "for i,value in enumerate(LogReg_models_sigmoid[0.4][0]):\n",
        "    word_weight.append({\n",
        "            \"word\": word_list[i], \n",
        "            \"weight\": value\n",
        "        })\n",
        "print(\"Top words contributing to negative sentiment\")\n",
        "word_weight = sorted(word_weight, key=lambda x: x['weight'], reverse=False)\n",
        "for i in word_weight[:20]:\n",
        "    print(\"Word: \",i['word'], i['weight'])\n",
        "print(\"--------\")\n",
        "print(\"Top words contributing to positive sentiment\")\n",
        "for i in word_weight[-20:]:\n",
        "    print(\"Word: \",i['word'], i['weight'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MLP PROPAGATION ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the highest weights amont MLP SoftMax Models Sigmoid \n",
        "# Different neurons focus on different thigns, and their importance is different accordingly. \n",
        "info_neurons_unsorted = {}\n",
        "second_layer_reverse = MLP_models_sigmoid[0.4][1].T\n",
        "for i in range(32):\n",
        "    info_neurons_unsorted[i+1] = {\n",
        "        \"words\": [word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, i])[::-1][0:15]], \"weights\": second_layer_reverse[:, i],\n",
        "        \"second_layer\": second_layer_reverse[:, i]\n",
        "    }\n",
        "    # print(\"layer\", i+1)\n",
        "    # print([word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, i])[::-1][0:15]])\n",
        "    # print(second_layer_reverse[:, i])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1\n",
            "Words ['terrible', 'boring', 'waste', 'returning', 'disappointing', 'horrible', 'poor', 'worst', 'junk', 'useless', 'refund', 'disappointment', 'awful', 'returned', 'disappointed']\n",
            "Second Layer Weights [-1.5922594]\n",
            "Layer 17\n",
            "Words ['boring', 'terrible', 'worst', 'waste', 'returning', 'useless', 'return', 'poor', 'junk', 'disappointed', 'ridiculous', 'awful', 'worse', 'horrible', 'disappointment']\n",
            "Second Layer Weights [-1.50782529]\n",
            "Layer 29\n",
            "Words ['boring', 'worst', 'terrible', 'waste', 'worthless', 'returning', 'disappointing', 'returned', 'disappointment', 'awful', 'poorly', 'ridiculous', 'unusable', 'falls', 'junk']\n",
            "Second Layer Weights [-1.47757262]\n",
            "Layer 10\n",
            "Words ['waste', 'returning', 'worst', 'terrible', 'useless', 'boring', 'horrible', 'hopes', 'poor', 'disappointed', 'disappointment', 'disappointing', 'dissapointed', 'returned', 'return']\n",
            "Second Layer Weights [-1.42848866]\n",
            "Layer 19\n",
            "Words ['returning', 'boring', 'terrible', 'waste', 'useless', 'worst', 'horrible', 'refund', 'return', 'awful', 'worse', 'disappointment', 'poor', 'hopes', 'dissapointed']\n",
            "Second Layer Weights [-1.37040058]\n"
          ]
        }
      ],
      "source": [
        "# In order from negative to positive \n",
        "info_neurons = sorted(info_neurons_unsorted.items(), key=lambda x: x[1][\"second_layer\"], reverse=False)\n",
        "\n",
        "for i in range(5):\n",
        "    print('Layer', info_neurons[i][0])\n",
        "    print('Words',info_neurons[i][1]['words'])\n",
        "    print('Second Layer Weights', info_neurons[i][1]['second_layer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 12\n",
            "Words ['Excellent', 'Great', 'love', 'delicious', 'amazing', 'Highly', 'awesome', 'satisfied', 'pleased', 'perfect', 'con', 'job', 'excellent', 'hooked', 'best']\n",
            "Second Layer Weights [2.18863573]\n",
            "Layer 30\n",
            "Words ['Excellent', 'Highly', 'love', 'awesome', 'amazing', 'Great', 'great', 'excellent', 'perfect', 'job', 'loves', 'delicious', 'pleased', 'perfectly', 'best']\n",
            "Second Layer Weights [2.14165146]\n",
            "Layer 14\n",
            "Words ['Excellent', 'Highly', 'excellent', 'great', 'delicious', 'job', 'This', 'easy', 'Great', 'exactly', 'pleased', 'highly', 'Works', 'awesome', 'enjoyed']\n",
            "Second Layer Weights [1.55770792]\n",
            "Layer 4\n",
            "Words ['Great', 'Highly', 'delicious', 'pleased', 'amazing', 'Excellent', 'awesome', 'GREAT', 'perfect', 'excellent', 'wonderful', 'exactly', 'love', 'Thanks', 'grill']\n",
            "Second Layer Weights [1.31162707]\n",
            "Layer 15\n",
            "Words ['excellent', 'Great', 'Excellent', 'GREAT', 'pleased', 'highly', 'delicious', 'perfect', 'Highly', 'use', 'hooked', 'job', 'exactly', 'LOVE', 'aspects']\n",
            "Second Layer Weights [1.26878686]\n"
          ]
        }
      ],
      "source": [
        "# In order from negative to positive \n",
        "info_neurons = sorted(info_neurons_unsorted.items(), key=lambda x: x[1][\"second_layer\"], reverse=True)\n",
        "\n",
        "for i in range(5):\n",
        "    print('Layer', info_neurons[i][0])\n",
        "    print('Words',info_neurons[i][1]['words'])\n",
        "    print('Second Layer Weights', info_neurons[i][1]['second_layer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MLP Analysis Wrong Logistic Regression Correct**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Forward Pass ----------------\n",
            "16\n",
            "Text,   After doing this film Steve Martin quit doing movies for a while and even turned down scripts written for him so if your a huge fan you may enjoy it  but Roxanne  LA Story  and Bowfinger are some of Steve s best works  \n",
            "Prediction:  positive\n",
            "X_pred (Before Sigmoid):  [[2.47658183]]\n",
            "X_pred (After Sigmoid):  [[0.92248373]]\n",
            "Neuron Analysis--------------------\n",
            "POSITIVE NEURON ANALYSIS--------\n",
            "Text,   After doing this film Steve Martin quit doing movies for a while and even turned down scripts written for him so if your a huge fan you may enjoy it  but Roxanne  LA Story  and Bowfinger are some of Steve s best works  \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['Excellent', 'Great', 'love', 'delicious', 'amazing']\n",
            "Top words in sentence ['best', 'but', 'works', 'enjoy', 'for']\n",
            "Neuron:  12\n",
            "Output Weight 0.6490857154576921\n",
            "Contribution 1.4206121861827072\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'love', 'awesome', 'amazing']\n",
            "Top words in sentence ['best', 'for', 'enjoy', 'art', 'him']\n",
            "Neuron:  30\n",
            "Output Weight 0.3271259114496303\n",
            "Contribution 0.7005896857794878\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Great', 'Highly', 'pleased', 'awesome']\n",
            "Top words in sentence ['are', 'enjoy', 'best', 'for', 'fan']\n",
            "Neuron:  25\n",
            "Output Weight 0.22005107167021734\n",
            "Contribution 0.24032703357597376\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'great', 'Great', 'awesome', 'highly']\n",
            "Top words in sentence ['best', 'for', 'works', 'enjoy', 'are']\n",
            "Neuron:  8\n",
            "Output Weight 0.18753538445565693\n",
            "Contribution 0.21419313051663758\n",
            "---------\n",
            "Top words for the neuron ['love', 'Excellent', 'Great', 'great', 'best']\n",
            "Top words in sentence ['best', 'for', 'art', 'enjoy', 'are']\n",
            "Neuron:  31\n",
            "Output Weight 0.16111836877422103\n",
            "Contribution 0.182026901921758\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'best', 'Great', 'awesome', 'perfectly']\n",
            "Top words in sentence ['best', 'for', 'fan', 'are', 'enjoy']\n",
            "Neuron:  23\n",
            "Output Weight 0.23074470533448693\n",
            "Contribution 0.1636979454360687\n",
            "---------\n",
            "NEGATIVE NEURON ANALYSIS -------------\n",
            "Text,   After doing this film Steve Martin quit doing movies for a while and even turned down scripts written for him so if your a huge fan you may enjoy it  but Roxanne  LA Story  and Bowfinger are some of Steve s best works  \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['boring', 'worst', 'terrible', 'waste', 'worthless']\n",
            "Top words in sentence ['rip', 'even', 'script', 'film', 'After']\n",
            "Neuron:  29\n",
            "Output Weight 0.2306340793805638\n",
            "Contribution -0.3407785999641321\n",
            "---------\n",
            "Top words for the neuron ['waste', 'returning', 'boring', 'useless', 'disappointment']\n",
            "Top words in sentence ['rip', 'but', 'After', 'turned', 'written']\n",
            "Neuron:  11\n",
            "Output Weight 0.18272113703273118\n",
            "Contribution -0.18748887583587437\n",
            "---------\n",
            "Top words for the neuron ['waste', 'boring', 'terrible', 'returning', 'return']\n",
            "Top words in sentence ['movies', 'written', 'film', 'Mart', 'quit']\n",
            "Neuron:  7\n",
            "Output Weight 0.11657652657222986\n",
            "Contribution -0.10605969772013425\n",
            "---------\n",
            "Top words for the neuron ['returning', 'boring', 'terrible', 'waste', 'useless']\n",
            "Top words in sentence ['film', 'work', 'script', 'even', 'rip']\n",
            "Neuron:  19\n",
            "Output Weight 0.07219378600848166\n",
            "Contribution -0.09893440609940757\n",
            "---------\n",
            "Top words for the neuron ['waste', 'returning', 'worst', 'terrible', 'useless']\n",
            "Top words in sentence ['rip', 'work', 'film', 'turned', 'quit']\n",
            "Neuron:  10\n",
            "Output Weight 0.06480291113947877\n",
            "Contribution -0.09257022361092974\n",
            "---------\n",
            "Top words for the neuron ['terrible', 'boring', 'waste', 'returning', 'disappointing']\n",
            "Top words in sentence ['for', 'rip', 'work', 'even', 'film']\n",
            "Neuron:  1\n",
            "Output Weight 0.052668164608316824\n",
            "Contribution -0.08386138030878897\n",
            "---------\n",
            "2.476581830810898\n",
            "0.9224837258323941\n"
          ]
        }
      ],
      "source": [
        "# We define several functions taht would help us decompose the classificaiton result \n",
        "# Performing MLP \n",
        "def perform_MLP_analysis(idx):\n",
        "    # Activation for certain examples \n",
        "    # Perform forward pass\n",
        "    print(\"Performing Forward Pass ----------------\")\n",
        "    # idx = logreg_wins_indices[0]\n",
        "    print(idx)\n",
        "    test_example_idx = test_ints[idx] \n",
        "    # Map to original review index and put it into all reviews test, but we stil use M_test, as we have set that previously \n",
        "    print(\"Text, \", reviews[test_example_idx])\n",
        "    # This specifically helps it to have 2d array \n",
        "    layer_0_final_test = M_test[idx:idx+1]\n",
        "    layer_1_final_test = np.maximum((np.dot(layer_0_final_test, MLP_models_sigmoid[0.4][0])), 0)\n",
        "    layer_2_final_test = np.dot(layer_1_final_test, MLP_models_sigmoid[0.4][1])\n",
        "    layer_2_s_final_test = 1 / (1 + np.exp(-layer_2_final_test))\n",
        "\n",
        "    #Converting probabilities\n",
        "    x_pred = (layer_2_s_final_test > 0.5).astype(int)\n",
        "    prediction = \"positive\" if x_pred[0][0] == 1 else \"negative\"\n",
        "    print(\"Prediction: \", prediction)\n",
        "    print('X_pred (Before Sigmoid): ', layer_2_final_test)\n",
        "    print('X_pred (After Sigmoid): ', layer_2_s_final_test)\n",
        "\n",
        "    print(\"Neuron Analysis--------------------\")\n",
        "    layer = []\n",
        "    second_layer_reverse = MLP_models_sigmoid[0.4][1].T.flatten()\n",
        "    for i,weight in enumerate(layer_1_final_test[0]):\n",
        "        layer.append({\n",
        "            'neuron': i+1,\n",
        "            'output_weight': weight,\n",
        "            'contribution': weight * second_layer_reverse[i]\n",
        "        })\n",
        "        \n",
        "    print(\"POSITIVE NEURON ANALYSIS--------\")\n",
        "    sorted_layers = sorted(layer, key=lambda x: x['contribution'], reverse=True)\n",
        "    # With one hot encoding, we have a list of words activated\n",
        "    print(\"Text, \", reviews[test_example_idx])\n",
        "    print(\"Neuron contributions (sorted by impact):\")\n",
        "    for index, info in enumerate(sorted_layers):\n",
        "        if(index>5):\n",
        "            break\n",
        "        # Top word for that specific neuron \n",
        "        words = [word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, info['neuron']-1])[::-1]]\n",
        "        print(\"Top words for the neuron\", words[0:5])\n",
        "        # Also retrieve the weights of the words for that specific neuron\n",
        "        top_words_in_sentence = [word for word in words if word in reviews[test_example_idx]]\n",
        "        print(\"Top words in sentence\",top_words_in_sentence[0:5])\n",
        "        print('Neuron: ',info['neuron'])\n",
        "\n",
        "        print('Output Weight',info['output_weight'])\n",
        "        print('Contribution',info['contribution'])\n",
        "        print(\"---------\")\n",
        "    \n",
        "    print(\"NEGATIVE NEURON ANALYSIS -------------\")\n",
        "    sorted_layers = sorted(layer, key=lambda x: x['contribution'], reverse=False)\n",
        "    # With one hot encoding, we have a list of words activated\n",
        "    print(\"Text, \", reviews[test_example_idx])\n",
        "    print(\"Neuron contributions (sorted by impact):\")\n",
        "    for index, info in enumerate(sorted_layers):\n",
        "        if(index>5):\n",
        "            break\n",
        "        # Top word for that specific neuron \n",
        "        words = [word_list[x] for x in np.argsort(MLP_models_sigmoid[0.4][0][:, info['neuron']-1])[::-1]]\n",
        "        print(\"Top words for the neuron\", words[0:5])\n",
        "        # Also retrieve the weights of the words for that specific neuron\n",
        "        top_words_in_sentence = [word for word in words if word in reviews[test_example_idx]]\n",
        "        print(\"Top words in sentence\",top_words_in_sentence[0:5])\n",
        "        print('Neuron: ',info['neuron'])\n",
        "\n",
        "        print('Output Weight',info['output_weight'])\n",
        "        print('Contribution',info['contribution'])\n",
        "        print(\"---------\")\n",
        "\n",
        "    # To see whether it has the same score as probability gathered in the previous forward pass\n",
        "    # An it actually is, suggesting our method correctly analyses the different componenet of the output\n",
        "    sum = 0\n",
        "    for items in sorted_layers:\n",
        "        sum += items['contribution']\n",
        "    print(sum)\n",
        "    # Run through sigmoid function \n",
        "    sum = 1 / (1 + np.exp(-sum))\n",
        "    print(sum)\n",
        "    \n",
        "perform_MLP_analysis(logreg_wins_indices[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perform forward pass for Logistic Regression --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Review Text  After doing this film Steve Martin quit doing movies for a while and even turned down scripts written for him so if your a huge fan you may enjoy it  but Roxanne  LA Story  and Bowfinger are some of Steve s best works  \n",
            "negative\n",
            "Prediction Numbers\n",
            "[-2.35835246]\n",
            "Performing weight analysis --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence  After doing this film Steve Martin quit doing movies for a while and even turned down scripts written for him so if your a huge fan you may enjoy it  but Roxanne  LA Story  and Bowfinger are some of Steve s best works  \n",
            "Word Contributing: even\n",
            "Weight: -0.9016876594132689\n",
            "Word Contributing: while\n",
            "Weight: -0.8623690251704159\n",
            "Word Contributing: film\n",
            "Weight: -0.8432528888303694\n",
            "Word Contributing: After\n",
            "Weight: -0.8098048917225451\n",
            "Word Contributing: turned\n",
            "Weight: -0.5916441396681925\n",
            "Word Contributing: written\n",
            "Weight: -0.5498078028357388\n",
            "Word Contributing: but\n",
            "Weight: -0.4627255563541934\n",
            "Word Contributing: are\n",
            "Weight: -0.3921287698942646\n",
            "Word Contributing: doing\n",
            "Weight: -0.33806716860348635\n",
            "Word Contributing: huge\n",
            "Weight: -0.3192199606355793\n",
            "Word Contributing: for\n",
            "Weight: -0.16366487413191405\n",
            "Word Contributing: Martin\n",
            "Weight: -0.07897972592089039\n",
            "Word Contributing: fan\n",
            "Weight: -0.015819698374573934\n",
            "Word Contributing: him\n",
            "Weight: 0.06196556270436593\n",
            "Word Contributing: Steve\n",
            "Weight: 0.09740721829904858\n",
            "Word Contributing: quit\n",
            "Weight: 0.11576981414675293\n",
            "Word Contributing: movies\n",
            "Weight: 0.2710184817519152\n",
            "Word Contributing: works\n",
            "Weight: 0.5765775254888935\n",
            "Word Contributing: Story\n",
            "Weight: 0.642433376175059\n",
            "Word Contributing: enjoy\n",
            "Weight: 0.8010814214464806\n",
            "Word Contributing: best\n",
            "Weight: 1.3536705890934382\n",
            "Sum of weights: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[-2.35835246]\n"
          ]
        }
      ],
      "source": [
        "def perform_LogReg_analysis(idx):\n",
        "    # LogReg forward pass \n",
        "    print(\"Perform forward pass for Logistic Regression\",'----'*50)\n",
        "    bias = LogReg_models_sigmoid[0.4][1]\n",
        "    test_example_idx = test_ints[idx]\n",
        "    z= M_test[idx:idx+1].dot(LogReg_models_sigmoid[0.4][0]) + LogReg_models_sigmoid[0.4][1]\n",
        "    q = (1 / (1+np.exp(-z)))\n",
        "    x_test_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "    prediction = \"positive\" if x_test_pred[0] == 1 else \"negative\"\n",
        "    print(\"Review Text\",reviews[test_example_idx])\n",
        "    print(prediction)\n",
        "    print(\"Prediction Numbers\")\n",
        "    print(z)\n",
        "    print(\"Performing weight analysis\",'----'*50)\n",
        "    # Perform weight analysis for the specific example \n",
        "    # Get the feature vector for this example\n",
        "    feature_vector = M_test[idx]  # Shape: (8000,) - contains 0s and 1s\n",
        "    # Analyse the activations for this sentence \n",
        "    activated_indices = [index for index,value in enumerate(feature_vector) if value == 1]\n",
        "    # Dictionary to attach words and it's corresponding weight \n",
        "    word_weight = []\n",
        "    for index_value in activated_indices:\n",
        "        word_weight.append({\n",
        "            'word': word_list[index_value],\n",
        "            'weight' : LogReg_models_sigmoid[0.4][0][index_value]\n",
        "        })\n",
        "    word_activation_sorted = sorted(word_weight, key=lambda x: x['weight'], reverse=False)\n",
        "    # # The sentence \n",
        "    print(\"Sentence\", reviews[test_example_idx])\n",
        "\n",
        "\n",
        "    for item in word_activation_sorted:\n",
        "        print(\"Word Contributing:\",item['word'])\n",
        "        print(\"Weight:\",item['weight'])\n",
        "    print(\"Sum of weights:\", \"----\"*40)\n",
        "    sum_total = 0\n",
        "    for word in word_activation_sorted:\n",
        "        sum_total += word['weight']\n",
        "    print(\"Sum of weights from decomposition:\",sum_total+bias)\n",
        "\n",
        "perform_LogReg_analysis(logreg_wins_indices[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MLP Analysis Wrong Logistic Regression Correct**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# index_analyse = mlp_wins_indices[1]\n",
        "index_analyse = logreg_wins_indices[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Forward Pass ----------------\n",
            "50\n",
            "Text,   I ve had dazzle products in the past  Usually a very good product  This DVD Recorder is just not very reliable  I would recommmend spending a little more and getting one of the higher end dazzle products \n",
            "Prediction:  positive\n",
            "X_pred (Before Sigmoid):  [[3.01547373]]\n",
            "X_pred (After Sigmoid):  [[0.9532683]]\n",
            "Neuron Analysis--------------------\n",
            "POSITIVE NEURON ANALYSIS--------\n",
            "Text,   I ve had dazzle products in the past  Usually a very good product  This DVD Recorder is just not very reliable  I would recommmend spending a little more and getting one of the higher end dazzle products \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['Excellent', 'Highly', 'love', 'awesome', 'amazing']\n",
            "Top words in sentence ['This', 'very', 'pen', 'good', 'just']\n",
            "Neuron:  30\n",
            "Output Weight 0.3666385792836228\n",
            "Contribution 0.7852120485249362\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Great', 'love', 'delicious', 'amazing']\n",
            "Top words in sentence ['This', 'reliable', 'pen', 'very', 'had']\n",
            "Neuron:  12\n",
            "Output Weight 0.3441723025168743\n",
            "Contribution 0.7532677972388718\n",
            "---------\n",
            "Top words for the neuron ['excellent', 'Great', 'Excellent', 'GREAT', 'pleased']\n",
            "Top words in sentence ['just', 'product', 'good', 'very', 'pen']\n",
            "Neuron:  15\n",
            "Output Weight 0.40228056226261016\n",
            "Contribution 0.5104082896486157\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Highly', 'excellent', 'great', 'delicious']\n",
            "Top words in sentence ['This', 'product', 'good', 'high', 'reliable']\n",
            "Neuron:  14\n",
            "Output Weight 0.2864950256110395\n",
            "Contribution 0.446275569591601\n",
            "---------\n",
            "Top words for the neuron ['Great', 'Highly', 'delicious', 'pleased', 'amazing']\n",
            "Top words in sentence ['This', 'very', 'product', 'cord', 'reliable']\n",
            "Neuron:  4\n",
            "Output Weight 0.29593038242214725\n",
            "Contribution 0.3881503008242392\n",
            "---------\n",
            "Top words for the neuron ['Excellent', 'Great', 'Highly', 'pleased', 'awesome']\n",
            "Top words in sentence ['reliable', 'com', 'good', 'This', 'had']\n",
            "Neuron:  25\n",
            "Output Weight 0.27419341258487556\n",
            "Contribution 0.2994581620186441\n",
            "---------\n",
            "NEGATIVE NEURON ANALYSIS -------------\n",
            "Text,   I ve had dazzle products in the past  Usually a very good product  This DVD Recorder is just not very reliable  I would recommmend spending a little more and getting one of the higher end dazzle products \n",
            "Neuron contributions (sorted by impact):\n",
            "Top words for the neuron ['returning', 'boring', 'terrible', 'waste', 'useless']\n",
            "Top words in sentence ['This', 'Usually', 'DVD', 'lit', 'order']\n",
            "Neuron:  19\n",
            "Output Weight 0.22238531723512694\n",
            "Contribution -0.3047569673558994\n",
            "---------\n",
            "Top words for the neuron ['terrible', 'boring', 'waste', 'returning', 'disappointing']\n",
            "Top words in sentence ['This', 'product', 'just', 'lit', 'very']\n",
            "Neuron:  1\n",
            "Output Weight 0.1132343573182931\n",
            "Contribution -0.18029847012347128\n",
            "---------\n",
            "Top words for the neuron ['boring', 'returning', 'ridiculous', 'disappointing', 'waste']\n",
            "Top words in sentence ['This', 'very', 'just', 'product', 'DVD']\n",
            "Neuron:  2\n",
            "Output Weight 0.11356887697334253\n",
            "Contribution -0.11480692347266158\n",
            "---------\n",
            "Top words for the neuron ['boring', 'worst', 'terrible', 'waste', 'worthless']\n",
            "Top words in sentence ['product', 'pro', 'lit', 'very', 'spend']\n",
            "Neuron:  29\n",
            "Output Weight 0.06010723809046209\n",
            "Contribution -0.08881280901414178\n",
            "---------\n",
            "Top words for the neuron ['waste', 'returning', 'boring', 'useless', 'disappointment']\n",
            "Top words in sentence ['DVD', 'This', 'had', 'pro', 'product']\n",
            "Neuron:  11\n",
            "Output Weight 0.07654962643501458\n",
            "Contribution -0.07854703423493954\n",
            "---------\n",
            "Top words for the neuron ['boring', 'terrible', 'worst', 'waste', 'returning']\n",
            "Top words in sentence ['product', 'Usually', 'get', 'very', 'high']\n",
            "Neuron:  17\n",
            "Output Weight 0.03311617880707503\n",
            "Contribution -0.04993341202138389\n",
            "---------\n",
            "3.015473729606333\n",
            "0.9532683030658237\n"
          ]
        }
      ],
      "source": [
        "perform_MLP_analysis(index_analyse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perform forward pass for Logistic Regression --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Review Text  I ve had dazzle products in the past  Usually a very good product  This DVD Recorder is just not very reliable  I would recommmend spending a little more and getting one of the higher end dazzle products \n",
            "negative\n",
            "Prediction Numbers\n",
            "[-0.06231004]\n",
            "Performing weight analysis --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence  I ve had dazzle products in the past  Usually a very good product  This DVD Recorder is just not very reliable  I would recommmend spending a little more and getting one of the higher end dazzle products \n",
            "Word Contributing: products\n",
            "Weight: -0.7198113549615826\n",
            "Word Contributing: end\n",
            "Weight: -0.5764965553893423\n",
            "Word Contributing: had\n",
            "Weight: -0.3027737855232317\n",
            "Word Contributing: DVD\n",
            "Weight: -0.20823633124071206\n",
            "Word Contributing: product\n",
            "Weight: -0.2058385543694446\n",
            "Word Contributing: just\n",
            "Weight: -0.17279090347284093\n",
            "Word Contributing: getting\n",
            "Weight: -0.00014015881418811621\n",
            "Word Contributing: very\n",
            "Weight: 0.015213280914938626\n",
            "Word Contributing: Usually\n",
            "Weight: 0.11460733680135839\n",
            "Word Contributing: This\n",
            "Weight: 0.11886953330216184\n",
            "Word Contributing: spending\n",
            "Weight: 0.42502298865672966\n",
            "Word Contributing: reliable\n",
            "Weight: 0.4322321348192526\n",
            "Word Contributing: good\n",
            "Weight: 0.47683085662964714\n",
            "Word Contributing: higher\n",
            "Weight: 0.4901057586855109\n",
            "Sum of weights: ----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[-0.06231004]\n"
          ]
        }
      ],
      "source": [
        "perform_LogReg_analysis(index_analyse)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
